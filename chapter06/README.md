# chapter06. Gate Added RNN

- 5장에서 본 RNN은 순환 경로를 포함하며 과거의 정보를 기억할 수 있었습니다.

- 이 RNN은 구조가 단순하여 쉽게 구현할 수 있었지만 성능이 좋지 못합니다.

  - 그 원인은 시계열 데이터에서 시간적으로 멀리 떨어진, 장기 의존 관계를 잘 학습할 수 없다는 데 있습니다.

- 그래서 요즘에는 앞 장의 단순한 RNN 대신에 LSTM이나 GRU라는 계층이 주로 쓰이고 실제로 LSTM이나 GRU를 그냥 'RNN'이라고 합니다.

  - 오히려 RNN을 명시적으로 '기본적인 RNN'이라고 부릅니다.

- LSTM이나 GRU에는 '게이트'라는 구조가 더해져있어 시계열 데이터의 장기 의존 관계를 학습할 수 있습니다.

- 이번 장에서는 앞 장에서 설명한 기본 RNN의 문제점을 알아보고 이를 대신하는 계층으로써 LSTM과 GRU같은 '게이트가 추가된 RNN'을 소개합니다.

  - 특히 LSTM의 구조를 시간을 들여 차분히 살펴보고, 이 구조가 '장기 기억'을 가능하게 하는 매커니즘을 이해해봅니다.

  - 그리고 LSTM을 사용해 언어 모델을 만들어, 실제로 데이터를 잘 학습하는 모습도 알아보겠습니다.

## 6.1 RNN의 문제점

- 앞 장에서 설명한 RNN은 시계열 데이터의 장기 의존 관계를 학습하기 어렵습니다.

  - 그 원인은 BPTT에서 기울기 소실 혹은 기울기 폭발이 일어나기 때문입니다.

- 이번 절에서는 앞 장에서 배운 RNN 계층을 복습하고, 뒤이어 RNN 계층이 장기 기억을 제대로 처리하지 못하는 이유를 알아보겠습니다.

### 6.1.1 RNN 복습

- RNN 게층은 순환 경로를 가지고 있는데 이 순환을 펼치면 다음 그림과 같이 옆으로 길게 뻗은 신경망이 됩니다.

<img src="README.assets/fig 6-1.png" alt="fig 6-1" style="zoom:50%;" />

- 위 그림에서 보듯, RNN 계층은 시계열 데이터 x를 입력하면 h를 출력합니다. 이 h는 RNN 계층의 **은닉 상태**라고 하며 과거 정보를 저장합니다.

- RNN의 특징은 바로 이전 시각의 은닉 상태를 이용한다는 점인데 이렇게 해서 과거 정보를 계승할 수 있습니다.

  - 이 때 RNN 계층이 수행하는 처리를 계산 그래프로 나타나매녀 다음과 같습니다.

  <img src="README.assets/fig 6-2.png" alt="fig 6-2" style="zoom:50%;" />

  - 위 그림처럼 RNN 계층의 순전파는 행렬의 곱과 합, 활성화 함수 tanh 함수에 의한 계산을 수행합니다.

- 이상이 앞 장에서 본 RNN 계층입니다. 이어서 RNN 계층이 안고 있는 문제, 즉 장기 기억에 취약하다는 문제를 살펴보겠습니다.

### 6.1.2 기울기 소실 또는 기울기 폭발

- 언어 모델은 주언진 단어들을 기초로 다음에 출현할 단어를 예측하는 일을 합니다.

  - 앞 장에서는 RNN을 사용해 언어 모델을 구현해 RNNLM이라 했는데 이번 절에서는 그 RNNLM의 단점을 확인해보겠습니다.

<img src="README.assets/fig 6-3.png" alt="fig 6-3" style="zoom:50%;" />

- 위 그림의 정답은 'Tom'입니다. RNNLM이 이 문제를 올바르게 맞히려면 'Tom이 방에서 TV를 보고 있다.'는 것과 '그 방에 Mary가 들어갔다.'는 것을 알아야 합니다.

  - 다시 말해 이런 정보를 RNN 계층의 은닉 상태에 인코딩해 보관해둬야 합니다.

- 그럼 이 예를 RNNLM 학습의 관점에서 생각해보겠습니다.

  - 여기에서는 정답 레이블로 'Tom'이라는 단어가 주어졌을 때, 기울기가 어떻게 전파되는지를 살펴보겠습니다.

  - 학습은 BPTT로 수행하며 정답 레이블이 'Tom'이라고 주어진 시점으로부터 과거 방향으로 기울기를 전달합니다.

<img src="README.assets/fig 6-4.png" alt="fig 6-4" style="zoom:50%;" />

- 위 그림과 같이 정답 레이블이 'Tom'임을 학습할 때 중요한 것이 바로 RNN 계층의 존재입니다.

  - RNN 계층이 과거 방향으로 '의미 있는 기울기'를 전달함으로써 시간 방향의 의존 관계를 학습할 수 있는 것입니다.

- 이때 기울기는 원래대로라면 학습해야 할 의미가 있는 정보가 들어 있고 그것을 과거로 전달해 장기 의존 관계를 학습합니다.

  - 하지만 만약 이 기울기가 중간에 사그라들어 거의 아무런 정보도 남지 않는다면 가중치 매개변수는 거의 갱신되지 않습니다.

  - 즉, 장기 의존 관계를 학습할 수 없게 됩니다.

- 안타깝지만, 현재의 단순한 RNN 계층에서는 시간을 거슬러 올라갈수록 기울기가 작아지거나 커질 수가 있으며 이는 거의 대부분의 상황에서 일어납니다.

### 6.1.3 기울기 소실과 기울기 폭발의 원인

- 그럼 RNN 계층에서 기울기 소실 혹은 기울기 폭발이 일어나는 원인을 살펴보겠습니다.

  - 다음과 같은 RNN 계층에서 시간 방향 기울기 전파에 주목하세요.

<img src="README.assets/fig 6-5.png" alt="fig 6-5" style="zoom:50%;" />

- 위 그림에서 길이가 T인 시계열 데이터를 가정해 T번째 정답 레이블에서 전해지는 기울기가 어떻게 변하는지 보겠습니다.

  - 앞의 문제에 대입하면 T번째 정답 레이블이 'Tom'인 경우로 이 때 시간 방향 기울기가 역전파로 전해질 때 'tanh', '+', 'MatMul' 연산을 거칩니다.

- '+'의 역전파는 상류에서 전해지는 기울기를 그대로 하류로 흘려보내므로 기울기가 변하지 않습니다.

  - 그러면 나머지 두 연산인 'tanh'와 'MatMul'이 변화시키는 기울기를 보겟습니다.

    - 'tanh'의 경우 'Appedix A. Differential of Sigmoid and tanh function'에서 자세히 나오겠지만 tanh 함수의 값과 미분 값은 다음과 같습니다.

    <img src="README.assets/fig 6-6.png" alt="fig 6-6" style="zoom:50%;" />

    - 점선이 tanh 함수으 미분인데 그 값이 1 이하이며 x가 0에서 멀어질수록 작아집니다.

    - 다르게 말해 역전파에서 기울기가 tanh를 지날 때마다 값이 계속 작아진다는 것이며 tanh 함수를 T번 통과하면 기울기도 T번 작아집니다.

#### Note

> RNN 계층의 활성화 함수로는 주로 tanh 함수를 사용하는 데 이를 ReLU로 바꾸면 기울기 소실을 줄일 수 있습니다.
>
> 그 이유는 ReLU는 입력이 0 이상이면 역전파일 때 상류의 기울기를 그대로 하류로 보내기 때문으로 기울기가 작아지지 않습니다.

- 다음은 MatMul 노드 차례입니다.

  - 이야기를 단순하게 하기 위해 tanh 노드를 무시하면 RNN 계층의 역전파의 기울기는 'MatMul'에 의해서만 변합니다.

<img src="README.assets/fig 6-7.png" alt="fig 6-7" style="zoom:50%;" />

- 위 그림은 상류에서 dh가 흘러올 때 MatMul 노드에서 역전파로 dhW<sub>h</sub><sup>T</sup> 행렬 곱으로 기울기를 계산함을 보여줍니다.

  - 그리고 같은 계산을 시계열 데이터의 시간 크기만큼 반복하는 것인데 여기서 주목할 점은 매번 같은 기울기 W<sub>h</sub><sup>T</sup>가 행렬 곱에 사용된다는 것입니다.

- 그러면 역전파 시 기울기가 MatMul 노드를 지날 때 마다 어떻게 변하게 되는지 코드를 통해 살펴보겠습니다.

> 6.1.3_rnn_gradient_graph.py를 확인하세요.

- 이 코드는 dh를 np.ones()로 초기화한 뒤 역전파의 MatMul 노드 수 만큼 dh를 갱신하고 각 단계의 dh 크기(norm)을 norm_list에 추가합니다.

  - 여기에서는 미니배치(N개)의 평균 'L2 노름'을 구해 dh 크기로 사용하고 있는데 L2 노름이란 각 원소를 제곱해 모두 더하고 제곱근을 취한 것입니다.

<img src="README.assets/fig 6-8.png" alt="fig 6-8" style="zoom:50%;" />

- 결과를 위와 같은데 기울기의 크기가 시간에 비례해 지수적으로 증가하는데 이를 **기울기 폭발**이라고 합니다.

  - 이러한 기울기 폭발이 일어나면 결국 오버플로를 일으켜 NaN이 되어 신경망 학습을 제대로 수행할 수 없게 됩니다.

- 그러면 Wh의 초깃값을 다음과 같이 변경한 후 두 번째 실험을 해보겠습니다.

```python
# Wh = np.random.randn(H, H)를 변경합니다.
Wh = np.random.randn(H, H) * 0.5
```

<img src="README.assets/fig 6-9.png" alt="fig 6-9" style="zoom:50%;" />

- 위와 같이 이번에는 기울기가 지수적으로 감소하는데 이것이 **기울기 소실**입니다.

  - 기울기 소실이 일어나면 기울기가 매우 빠르게 작아지며 기울기가 일정 수준 이하로 작아집니다.

  - 그럴 경우 가중치 매개변수가 더 이상 갱신되지 않아 장기 의존 관계를 학습할 수 없습니다.

- 지금까지 결과는 기울기의 크기가 지수적으로 증가하거나 감소하는데 이는 행렬 Wh를 T번 반복해서 '곱'한 결과입니다.

  - 만약 Wh가 스칼라라면 더 이해하기 쉬운데 Wh가 1보다 크면 지수적으로 증가하고 1보다 작으면 지수적으로 감소합니다.

  - 하지만 이처럼 Wh가 스칼라가 아닌 행렬이라면 행렬의 '특잇값'이 척도가 됩니다.

    - 행렬의 특잇값은 데이터가 얼마나 퍼져 있는지를 나타내는 것으로 특잇값, 정확하게는 최대값이 1보다 큰지 여부를 보면 기울기가 변하는 척도를 예측할 수 있습니다.

#### Warning

> 특잇값의 최대값이 1보다 크면 지수적으로 증가, 1보다 작으면 감소할 가능성이 높습니다.
>
> 하지만 그럴 가능성이 높을 뿐 특잇값이 1보다 크다고 반드시 폭발하는 것은 아닙니다. 즉, 필요조건일 뿐 충분조건은 아닙니다.
>
> RNN의 기울기 소실과 기울기 폭발은 관련 논문을 더 참고하세요.

### 6.1.4 기울기 폭발 대책

- 지금까지 RNN의 문제점을 살펴보았는데 이제 이 해결책을 알아보겠습니다.

- 여기서는 우선 기울기 폭발의 해결책을 볼텐데 전통적인 기법인 **기울기 클리핑**이 있습니다.

<img src="README.assets/e 6-0.png" alt="e 6-0" style="zoom:50%;" />

- 신경망에서 사용되는 모든 매개변수에 대한 기울기를 하나로 처리한다고 가정하고 이를 기호 g로 표기하고 문턱값으로 threshold를 설정합니다.

  - 이때 기울기의 L2 norm(||g||)가 문턱값을 초과하면 두 번째 줄의 수식이 기울기를 수정하며 이를 기울기 클리핑이라고 합니다.

  - 이는 단순한 알고리즘이지만 많은 경우에 잘 작동합니다.

#### Warning

> g는 신경망에서 사용되는 모든 매개변수의 기울기를 하나로 모은 것입니다.
>
> 예를 들어 두 개의 가중치 W1과 W2 매개변수를 사용하는 모델은 기울기 dW1와 dW2를 모두 합한 것이 g입니다.

- 이제 기울기 클리핑을 Python으로 구현해보겠습니다.

  - 기울기 클리핑은 clip_grads 함수로 구현하고 인수 grads로 기울기 리스트, max_norm으로 문턱값을 받습니다.

> 자세한 내용은 6.1.4_clip_grads.py를 확인하세요.

- 이것이 기울기 클리핑의 구현인데 clip_grads 함수는 자주 사용되므로 commons/util.py에 같은 구현이 준비되어 있습니다.

#### Note

> 이 책에서는 RNNLM을 학습시키는 RnnlmTrainer 클래스를 제공합니다.
>
> 이 클래스 내부에서도 기울기 클리핑 함수를 사용해 기울기 폭발을 방지합니다.
>
> RnnlmTrainer 클래스의 기울기 클리핑은 '6.4 LSTM을 사용한 언어 모델'절에서 다시 설명합니다.

- 이것으로 기울기 클리핑에 대한 설명을 마치고 이제 기울기 소실의 대책에 대해 알아보겠습니다.
