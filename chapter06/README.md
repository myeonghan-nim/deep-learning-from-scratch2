# chapter06. Gate Added RNN

- 5장에서 본 RNN은 순환 경로를 포함하며 과거의 정보를 기억할 수 있었습니다.

- 이 RNN은 구조가 단순하여 쉽게 구현할 수 있었지만 성능이 좋지 못합니다.

  - 그 원인은 시계열 데이터에서 시간적으로 멀리 떨어진, 장기 의존 관계를 잘 학습할 수 없다는 데 있습니다.

- 그래서 요즘에는 앞 장의 단순한 RNN 대신에 LSTM이나 GRU라는 계층이 주로 쓰이고 실제로 LSTM이나 GRU를 그냥 'RNN'이라고 합니다.

  - 오히려 RNN을 명시적으로 '기본적인 RNN'이라고 부릅니다.

- LSTM이나 GRU에는 '게이트'라는 구조가 더해져있어 시계열 데이터의 장기 의존 관계를 학습할 수 있습니다.

- 이번 장에서는 앞 장에서 설명한 기본 RNN의 문제점을 알아보고 이를 대신하는 계층으로써 LSTM과 GRU같은 '게이트가 추가된 RNN'을 소개합니다.

  - 특히 LSTM의 구조를 시간을 들여 차분히 살펴보고, 이 구조가 '장기 기억'을 가능하게 하는 매커니즘을 이해해봅니다.

  - 그리고 LSTM을 사용해 언어 모델을 만들어, 실제로 데이터를 잘 학습하는 모습도 알아보겠습니다.

## 6.1 RNN의 문제점

- 앞 장에서 설명한 RNN은 시계열 데이터의 장기 의존 관계를 학습하기 어렵습니다.

  - 그 원인은 BPTT에서 기울기 소실 혹은 기울기 폭발이 일어나기 때문입니다.

- 이번 절에서는 앞 장에서 배운 RNN 계층을 복습하고, 뒤이어 RNN 계층이 장기 기억을 제대로 처리하지 못하는 이유를 알아보겠습니다.

### 6.1.1 RNN 복습

- RNN 게층은 순환 경로를 가지고 있는데 이 순환을 펼치면 다음 그림과 같이 옆으로 길게 뻗은 신경망이 됩니다.

<img src="README.assets/fig 6-1.png" alt="fig 6-1" style="zoom:50%;" />

- 위 그림에서 보듯, RNN 계층은 시계열 데이터 x를 입력하면 h를 출력합니다. 이 h는 RNN 계층의 **은닉 상태**라고 하며 과거 정보를 저장합니다.

- RNN의 특징은 바로 이전 시각의 은닉 상태를 이용한다는 점인데 이렇게 해서 과거 정보를 계승할 수 있습니다.

  - 이 때 RNN 계층이 수행하는 처리를 계산 그래프로 나타나매녀 다음과 같습니다.

  <img src="README.assets/fig 6-2.png" alt="fig 6-2" style="zoom:50%;" />

  - 위 그림처럼 RNN 계층의 순전파는 행렬의 곱과 합, 활성화 함수 tanh 함수에 의한 계산을 수행합니다.

- 이상이 앞 장에서 본 RNN 계층입니다. 이어서 RNN 계층이 안고 있는 문제, 즉 장기 기억에 취약하다는 문제를 살펴보겠습니다.

### 6.1.2 기울기 소실 또는 기울기 폭발

- 언어 모델은 주언진 단어들을 기초로 다음에 출현할 단어를 예측하는 일을 합니다.

  - 앞 장에서는 RNN을 사용해 언어 모델을 구현해 RNNLM이라 했는데 이번 절에서는 그 RNNLM의 단점을 확인해보겠습니다.

<img src="README.assets/fig 6-3.png" alt="fig 6-3" style="zoom:50%;" />

- 위 그림의 정답은 'Tom'입니다. RNNLM이 이 문제를 올바르게 맞히려면 'Tom이 방에서 TV를 보고 있다.'는 것과 '그 방에 Mary가 들어갔다.'는 것을 알아야 합니다.

  - 다시 말해 이런 정보를 RNN 계층의 은닉 상태에 인코딩해 보관해둬야 합니다.

- 그럼 이 예를 RNNLM 학습의 관점에서 생각해보겠습니다.

  - 여기에서는 정답 레이블로 'Tom'이라는 단어가 주어졌을 때, 기울기가 어떻게 전파되는지를 살펴보겠습니다.

  - 학습은 BPTT로 수행하며 정답 레이블이 'Tom'이라고 주어진 시점으로부터 과거 방향으로 기울기를 전달합니다.

<img src="README.assets/fig 6-4.png" alt="fig 6-4" style="zoom:50%;" />

- 위 그림과 같이 정답 레이블이 'Tom'임을 학습할 때 중요한 것이 바로 RNN 계층의 존재입니다.

  - RNN 계층이 과거 방향으로 '의미 있는 기울기'를 전달함으로써 시간 방향의 의존 관계를 학습할 수 있는 것입니다.

- 이때 기울기는 원래대로라면 학습해야 할 의미가 있는 정보가 들어 있고 그것을 과거로 전달해 장기 의존 관계를 학습합니다.

  - 하지만 만약 이 기울기가 중간에 사그라들어 거의 아무런 정보도 남지 않는다면 가중치 매개변수는 거의 갱신되지 않습니다.

  - 즉, 장기 의존 관계를 학습할 수 없게 됩니다.

- 안타깝지만, 현재의 단순한 RNN 계층에서는 시간을 거슬러 올라갈수록 기울기가 작아지거나 커질 수가 있으며 이는 거의 대부분의 상황에서 일어납니다.

### 6.1.3 기울기 소실과 기울기 폭발의 원인

- 그럼 RNN 계층에서 기울기 소실 혹은 기울기 폭발이 일어나는 원인을 살펴보겠습니다.

  - 다음과 같은 RNN 계층에서 시간 방향 기울기 전파에 주목하세요.

<img src="README.assets/fig 6-5.png" alt="fig 6-5" style="zoom:50%;" />

- 위 그림에서 길이가 T인 시계열 데이터를 가정해 T번째 정답 레이블에서 전해지는 기울기가 어떻게 변하는지 보겠습니다.

  - 앞의 문제에 대입하면 T번째 정답 레이블이 'Tom'인 경우로 이 때 시간 방향 기울기가 역전파로 전해질 때 'tanh', '+', 'MatMul' 연산을 거칩니다.

- '+'의 역전파는 상류에서 전해지는 기울기를 그대로 하류로 흘려보내므로 기울기가 변하지 않습니다.

  - 그러면 나머지 두 연산인 'tanh'와 'MatMul'이 변화시키는 기울기를 보겟습니다.

    - 'tanh'의 경우 'Appedix A. Differential of Sigmoid and tanh function'에서 자세히 나오겠지만 tanh 함수의 값과 미분 값은 다음과 같습니다.

    <img src="README.assets/fig 6-6.png" alt="fig 6-6" style="zoom:50%;" />

    - 점선이 tanh 함수으 미분인데 그 값이 1 이하이며 x가 0에서 멀어질수록 작아집니다.

    - 다르게 말해 역전파에서 기울기가 tanh를 지날 때마다 값이 계속 작아진다는 것이며 tanh 함수를 T번 통과하면 기울기도 T번 작아집니다.

#### Note

> RNN 계층의 활성화 함수로는 주로 tanh 함수를 사용하는 데 이를 ReLU로 바꾸면 기울기 소실을 줄일 수 있습니다.
>
> 그 이유는 ReLU는 입력이 0 이상이면 역전파일 때 상류의 기울기를 그대로 하류로 보내기 때문으로 기울기가 작아지지 않습니다.

- 다음은 MatMul 노드 차례입니다.

  - 이야기를 단순하게 하기 위해 tanh 노드를 무시하면 RNN 계층의 역전파의 기울기는 'MatMul'에 의해서만 변합니다.

<img src="README.assets/fig 6-7.png" alt="fig 6-7" style="zoom:50%;" />

- 위 그림은 상류에서 dh가 흘러올 때 MatMul 노드에서 역전파로 dhW<sub>h</sub><sup>T</sup> 행렬 곱으로 기울기를 계산함을 보여줍니다.

  - 그리고 같은 계산을 시계열 데이터의 시간 크기만큼 반복하는 것인데 여기서 주목할 점은 매번 같은 기울기 W<sub>h</sub><sup>T</sup>가 행렬 곱에 사용된다는 것입니다.

- 그러면 역전파 시 기울기가 MatMul 노드를 지날 때 마다 어떻게 변하게 되는지 코드를 통해 살펴보겠습니다.

> 6.1.3_rnn_gradient_graph.py를 확인하세요.

- 이 코드는 dh를 np.ones()로 초기화한 뒤 역전파의 MatMul 노드 수 만큼 dh를 갱신하고 각 단계의 dh 크기(norm)을 norm_list에 추가합니다.

  - 여기에서는 미니배치(N개)의 평균 'L2 노름'을 구해 dh 크기로 사용하고 있는데 L2 노름이란 각 원소를 제곱해 모두 더하고 제곱근을 취한 것입니다.

<img src="README.assets/fig 6-8.png" alt="fig 6-8" style="zoom:50%;" />

- 결과를 위와 같은데 기울기의 크기가 시간에 비례해 지수적으로 증가하는데 이를 **기울기 폭발**이라고 합니다.

  - 이러한 기울기 폭발이 일어나면 결국 오버플로를 일으켜 NaN이 되어 신경망 학습을 제대로 수행할 수 없게 됩니다.

- 그러면 Wh의 초깃값을 다음과 같이 변경한 후 두 번째 실험을 해보겠습니다.

```python
# Wh = np.random.randn(H, H)를 변경합니다.
Wh = np.random.randn(H, H) * 0.5
```

<img src="README.assets/fig 6-9.png" alt="fig 6-9" style="zoom:50%;" />

- 위와 같이 이번에는 기울기가 지수적으로 감소하는데 이것이 **기울기 소실**입니다.

  - 기울기 소실이 일어나면 기울기가 매우 빠르게 작아지며 기울기가 일정 수준 이하로 작아집니다.

  - 그럴 경우 가중치 매개변수가 더 이상 갱신되지 않아 장기 의존 관계를 학습할 수 없습니다.

- 지금까지 결과는 기울기의 크기가 지수적으로 증가하거나 감소하는데 이는 행렬 Wh를 T번 반복해서 '곱'한 결과입니다.

  - 만약 Wh가 스칼라라면 더 이해하기 쉬운데 Wh가 1보다 크면 지수적으로 증가하고 1보다 작으면 지수적으로 감소합니다.

  - 하지만 이처럼 Wh가 스칼라가 아닌 행렬이라면 행렬의 '특잇값'이 척도가 됩니다.

    - 행렬의 특잇값은 데이터가 얼마나 퍼져 있는지를 나타내는 것으로 특잇값, 정확하게는 최대값이 1보다 큰지 여부를 보면 기울기가 변하는 척도를 예측할 수 있습니다.

#### Warning

> 특잇값의 최대값이 1보다 크면 지수적으로 증가, 1보다 작으면 감소할 가능성이 높습니다.
>
> 하지만 그럴 가능성이 높을 뿐 특잇값이 1보다 크다고 반드시 폭발하는 것은 아닙니다. 즉, 필요조건일 뿐 충분조건은 아닙니다.
>
> RNN의 기울기 소실과 기울기 폭발은 관련 논문을 더 참고하세요.

### 6.1.4 기울기 폭발 대책

- 지금까지 RNN의 문제점을 살펴보았는데 이제 이 해결책을 알아보겠습니다.

- 여기서는 우선 기울기 폭발의 해결책을 볼텐데 전통적인 기법인 **기울기 클리핑**이 있습니다.

<img src="README.assets/e 6-0.png" alt="e 6-0" style="zoom:50%;" />

- 신경망에서 사용되는 모든 매개변수에 대한 기울기를 하나로 처리한다고 가정하고 이를 기호 g로 표기하고 문턱값으로 threshold를 설정합니다.

  - 이때 기울기의 L2 norm(||g||)가 문턱값을 초과하면 두 번째 줄의 수식이 기울기를 수정하며 이를 기울기 클리핑이라고 합니다.

  - 이는 단순한 알고리즘이지만 많은 경우에 잘 작동합니다.

#### Warning

> g는 신경망에서 사용되는 모든 매개변수의 기울기를 하나로 모은 것입니다.
>
> 예를 들어 두 개의 가중치 W1과 W2 매개변수를 사용하는 모델은 기울기 dW1와 dW2를 모두 합한 것이 g입니다.

- 이제 기울기 클리핑을 Python으로 구현해보겠습니다.

  - 기울기 클리핑은 clip_grads 함수로 구현하고 인수 grads로 기울기 리스트, max_norm으로 문턱값을 받습니다.

> 자세한 내용은 6.1.4_clip_grads.py를 확인하세요.

- 이것이 기울기 클리핑의 구현인데 clip_grads 함수는 자주 사용되므로 commons/util.py에 같은 구현이 준비되어 있습니다.

#### Note

> 이 책에서는 RNNLM을 학습시키는 RnnlmTrainer 클래스를 제공합니다.
>
> 이 클래스 내부에서도 기울기 클리핑 함수를 사용해 기울기 폭발을 방지합니다.
>
> RnnlmTrainer 클래스의 기울기 클리핑은 '6.4 LSTM을 사용한 언어 모델'절에서 다시 설명합니다.

- 이것으로 기울기 클리핑에 대한 설명을 마치고 이제 기울기 소실의 대책에 대해 알아보겠습니다.

## 6.2 기울기 소실과 LSTM

- RNN 학습에서는 기울기 소실도 큰 문제입니다. 그리고 이 문제를 해결하려면 RNN 계층의 아키텍처를 근본부터 뜯어고쳐야 합니다.

- 여기서 등장하는 것이 이번 장의 핵심 주제인 '게이트가 추가된 RNN'입니다.

  - 게이트가 추가된 RNN으로는 많은 아키텍처가 제안되어 있고, 대표적으로 LSTM과 GRU가 있습니다.

- 이번 절에서는 LSTM에 집중하여 그 구조를 차분히 살펴보면서 LSTM이 기울기 소실을 일으키지 않거나 일으키기 어렵게 한다는 점을 알아보고 GRU는 'Appedix C. GRU'에서 알아보겠습니다.

### 6.2.1 LSTM의 인터페이스

- LSTM을 자세히 살펴보기 전에 계산 그래프를 단순화하는 도법을 하나 도입하겠습니다. 이 도법은 행렬 계산 등을 하나의 직사각형 노드로 정리해 그리는 방식입니다.

<img src="README.assets/fig 6-10.png" alt="fig 6-10" style="zoom:50%;" />

- 위 그림은 tanh와 행렬 곱의 연산을 tanh라는 직사각형 노드 하나로 그렸습니다.

  - 이 직사각형 노드 안에 행렬 곱과 편향의 합, tanh 함수에 의한 변환이 모두 포함된 것입니다.

- 사전 준비는 이것으로 끝이며 이제 LSTM의 인터페이스를 RNN과 비교하는 것부터 시작하겠습니다.

<img src="README.assets/fig 6-11.png" alt="fig 6-11" style="zoom:50%;" />

- 위 그림에서 보듯 LSTM 계층의 인터페이스에는 c라는 경로가 있다는 차이가 존재합니다. 그리고 이 c를 **기억 셀**이라 하며 이는 LSTM 전용 기억 매커니즘입니다.

- 기억 셀의 특징은 데이터를 자기 자신으로만, 즉 LSTM 계층 내에서만 주고받는다는 것입니다.

  - 즉, LSTM 계층 내에서만 완결되고, 다른 계층으로는 출력하지 않습니다.

  - 반면, LSTM의 은닉 상태 h는 RNN 계층과 마찬가리고 다른 계층, 즉 위쪽으로 출력됩니다.

#### Note

> LSTM의 출력을 받는 쪽에서 볼 때 LSTM의 출력은 은닉 상태 벡터 h뿐입니다.
>
> 따라서, 기억 셀 c는 외부에서 보이지 않으므로 존재 자체를 생각할 필요가 없습니다.

### 6.2.2 LSTM 계층 조립하기

- 이제 LSTM 계층의 내용을 구성을 하나하나 알아가며 살펴보겠습니다.

- LSTM에는 기억 셀 c<sub>t</sub>가 있으며 이 곳에는 시각 t에서의 LSTM의 기억이 저장되어 있고 과거부터 시각 t까지 모든 정보가 저장되어 있습니다.

  - 그리고 필요한 정보를 모두 기억한 이 기억을 바탕으로 외부 계층과 다음 시각의 LSTM에 은닉 상태 h를 출력합니다.

  - 이 때 출력하는 h는 기억 셀의 값을 tanh 함수로 변환한 값입니다.

<img src="README.assets/fig 6-12.png" alt="fig 6-12" style="zoom:50%;" />

- 위 그림처럼 현재의 기억 셀 c<sub>t</sub>는 3개의 입력으로부터 '어떤 계산'을 수행하여 구할 수 있습니다.

- 여기서 핵심은 갱신된 c<sub>t</sub>를 사용해 은닉 상태 h<sub>t</sub>를 계산한다는 것이고 이 계산에 tanh가 들어가므로 c<sub>t</sub>의 각 요소에 tanh 함수를 적용한다는 것입니다.

#### Warning

> 기억 셀 c<sub>t</sub>와 은닉 상태 h<sub>t</sub>의 관계는 c<sub>t</sub>의 각 원소에 tanh 함수를 적용한 것일 뿐입니다.
>
> 즉, 이는 기억 셀과 은닉 상태의 원소 수가 같다는 것을 의미합니다.

- 이어서 설명하기 전에 '게이트'의 기능에 대해 간단히 설명하겠습니다.

- 게이트는 '문'으로 문을 열고 닫는 것 처럼 데이터의 흐름을 제어합니다.

<img src="README.assets/fig 6-13.png" alt="fig 6-13" style="zoom:50%;" />

- LSTM에서 사용하는 게이트는 단순히 열고 닫는 것뿐 아니라 어느 정도 열지를 조절할 수 있습니다.

  - 다시 말해, 다음 단계로 흘려보낼 물의 양을 제어할 수 있는데 이 정도를 '열림 상태'라고 부르며 다음과 같습니다.

<img src="README.assets/fig 6-14.png" alt="fig 6-14" style="zoom:50%;" />

- 게이트의 열심 상태는 0과 1 사이의 실수로 나타나고 이 값이 다음으로 흐르는 물의 양을 결정합니다.

  - 여기서 중요한 것은 얼마나 열릴지에 대한 것도 데이터로부터 자동으로 학습한다는 점입니다.

#### Note

> 게이트는 게이트의 열림 상태를 제어하기 위해 전용 가중치 매개변수를 이용하며 이 가중치 매개변수는 학습 데이터로부터 갱신됩니다.
>
> 참고로, 게이트의 열림 상태를 구할 때는 시그모이드 함수를 사용하는 데 이 함수의 출력이 0과 1 사이의 실수이기 때문입니다.

### 6.2.3 output 게이트

- 다시 LSTM으로 돌아오면 앞서 은닉 상태 h<sub>t</sub>는 기억 셀 c<sub>t</sub>에 단순히 tanh 함수를 적용했을 뿐이라고 했습니다.

- 이번 절에서는 tanh(c)에 게이트를 적용하는 것, 즉 tanh(c)의 각 원소에 대해 '그것이 다음 시각의 은닉 상태에 얼마나 중요한가'를 조정해보겠습니다.

  - 한편, 이 게이트는 다음 은닉 상태 h<sub>t</sub>의 출력을 담당는 데이트 이므로 **output 게이트**라고 합니다.

- output 게이트의 열림 상태는 입력 x<sub>t</sub>와 이전 상태 h<sub>t-1</sub>로부터 구합니다.

  - 이 때 계산은 다음과 같습니다.

  <img src="README.assets/e 6-1.png" alt="e 6-1" style="zoom:50%;" />

  - 여기서 사용하는 가중치 매개변수와 편향에는 output을 의미하는 o를 첨자로 추가하고 이후에도 마찬가지로 첨자를 붙여 게이트임을 표시하겠습니다.

  - 한편, 이 식을 감싸고 있는 함수는 시그모이드 함수로 이와 같이 표현하겠습니다.

- 위 식과 같이 입력 x<sub>t</sub>에는 가중치 W<sub>x</sub><sup>(o)</sup>가, 이전 시각의 은닉 상태 h<sub>t-1</sub>에는 가중치 W<sub>h</sub><sup>(o)</sup>가 붙습니다.

  - 그리고 이 핼렬들의 곱에 편향 b<sup>(o)</sup>를 더한 다음 시그모이드 함수를 거쳐 출력 게이트의 출력 o를 구합니다.

  - 마지막으로 이 o와 tanh(c)의 원소별 곱을 h<sub>t</sub>로 출력하는 것입니다. 이들의 계산 그래프는 다음과 같습니다.

  <img src="README.assets/fig 6-15.png" alt="fig 6-15" style="zoom:50%;" />

- 위 그림과 같이 output 게이트에서 수행하는 식을 시그모이드 함수 표식을 써서 표기하겠습니다.

  - 그리고 이 출력을 o라고 할 깨 h<sub>t</sub>는 o와 tanh(c)의 곱인데 여기서 곱은 원소별 곱으로 이를 **아다마르 곱**이라고 합니다.

  <img src="README.assets/e 6-2.png" alt="e 6-2" style="zoom:50%;" />

- 이상이 LSTM의 output 게이트로 이것으로 LSTM의 출력 부분이 완성되었습니다. 이어서 기억 셀 갱신 부분을 살펴보겠습니다.

#### Warning

> tanh의 출력은 -1과 1 사이의 실수로 이 수치가 그 안에 인코딩된 '정보'의 강약을 표시한다고 해석할 수 있습니다.
>
> 한편 시그모이드 함수의 출력은 0과 1 사이의 실수로 데이터를 얼마큼 통과시킬지를 정하는 비율을 뜻합니다.
>
> 따라서 주로 게이트에서는 시그모이드 함수가, 실질적인 '정보'를 지니는 데이터에는 tanh 함수가 활성화 함수로 사용됩니다.

### 6.2.4 forget 게이트

- 망각은 더 나은 전진을 낳습니다. 따라서 우리가 다음에 해야할 일은 기억 셀에 '무엇을 잊을까'를 명확하게 지시하는 것이며 이는 물론 게이트를 사용해 해결합니다.

- 그러면 c<sub>t-1</sub>의 기억 중에서 불필요한 기억을 잊게 해주는 게이트를 추가하겠으며 이를 **forget 게이트**라고 하겠습니다. 이 게이트를 LSTM에 추가한 계산 그래프는 다음과 같습니다.

<img src="README.assets/fig 6-16.png" alt="fig 6-16" style="zoom:50%;" />

- forget 게이트의 연산도 시그모이드 함수의 표식을 써서 표기했고 이 곳에는 forget 게이트 전용 가중치 매개변수가 있습니다.

  - forget 게이트에서 수행하는 계산은 다음과 같습니다.

  <img src="README.assets/e 6-3.png" alt="e 6-3" style="zoom:50%;" />

- 위 식을 통해 forget 게이트의 출력 f가 구해집니다. 그리고 이 f에 이전 기억 셀 c<sub>t-1</sub>과의 원소별 곱을 통해 c</sub>t</sub>을 구합니다.

### 6.2.5 새로운 기억 셀

- forget 게이트를 거치면서 이전 시각의 기억 셀로부터 잊어야 할 기억이 삭제되었지만 이 상태로는 기억 셀이 잊는 것 밖에 하지 못합니다.

  - 따라서 새로 기억해야 할 정보를 기억 셀에 추가해야하며 그를 위해 tanh 노드를 추가합니다.

  <img src="README.assets/fig 6-17.png" alt="fig 6-17" style="zoom:50%;" />

- 이처럼 tanh 노드가 계산한 결과가 이전 시각의 기억셀 c<sub>t-1</sub>에 더해져 기억 셀에 새로운 '정보'가 추가됩니다.

  - 이 tanh 노드는 '게이트'가 아니며 새로운 '정보'를 기억 셀에 추가하는 것이 목적이라 활성화 함수로 시그모이드 함수가 아닌 tanh 함수가 사용됩니다.

  <img src="README.assets/e 6-4.png" alt="e 6-4" style="zoom:50%;" />

- 여기에서 기억 셀에 추가되는 새로운 기억을 g로 표기했으며 이 g가 이전 시각의 기억 셀인 c<sub>t-1</sub>에 더해져 새로운 기억이 생겨납니다.

### 6.2.6 input 게이트

- 마지막으로 g에 게이트 하나가 추가될 예정인데 이 게이트를 **input 게이트**라고 하겠습니다.

<img src="README.assets/fig 6-18.png" alt="fig 6-18" style="zoom:50%;" />

- input 게이트는 g의 각 원소가 새로 추가되는 정보로써의 가치가 얼마나 큰지를 판단합니다.

  - 즉, 새 정보를 무비판적으로 수용하는 게 아니라 적절히 취사선택하는 것이 이 게이트의 역할입니다.

  - 다른 관점에서 보면, input 게이트에 의해 가중된 정보가 새로 추가되는 셈입니다.

- 여기서 input 게이트를 시그모이드 표식으로, 그 출력을 i로 표기했으며 그 계산은 다음과 같습니다.

<img src="README.assets/e 6-5.png" alt="e 6-5" style="zoom:50%;" />

- 그런 다음 i와 g의 원소별 곱 결과를 기억 셀에 추가하는 것이 LSTM 안에서 이뤄지는 처리입니다.

#### Warning

> LSTM에는 '변종'이 몇 가지 있습니다.
>
> 지금까지 설명한 LSTM이 대표적인 LSTM이지만, 이 외에도 게이트 연결 방법이 약간 다른 계층도 볼 수 있습니다.

### 6.2.7 LSTM의 기울기 흐름

- LSTM의 구조는 설명했지만 이것이 어떤 원리로 기울기 소실을 없애주는지는 기억 셀 c의 역전파에 주목하면 됩니다.

<img src="README.assets/fig 6-19.png" alt="fig 6-19" style="zoom:50%;" />

- 위 그림은 기억 셀에만 집중해 그 역전파의 흐름을 그린 것입니다. 이 때 기억 셀의 역전파는 '+'와 'x' 노드만을 지나게 됩니다.

  - '+' 노드는 상류에서 전해지는 기울기를 그대로 흘릴 뿐이므로 기울기 변화(감소)는 일어나지 않습니다.

  - 남은 건 'x' 노드로 이 노드는 '행렬 곱'이 아닌 '원소별 곱'을 계산합니다.

    - 앞에서 본 RNN의 역전파에서는 똑같은 가중치 행렬을 사용해 '행렬 곱'을 반복해 기울기 소실 혹은 기울기 폭발이 일어났습니다.

    - 반면에 LSTM의 역전파에서는 '행렬 곱'이 아닌 '원소별 곱'이 이뤄지고 매 시각 다른 게이트 값을 이용해 원소별 곱을 계산합니다.

    - 즉, 매번 새로운 게이트 값을 이용하므로 곱셈 효과가 누적되지 않아 기울기 소실이 일어나지 않거나 일어나기 어려운 것입니다.

  - 또한, 'x' 노드의 계산은 forget 게이트가 제어하고 매 시각 다른 게이트 값을 출력합니다.

    - 그래서 forget 게이트가 '잊어야 한다'고 판단한 기억 셀의 원소는 기울기가 작아지고 '잊어서는 안 된다'고 판단한 원소에 대해서는 그 기울기가 약화되지 않은 채로 과거 방향으로 전해집니다.

  - 따라서, 기억 셀의 기울기가 특히 오래 기억해야 할 정보일 경우 소실 없이 전파되리라 기대할 수 있습니다.

- 이상이 LSTM의 기억 셀에서는 기울기 소실이 일어나지 않거나 일어나기 어려운 이유입니다. 따라서 기억 셀이 장기 의존 관계를 유지하거나 학습하리라 기대할 수 있습니다.

#### Warning

> LSTM은 Long Short-Term Memory의 약어로 **단기 기억**을 **긴** 시간 지속할 수 있음을 의미합니다.

## 6.3 LSTM 구현

- 그럼 LSTM을 구현해보겠습니다. 우선 최초 한 단계만 처리하는 LSTM 클래스를 구현하고 이어서 T개의 단계를 한 번에 처리하는 Time LSTM 클래스를 구현하겠습니다.

  - LSTM에 수행하는 계산을 정리한 수식들은 다음과 같습니다.

<img src="README.assets/e 6-6.png" alt="e 6-6" style="zoom:50%;" />

<img src="README.assets/e 6-7.png" alt="e 6-7" style="zoom:50%;" />

<img src="README.assets/e 6-8.png" alt="e 6-8" style="zoom:50%;" />

- 위 식들이 LSTM에서 수행하는 계산인데 주목할 부분은 식 6.6에 포함된 **아핀 변환**입니다.

  - 아핀 변환이란 행렬 변환과 평행 이동을 결합한 형태, 즉 식 6.6의 형태를 의미합니다.

- 식 6.6의 네 수식에서는 아핀 변환을 개별적으로 수행하지만 이를 하나의 식으로 정리해 계산할 수 있습니다. 그 과정은 다음과 같습니다.

<img src="README.assets/fig 6-20.png" alt="fig 6-20" style="zoom:50%;" />

- 이처럼 4개의 가중치 혹은 편향을 하나로 모을 수 있고 그 결과 원래 개별적으로 총 4번 수행하던 아핀 변환을 단 1회의 계산으로 끝마칠 수 있습니다.

  - 이 경우 계산 속도가 향상되는데 이는 행렬 라이브러리가 '큰 행렬'을 한 번에 모아 계산할 때가 각각 계산할 때보다 빠르기 때문입니다.

  - 또한, 가중치를 한 데로 모아 관리하므로 소스 코드도 간결해집니다.

- 그러면 **W<sub>x</sub>**, **W<sub>h</sub>**, **b** 각각에 4개분의 가중치 혹은 편향이 포함되어 있다고 가정하고 이 때 LSTM을 계산 그래프로 그려보면 다음과 같습니다.

<img src="README.assets/fig 6-21.png" alt="fig 6-21" style="zoom:50%;" />

- 위 그림과 같이 처음에 4개분의 아핀 변환을 한꺼번에 수행하고 그 결과를 slice 노드를 통해 4개의 결과를 꺼냅니다.

  - slice는 아핀 변환의 결과를 균등하게 네 조각으로 나눠서 꺼내주는 단순한 노드입니다.

  - slice 노드 다음에는 활성화 함수(시그모이드 혹인 tanh 함수)를 거쳐 앞 절에서 설명한 계산을 수행합니다.

- 그럼 이를 참고해서 LSTM 클래스를 구현하겠습니다. 우선 LSTM 클래스의 초기화 코드입니다.

> 전체적인 코드는 chapter06/commons/time_layers.py의 LSTM 클래스를 확인하세요.

```python
class LSTM:
    def __init__(self, Wx, Wh, b):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None
```

- 초기화 인수로 가중치 매개변수 Wx, Wh와 편향 b를 받습니다. 각 인수는 4개분의 가중치가 담겨 있습니다.

  - 이 인수들을 인스턴스 변수 params에 할당하고 이에 대응하는 형태로 기울기도 초기화합니다.

  - 한편 cache는 순전파 때 중간 결과를 보관했다가 역전파 계산에 사용하는 인스턴스 변수입니다.

- 계속해서 순전파 구현을 보겠습니다. 순전파는 forward(x, h_prev, c_prev) 메서드로 구현하며 인수로 현재 시각의 입력 x, 이전 시각의 은닉 상태 h_prev, 이전 시각의 기억 셀 c_prev를 받습니다.

```python
    def forward(self, x, h_prev, c_prev):
        Wx, Wh, b = self.params
        N, H = h_prev.shape

        A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b

        # slice를 진행합니다.
        f, g, i, o = A[:, :H], A[:, H:2 * H], A[:, 2 * H:3 * H], A[:, 3 * H:]
        f, g, i, o = sigmoid(f), np.tanh(g), sigmoid(i), sigmoid(o)

        c_next = f * c_prev + g * i
        h_next = o * np.tanh(c_next)

        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)
        return h_next, c_next
```

- 이 메서드에서는 가장 먼저 아핀 변환을 합니다. 이 때 인스턴스 변수 Wx, Wh, b는 각각 4개분의 매개변수가 저장되어 있으므로 행렬의 형상이 다음과 같아집니다.

<img src="README.assets/fig 6-22.png" alt="fig 6-22" style="zoom:50%;" />

- 미니배치 수를 N, 입력 데이터의 차원 수를 D, 기억 셀과 은닉 상태의 차원 수를 H로 표시했을 때 계산 결과인 A에 네 개분의 아핀 변환 결과가 저장됩니다.

  - 따라서 이 결과에서 데이터를 꺼낼 때 H 간격으로 슬라이스해서 꺼내고 이 데이터를 다음 연산 노드에 분배합니다.

  - 나머지 구현도 LSTM의 수식과 계산 그래프를 참고하면 특별히 어려운 부분은 없을 것입니다.

#### Warning

> LSTM 계층은 4개분의 가중치를 하나로 모아서 보관합니다.
>
> 그 덕분에 LSTM 계층은 매개변수를 총 3개(Wx, Wh, b)만 관리하면 됩니다.
>
> 참고로 RNN 계층도 동일하게 Wx, Wh, b라는 3개의 매개변수만 사용하지만 LSTM과 RNN 계층의 매개변수 형상이 다릅니다.

- LSTM의 역전파는 그림 6-21의 계산 그래프를 역방향으로 전파해 구할 수 있습니다.

  - 지금까지 익힌 지식을 활용하면 어렵지 않겠지만 slice 노드는 처음 등장했으니 그 역전파에 대해 알아볼 필요가 있습니다.

- slice 노드는 행렬을 네 조각으로 나눠서 분배했습니다. 따라서 그 역전파에서는 반대로 4개의 기울기를 결합해야 합니다.

<img src="README.assets/fig 6-23.png" alt="fig 6-23" style="zoom:50%;" />

- 그림 6-23에서 보듯 slice 노드의 역전파에서는 4개의 행렬을 연결합니다. 그림에서는 4개의 기울기(**df, dg, di, do**)를 연결해 **dA**로 만들었습니다.

  - 이를 numpy로 수행하려면 np.hstack() 메서드를 사용하면 됩니다. 이 메서드는 인수로 주어진 배열들을 가로로 연결합니다.(세로로 연결하려면 vstack()을 사용합니다.)

  - 따라서 이 처리는 다음 한 줄로 정리할 수 있습니다.

  ```python
  dA = np.hstack((df, dg, di, do))
  ```

- 이상으로 slice 노드의 역전파를 알아봤습니다.

### 6.3.1 Time LSTM 구현

- 계속해서 Time LSTM 구현으로 넘어가겠습니다. Time LSTM은 T개분의 시계열 데이터를 한꺼번에 처리하는 계층입니다. 전체적인 그림은 다음과 같습니다.

<img src="README.assets/fig 6-24.png" alt="fig 6-24" style="zoom:50%;" />

- 그런데 앞서 말한 것처럼 RNN에서는 학습할 때 Truncated BPTT를 수행합니다.

  - Truncated BPTT는 역전파의 연결은 적당한 길이로 끊지만 순전파의 흐름은 그대로 유지합니다.

  - 그러므로 다음 그림처럼 은닉 상태와 기억 셀을 인스턴스 변수로 유지하도록하여 다음번에 forward()가 호출되었을 때 이전 시각의 은닉 상태와 기억 셀에서 시작할 수 있습니다.

<img src="README.assets/fig 6-25.png" alt="fig 6-25" style="zoom:50%;" />

- 이전에 이미 Time RNN 계층을 구현한 것처럼 Time LSTM 계층도 같은 요령으로 구현하면 됩니다.

> 전체 코드는 chapter06/commons/time_layers.py의 TimeLSTM 클래스를 확인하세요.

```python
class TimeLSTM:
    def __init__(self, Wx, Wh, b, stateful=False):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.layers = None
        self.h, self.c = None, None
        self.dh = None
        self.stateful = stateful

    def forward(self, xs):
        Wx, Wh, b = self.params
        N, T, D = xs.shape
        H = Wh.shape[0]

        self.layers = []
        hs = np.empty((N, T, H), dtype='f')

        if not self.stateful or self.h is None:
            self.h = np.zeros_like((N, H), dtype='f')
        if not self.stateful or self.c is None:
            self.c = np.zeros_like((N, H), dtype='f')

        for t in range(T):
            layer = LSTM(*self.params)
            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)
            hs[:, t, :] = self.h
            self.layers.append(layer)

        return hs

    def backward(self, dhs):
        Wx, Wh, b = self.params
        N, T, H = dhs.shape
        D = Wx.shape[0]

        dxs = np.empty((N, T, D), dtype='f')
        dh, dc = 0, 0

        grads = [0, 0, 0]
        for t in reversed(range(T)):
            layer = self.layers[t]
            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)
            dxs[:, t, :] = dx
            for i, grad in enumerate(layer.grads):
                grads[i] += grad

        for i, grad in enumerate(grads):
          self.grads[i][...] = grad

        self.dh = dh
        return dxs

    def set_state(self, h, c=None):
        self.h, self.c = h, c

    def reset_state(self):
        self.h, self.c = None, None
```

- LSTM은 은닉 상태 h와 함께 기억 셀 c도 이용하지만, TimeLSTM 클래스의 구현은 Time RNN 클래스와 흡사합니다.

  - 여기에서도 인수 stateful로 상태를 유지할지를 지정합니다.

- 이제 이 TimeLSTM을 이용해서 언어 모델을 만들어보겠습니다.
