# chapter03. word2vec

- 앞 장에 이어 이번 장의 주제도 단어의 분산 표현이지만 앞 장에서 '통계 기반 기법'을 사용한 것과 달리 더 강력한 '추론 기반 기법'을 사용할 것입니다.

- 이름에서 보듯 '추론 기반 기법'은 추론을 하는 기법으로 신경망을 사용하는데 그 신경망이 word2vec입니다.

  - 이번 장에서는 word2vec의 구조를 알아보고 이를 직접 구현해보며 확실하게 이해할 것입니다.

- 이번 장의 목표는 단순한 word2vec 구현하기로 처리 효율을 희생한 대신 이해하기 쉬운 word2vec입니다.

  - 따라서 큰 데이터셋에서는 어렵지만 작은 데이터셋은 문제없이 처리할 수 있습니다.

  - 또한, 다음 장에서 이 word2vec에 여러 개선사항을 더해 '진짜' word2vec을 구현할 예정입니다.

## 3.1 추론 기반 기법과 신경망

- 단어를 벡터로 표현하는 방법 중 가장 성공적인 기법은 크게 '통계 기반 기법'과 '추론 기반 기법'입니다.

  - 이 둘은 단어의 의미를 얻는 방식이 서로 크게 다르지만 그 배경에는 모두 분포 가설이 있습니다.

- 이번 절에서는 통계 기반 기법의 문제를 지적하고 그 대인아니 추론 기반 기법의 이점을 거시적인 관점에서 설명합니다.

  - 또한 word2vec의 전처리를 위한 신경망으로 '단어'를 처리하는 예시를 살펴볼 것입니다.

### 3.1.1 통계 기반 기법의 문제점

- 지금까지 본 것처럼 통계 기반 기법은 주변 단어의 빈도를 기초로 단어를 표현합니다.

  - 구체적으로 단어의 동시발생 행렬을 만들고 그 행렬에 SVD를 적용하여 밀집 벡터로 재구성합니다. 하지만 이 방식은 대규모 말뭉치를 다룰 때 문제가 발생합니다.

- 현실에서 다루는 말뭉치의 어휘 수는 어마어마합니다. 예컨대 영어의 어휘 수는 100만을 훌쩍 넘는데 어휘가 100만 개라면 통계 기반 기법에서는 그 제곱의 행렬을 만들게 됩니다. 이는 비현실적입니다.

#### Note

> SVD를 n by n 행렬에 적용하는 비용은 O(n<sup>3</sup>)입니다.
>
> 이는 계산 시간이 n의 세제곱에 비례한다는 뜻으로 슈퍼컴퓨터를 동원해도 처리하기 힘든 수준입니다.
>
> 실제로 근사적 기법과 희소행렬의 성질 등으로 이를 개선할 수 있지만 그렇다고 해도 여전히 상당한 컴퓨팅 자원을 소모합니다.

- 통계 기반 기법은 말뭉치 전체의 통계를 이용해 단 1회의 처리만에 단어의 분산 표현을 얻습니다.

- 한편, 추론 기반 기법에서는 신경망을 이용한다면 미니배치로 학습하는 것이 일반적입니다.

  - 미니배치 학습에서는 신경망이 한 번에 소량의 학습 샘플씩 반복해서 학습하여 가중치를 갱신해갑니다.

  <img src="README.assets/fig 3-1.png" alt="fig 3-1" style="zoom:50%;" />

- 위의 그림처럼 통계 기반 기법은 학습 데이터를 한 번에 처리합니다.(배치 학습) 반면에 추론 기반 기법은 학습 데이터의 일부를 사용해 순차적으로 학습합니다.(미니배치 학습)

  - 이는 말뭉치의 어휘 수가 많아 SVD 등 계산량이 큰 작업을 처리하기 어려운 경우에도 신경망을 학습시킬 수 있다는 것으로 데이터를 작게 나눠 학습시킵니다.

  - 게다가 여러 GPU를 사용한 병렬 계산도 가능하므로 학습 속도를 높일 수도 있습니다.

  - 추론 기반 기법이 통계 기반 기법보다 매력적인 점은 이 외에도 여러 가지가 있으며 이는 추론 기반 기법을 학습한 뒤 '3.5.3 통계 기반 vs 추론 기반'에서 알아보겠습니다.

### 3.1.2 추론 기반 기법 개요

- 추론 기반 기법은 '추론'이 주된 작업인데 추론은 주변 단어가 주어졌을 때 빈 칸에 들어갈 단어를 추측하는 작업입니다.

<img src="README.assets/fig 3-2.png" alt="fig 3-2" style="zoom:50%;" />

- 이처럼 추론 문제를 풀고 학습하는 것이 '추론 기반 기법'이 다루는 문제로 이 문제를 반복해서 풀면서 단어의 출현 패턴을 학습합니다.

  - 이를 '모델 관점'에서 바라보면 다음과 같습니다.

<img src="README.assets/fig 3-3.png" alt="fig 3-3" style="zoom:50%;" />

- 이처럼 추론 기반 기법에는 어떤 모델이 등장하는데 이 모델로 신경망을 사용할 것입니다.

  - 모델은 맥락의 정보를 입력받아 출현할 수 있는 각 단어의 출현 확률을 출력하고 그 안애서 말뭉치를 사용해 올바른 추측을 내릴 수 있도록 학습합니다.

  - 그리고 그 학습의 결과로 단어의 분산 표현을 얻는 것이 추론 기반 기법의 전체 그림입니다.

#### Note

> 추론 기반 기법도 통계 기반 기법처럼 분포 가설이 존재합니다.
>
> 분포 가설이란 '단어의 의미는 주변 단어에 의해 형성된다'는 가설로 추론 기반 기법에서는 이를 앞과 같은 추측 문제로 귀결시켰습니다.
>
> 이처럼 두 기법 모두 분포 가설에 근거하는 '단어의 동시발생 가능성'을 얼마나 잘 모델링하는가가 중요한 연구입니다.

### 3.1.3 신경망에서의 단어 처리

- 지금부터 신경망을 이용해 단어를 처리합니다만 신경망은 단어를 그래도 처리할 수 없으니 단어를 '고정 길이의 벡터'로 변환합니다. 이 방법이 **원핫 표현(벡터)**입니다.

  - 원핫 표현이란 벡터의 원소 중 하나만 1이고 나머지는 0인 벡터를 의미합니다.

- 원핫 표현은 자세하게 다음과 같이 'You say goodbye and I say hello'라는 문장이 있을 때 이를 다음과 같이 처리합니다.

<img src="README.assets/fig 3-4.png" alt="fig 3-4" style="zoom:50%;" />

- 이처럼 단어는 텍스트, 단어 ID 그리고 원핫 표현 형태로 나타낼 수 있고 단어를 원핫 표현으로 변환하는 방법은 다음과 같습니다.

  - 먼저 총 어휘 수만큼 원소를 갖는 벡터를 준비하고 인덱스가 단어 ID와 같은 원소를 1로 나머지를 0으로 설정합니다.

  - 그럼 단어를 고정 길이 벡터로 변환하면 신경망 입력층은 뉴런의 수를 고정할 수 있습니다.

<img src="README.assets/fig 3-5-1585116923977.png" alt="fig 3-5" style="zoom:50%;" />

- 위의 예시처럼 입력층의 뉴런은 총 7개이고 각 뉴런은 각 단어에 대응합니다. 이제 이를 사용해 단어를 벡터로 나타낼 수 있고 신경망을 구성하는 '계층'들을 벡터로 처리할 수 있습니다.

  - 다시 말해, 단어를 신경망으로 처리할 수 있다는 뜻으로 다음처럼 원핫 표현의 단어 하나를 완전연결계층을 통해 변환할 수 있습니다.

<img src="README.assets/fig 3-6.png" alt="fig 3-6" style="zoom:50%;" />

- 위 신경망은 완전연결계층이므로 각 노드가 이웃 층의 모든 노드와 연결되어 있습니다.

  - 이 화살표에는 가중치(매개변수)가 존재하며 입력층 뉴런과 가중치의 합이 은닉층 뉴런이 됩니다. 참고로 이번 장의 완전연결 계층에서는 편향을 생략합니다.

#### Note

> 편향을 이용하지 않는 완전연결계층은 '행렬 곱' 연산에 해당합니다.
>
> 그래서 이 책에서 완전연결계층은 1장에서 구현한 MatMul 계층과 같아집니다.
>
> 참고로 딥러닝 프레임워크들은 일반적으로 완전연결계층을 생성할 때 편향을 이용할지 선택할 수 있습니다.

- 위에서 본 그림에서 가중치를 명확히 표현하기 위해 바꿔 그리면 다음과 같습니다.

<img src="README.assets/fig 3-7.png" alt="fig 3-7" style="zoom:50%;" />

- 그럼 이를 코드로 살펴보고 넘어가겠습니다. 거두절미하고 이 완전연결 계층에 의한 변환은 python으로 다음과 같습니다.

```python
import numpy as np

c = np.array([[1, 0, 0, 0, 0, 0, 0]])  # 입력
W = np.random.randn(7, 3)  # 가중치
h = np.matmul(c, W)  # 중간 노드
print(h)
```

- 이 코드는 단어가 0인 단어를 원핫 표현으로 표현하고 완전연결계층을 통과시켜 변환하는 모습입니다.

  - 복습해보자면, 완전연결계층의 계산은 행렬곱으로 수행할 수 있고 이 연산은 np.matmul()이 해결합니다.

#### Warning

> 이 코드에서 입력 데이터 c의 차원 수(ndim)은 2입니다.
>
> 이는 미니배치 처리를 고려한 것으로 최초의 차원(0번째 차원)에 각 데이터를 저장합니다.

- 앞선 코드에서 주목할 부분은 c와 W의 행렬 곱 부분으로 c는 원핫 표현이라서 단어 ID에 대응하는 원소만 1이고 나머지는 0인 벡터입니다. 따라서 앞선 c와 W의 연산은 결국 가중치의 행벡터 하나를 '뽑아낸' 것과 같습니다.

<img src="README.assets/fig 3-8.png" alt="fig 3-8" style="zoom:50%;" />

- 그저 가중치에 행벡터 하나를 뽑아낼 뿐인데 행렬 곱을 계산하는 건 비효율적으로 보일 수 있는데 이는 '4.1 word2vec의 개선 1'에서 개선할 예정입니다. 다음처럼.

```python
from commons.layers import MatMul

c = np.array([[1, 0, 0, 0, 0, 0, 0]])
W = np,random.randn(7, 3)

layer = MatMul(W)
h = layer.forward(c)
print(h)
```

- 위 코드는 commons 디렉토리 안에 있는 MatMul 계층을 import하여 사용합니다. 그런 다음 해당 계층에 가중치 W를 설정하고 forward를 사용해 순전파를 계산합니다.

## 3.2 단순한 word2vec

- 앞 절에서는 추론 기반 기법을 배우고 신경망으로 단어를 처리하는 방법을 코드로 알아봤습니다. 이제 이번에는 word2vec을 구현해보겠습니다.

  - 이번 절에서 사용할 신경망은 word2vec에서 제안하는 **CBOW** 모델입니다.

#### Warning

> word2vec이라는 용어는 원래 프로그램이나 도구를 가리키는 데 사용되지만 용어가 유명해지며 신경망 모델을 가리키는 경우도 많아졌습니다.
>
> CBOW 모델과 skip-gram 모델은 word2vec에서 사용되는 신경망으로 이번 절에서는 CBOW를 이야기하며 두 모델의 차이는 '3.5.2 skip-gram 모델'에서 설명하겠습니다.

### 3.2.1 CBOW 모델의 추론 처리

- CBOW 모델은 맥락에서 타깃을 추측하는 용도의 신경망입니다. 이 모델이 가능한 정확하게 추론하도록 훈련하여 단어의 분산 표현을 얻는 것이 목표입니다.

- CBOW 모델의 입력은 맥락으로 앞서 본 'you'나 'goodbye'와 같은 단어들의 목록입니다.

  - 가장 먼저 이 맥락을 원핫 표현으로 변환하여 CBOW 모델이 처리할 수 있도록 준비합니다. 이는 다음과 같이 표현됩니다.

  <img src="README.assets/fig 3-9.png" alt="fig 3-9" style="zoom:50%;" />

- 이 그림이 CBOW 모델의 신경망으로 입력층이 2개에 은닉층을 거쳐 출력층에 도달하는 형태입니다.

  - 두 입력층에서 은닉층으로의 변환은 똑같은 완전연결게층이 처리하고 은닉층에서 출력층으로의 변환은 다른 완전연결계층이 처리합니다.

#### Warning

> 이 그림에서 입력층이 2개인 이유는 맥락으로 고려할 단어를 2개로 정했기 때문입니다.
>
> 즉, 맥락에 포함시킬 단어가 N개라면 입력층도 N개가 됩니다.

- 이제 이 모델의 은닉층에 주목하면 은닉층의 뉴런은 입력층의 완전연결계층에 의해 변환된 값이 되는데 입력층이 여러 개라면 전체를 '평균'하면 됩니다.

  - 앞선 예에 대입하면 완전연결계층에 의한 첫 번째 입력층이 h<sub>1</sub>이 되고 두 번째 입력층이 h<sub>2</sub>가 되면 은닉층은 0.5 \* (h<sub>1</sub> + h<sub>2</sub>)가 됩니다.

  - 마지막으로 출력층의 뉴런은 7개인데 이 뉴런 하나가 각 단어에 대응합니다.

    - 출력층의 뉴런은 각 단어의 '점수'를 뜻하며 값이 높을수록 대응 단어의 출현 확률도 높아집니다.

    - 여기서 점수란 확률로 해석되기 전의 값이고 이 점수에 소프트맥스 함수를 적용하면 '확률'을 얻을 수 있습니다.

#### Warning

> 점수를 Softmax 계층에 통과시킨 후의 뉴런을 출력층이라고 합니다. 이 책에서는 점수를 출력하는 층을 출력층이라고 하겠습니다.

- 이제 입력층에서 은닉층으로의 변환은 완전연결계층에 의해 이루어지는데 이 예시에서 완전연결계층의 가중치 W<sub>in</sub>은 7 by 3 행렬로 단어의 분산 표현의 정체입니다.

<img src="README.assets/fig 3-10.png" alt="fig 3-10" style="zoom:50%;" />

- 위 그림에서 보듯 가중치의 각 행에는 해당 단어의 분산 표현이 담겨 있습니다. 따라서 학습을 진행할수록 맥락에 출현하는 단어를 잘 추측하는 방향으로 분산 표현들이 갱신될 것입니다.

  - 그리고 이렇게 얻은 벡터에는 '단어의 의미'도 잘 녹아들어 있는데 이것이 word2vec의 전체 그림입니다.

#### Note

> 은닉층의 뉴런 수를 입력층의 뉴런 수보다 적게하는 것이 중요한 핵심입니다.
>
> 이렇게 해야 은닉층에는 단어 예측에 필요한 정보를 '간결하게' 담게 되며, 결과적으로 밀집벡터 표현을 얻을 수 있습니다.
>
> 이 때 은닉층의 정보는 사람이 이해할 수 없는 코드로 쓰여있는데 이것이 바로 '인코딩'입니다.
>
> 한 편 은닉층의 정보에서 원하는 결과를 얻는 작업은 '디코딩'입니다. 즉, 디코딩이랑 인코딩된 정보를 우리 인간이 이해할 수 있는 표현으로 복원하는 작업입니다.

- 지금까지 CBOW 모델을 '뉴런 관점'에서 그려왔다면 이제 이를 '계층 관점'에서 그려보겠습니다. 그 모습은 다음과 같습니다.

<img src="README.assets/fig 3-11.png" alt="fig 3-11" style="zoom:50%;" />

- 위 그림에서 보듯 CBOW 모델 가장 앞에는 2개의 MatMul 계층이 있고 이어 두 계층의 출력을 더한 뒤 0.5를 곱해 '평균'을 계산합니다. 이는 은닉층 뉴런이 됩니다.

  - 마지막으로 은닉층 뉴런에 또 다른 MatMul 계층이 적용되면 '점수'가 출력됩니다.

#### Warning

> 편향을 사용하지 않는 완전연결계층의 처리는 MatMul 계층의 순전파와 같습니다. 이 계층은 내부에서 행렬 곱을 계산합니다.

- 그럼 이를 참고해 CBOW 모델의 추론 처리를 python으로 구현하겠습니다.

> 3.2.1_cbow_predict.py를 확인하세요.

- 해당 파일의 과정이 CBOW 모델의 추론 과정입니다. 여기서 보듯 CBOW 모델은 활성화 함수를 사용하지 않는 간단한 구성의 신경망입니다.

  - 다만 차이는 입력층이 여러 개가 있고 입력층들의 가중치가 공유된다는 점입니다.

### 3.2.2 CBOW 모델의 학습

- 지금까지 설명한 CBOW 모델은 출력층에서 각 단어의 점수를 출력하는데 여기에 소프트맥스 함수를 적용하면 '확률'을 얻을 수 있습니다.

  - 이 확률은 맥락이 주어졌을 때 그 중앙에 어떤 단어가 출현하는지를 나타냅니다.

- 다음 예처럼 맥락이 'you'와 'goodbye'이고 정답 레이블이 'say'일 때 가중치가 적절히 설정된 신경망이라면 확률을 나타내는 뉴런들 중 정답에 해당하는 뉴런의 값이 클 것입니다.

<img src="README.assets/fig 3-12.png" alt="fig 3-12" style="zoom:50%;" />

- CBOW 모델의 학습은 올바른 예측을 할 수 있도록 가중치를 조정하는 일을 하는데 그 결과 가중치 W<sub>in</sub>에 단어의 출현 패턴을 파악한 벡터가 학습됩니다.

  - 물론 정확하게는 in, out 모두에 이를 기록하며 CBOW(+skip-gram) 모델로 얻을 수 있는 단어의 분산 표현은 단어의 의미 면에서나 문법 면에서 모두 우리의 직관에 부합하는 경우가 많습니다.

  - 이 분산 표현은 위키백과 등 대규모 말뭉치를 사용해 얻을 수 있는 단어의 분산 표현이라면 더더욱 그렇습니다.

#### Note

> CBOW 모델은 단어의 출현 패턴을 학습 시 사용한 말뭉치에서 배웁니다.
>
> 따라서 말뭉치가 다르다면 학습 후 얻게 되는 단어의 분산 표현도 달라집니다.
>
> 예를 들어 '스포츠' 기사만을 다룬 것과 '음악' 기사만을 다룬 것은 단어의 분산 표현이 크게 달라질 것입니다.

- 다시 앞에서 본 신경망의 학습에 대해 생각해보겠습니다. 우리가 다루는 모델은 다중 클래스 분류를 수행하는 신경망으로 이를 학습하려면 소프트맥스와 교차 엔트로피 오차만 이용하면 됩니다.

  - 여기에서는 소프트맥스 함수를 이용해 점수를 확률로 반환하고 그 확률과 정답 레이블의 교차 엔트로피 오차를 구한 후 그 값을 손실로 사용합니다.

  <img src="README.assets/fig 3-13.png" alt="fig 3-13" style="zoom:50%;" />

  - 이처럼 추론 처리를 수행하는 CBOW 모델에 Softmax 계층과 Cross Entropy Error 계층을 추가했을 뿐으로 손실을 얻을 수 있습니다.

  - 이상이 CBOW 모델의 손실을 구하는 계산의 흐름이자 순방향 전파입니다.

- 덧붙여 앞 선 그림은 Softmax와 Cross Entropy 계층을 사용했지만 이 두 계층을 합쳐 Softmax with Loss라는 하나의 계층으로 구현할 수 있습니다. 그 모습은 다음과 같습니다.

<img src="README.assets/fig 3-14.png" alt="fig 3-14" style="zoom:50%;" />

### 3.2.3 word2vec의 가중치와 분산 표현

- 지금까지 설명한 것처럼 word2vec에서 사용되는 신경망에는 두 가지 가중치가 있습니다.

  - 바로 입력 측 완전연결계층과 출력 측 완전연결계층이고 입력 측의 경우 각 행이 각 단어의 분산 표현에 해당합니다.

  - 또한 출력 측 가중치도 단어의 의미가 인코딩된 벡터가 저장되고 있다고 생각할 수 있는데 출력 측의 가중치는 다음과 같이 각 단어의 분산 표현이 열 방향으로 저장됩니다.

  <img src="README.assets/fig 3-15.png" alt="fig 3-15" style="zoom:50%;" />

- 여기서 최종적으로 이용하는 단어의 분산 표현으로 어느 쪽이 더 나은지 고민해야 합니다. 가능한 경우는 입력 측 가중치, 출력 측 가중치, 둘 다입니다.

  - 여기서 둘 다 사용하는 경우는 각 가중치를 어떻게 조합하느냐에 따라 몇 가지 방법이 생기는 데 그 중 하나는 두 가중치를 단순히 더한다는 것입니다.

- wrod2vec(특히 skip-gram) 모델에서는 입력 측의 가중치만 이용하는 것이 대중적인 선택입니다. 많은 연구도 그러한 방향으로 진행되므로 우리도 이를 따라 W<sub>in</sub>만 단어의 분산 표현으로 활용하겠습니다.

#### Note

> word2vec과 skip-gram의 경우 W<sub>in</sub>을 사용한 쪽이 더 나은 성과를 보여줍니다.
>
> 반면에 word2vec과 비슷한 기법인 GloVe에서는 두 가중치를 더했을 때 좋은 결과를 얻었습니다.

## 3.3 학습 데이터 준비

- 지금부터 word2vec 학습에 쓰일 학습 데이터를 준비하겠습니다. 이번에도 간단하게 'You say goodbye and I say hello.'라는 한 문장을 이용하겠습니다.

### 3.3.1 맥락과 타깃

- word2vec에서 이용하는 신경망의 입력은 '맥락'이고 그 정답 레이블은 맥락에 둘러싸인 중앙 단어, '타깃'입니다.

  - 즉, 우리가 해야할 일은 신경망에 '맥락'을 입력했을 때 '타깃'이 출현할 확률을 높이는 것입니다.

- 말뭉치에서 '맥락'과 '타깃'을 만드는 방법은 다음과 같이 생각할 수 있습니다.

<img src="README.assets/fig 3-16.png" alt="fig 3-16" style="zoom:50%;" />

- 말뭉치에서 목표로 하는 단어를 '타깃'으로, 그 주변 단어를 '맥락'으로 뽑아내고 이 작업을 말뭉치 안의 양끝 단어를 제외한 모든 단어에 대해 수행합니다.

  - 이렇게 만든 '맥락'과 '타깃'에 '맥락'에서 맥락이 신경망의 입력으로, 타깃이 정답 레이블이 됩니다.

  - 참고로 각 샘플 데이터에서 맥락의 수는 여러 개가 될 수 있으나 타깃은 오직 하나입니다. 그래서 맥락을 영어로 쓸 때는 s를 붙이는 것이 좋습니다.

- 이어 말뭉치에서 맥락과 타깃을 만드는 함수를 구현할 텐데 우선 말뭉치 텍스트를 단어 ID로 변환해야 합니다.

```python
import commons.util import preprocess

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
```

- 다음 단어 ID 배열인 corpus에서 맥락과 타깃을 만들어내는데 구체적으로는 corpus를 입력받아 맥락과 타깃을 반환하는 함수를 만듭니다.

<img src="README.assets/fig 3-17.png" alt="fig 3-17" style="zoom:50%;" />

- 여기서 맥락은 2차원 배열로 0번째 차원에는 각 맥락 데이터가 저장됩니다.

  - 다시 말해 `contexts[0]`는 0번째 맥락이 저장되고 `contexts[1]`에는 1번째 맥락이 저장되는 방식입니다.

  - 또한 `target[0]`에 0번째 타깃이 저장되어 서로 대응합니다.

- 이제 맥락과 타깃을 만드는 함수를 구현할 차례입니다. 이름은 create_contexts_target(corpus, window_size)로 자세한 내용은 다음과 같습니다.

```python
def create_contexts_target(corpus, window_size=1):  # commons.util.py
    target = corpus[window_size:-window_size]
    contexts = []

    for idx in range(window_size, len(corpus) - window_size):
        cs = []
        for t in range(-window_size, window_size + 1):
            if t == 0:
                continue
            cs.append(corpus[idx + t])
        contexts.append(cs)

    return np.array(contexts), np.array(target)
```

- 이 함수는 단어 ID 배열과 맥락의 윈도우 크기를 입력받아 맥락과 타깃을 numpy 다차원 배열로 반환합니다.

  - 이렇게 만들어진 맥락과 타깃은 CBOW 모델에 전달하면 완료되지만 맥락과 타깃의 각 원소는 단어 ID이므로 이를 원핫 표현으로 바꾸겠습니다.

### 3.3.2 원핫 표현으로 변환

- 맥락과 타깃을 원핫 표현으로 바꾸는 과정은 다음과 같습니다.

<img src="README.assets/fig 3-18.png" alt="fig 3-18" style="zoom:50%;" />

- 이처럼 맥락과 타깃을 단어 ID에서 원핫 표현으로 변환하면 됩니다. 이 때 다시 한 번 각 다차원 배열의 형상에 주목해야 합니다.

  - 잘 보면, 단어 ID의 형상에 비해 차원이 하나 더 늘어남을 알 수 있습니다.

- 이 과정을 프로그래밍하면 convert_one_hot() 함수를 만들면 됩니다. 이 함수는 단어 ID 목록과 어휘 수를 받습니다.

> 자세한 내용은 chapter03/commons/util.py의 convert_one_hot 함수를 확인하세요.

- 이제 이를 기반으로 CBOW 모델을 구현하겠습니다.

## 3.4 CBOW 모델 구현

- 이제 CBOW 모델을 구현하겠습니다. 모델의 형식은 다음과 같습니다.

<img src="README.assets/fig 3-19.png" alt="fig 3-19" style="zoom:50%;" />

- 이를 간단하게 구현한 신경망은 SimpleCBOW라는 이름이 될 것이며 이를 초기화 메서드부터 구현하면 다음과 같습니다.

> 3.4_simple_cbow.py를 확인하세요.

- 초기화 메서드는 인수로 어휘의 수와 은닉층의 뉴런 수를 받고 다음의 과정을 진행합니다.

  - 먼저 가중치 W_in과 W_out을 생성해 각각 작은 무작위 값으로 초기화합니다. 이때 배열을 astype('f')를 사용해 32비트 부동소수점 수로 지정합니다.

  - 이어 필요한 계층을 생성합니다. 입력의 MatMul 2개와 출력의 MatMul 1개, Softmax with Loss 1개입니다.

    - 입력의 MatMul 계층은 맥락에 사용하는 단어 수, 즉 윈도우 크기만큼 만들어야 하며 모두 같은 가중치를 사용하도록 초기화 합니다.

  - 마지막으로 이 신경망에 사용되는 매개변수와 기울기를 인스턴스 변수인 params와 grads에 각각 모아둡니다.

#### Warning

> 이 코드에서 여러 계층에서 같은 가중치를 공유하고 있어 params에 같은 가중치가 여러 개 존재하게 됩니다.
>
> 이렇게 되면 Adam이나 Momentum 등의 optimizer의 처리가 본래와 다르게 됩니다.
>
> 이를 방지하기 위해 Trainer 내부에 매개변수를 갱신할 때 매개변수의 중복을 없애는 간단한 작업이 수행됩니다.
>
> 자세한 내용은 commons/trainer.py의 remove_duplicate 함수를 확인하세요.

- 이어서 신경망의 순전파는 forward 메서드를 사용합니다. 해당 메서드는 인수로 맥락, 타깃을 받고 손실을 반환합니다.

  - 맥락 contexts는 3차원 numpy array로 가정하며 이전의 예제에 따르면 이 배열의 형상은 (6, 2, 7)이 됩니다.

    - 해당 배열의 0번째 차원이 미니배치의 수, 1번째 차원이 맥락의 윈도우 크기, 2번째 차원이 원핫 벡터입니다.

  - 또한, 타깃 target의 형상은 2차원으로 (6, 7)과 비슷한 형상입니다.

- 마지막으로 역전파는 backward 메서드로 구현하며 이 계산 그래프는 다음과 같습니다.

<img src="README.assets/fig 3-20.png" alt="fig 3-20" style="zoom:50%;" />

- 신경망의 역전파는 기숭기를 순전파의 반대 방향으로 전파하며 역전파는 1에서 시작해 바로 Softmax with Loss 계층으로 흐릅니다.

  - 이어 ds로 바뀐 결과가 출력 측의 MatMul 계층에 입력됩니다.

  - 이어 da로 바뀐 결과가 곱하기와 더하기 연산으로 역전파되는데 곱하기의 역전파는 순전파의 입력을 서로 바꿔 기울기에 곱하며 더하기는 그대로 통과시킵니다.

- 이것으로 역전파 구현까지 모두 마쳤으며 각 매개변수의 기울기를 grads에 모아두었습니다.

  - 따라서 forward와 backward 메서드를 실행하는 것으로 grads의 기울기가 갱신됩니다. 이제 이를 통해 학습을 시켜보겠습니다.

### 3.4.1 학습 코드 구현

- CBOW 모델의 학습은 일반 신경망 모델과 같습니다. 학습 데이터를 신경망에 입력하고 기울기를 구한 뒤 가중치 매개변수를 순서대로 갱신할 것입니다.

  - 이 과정에는 1장에서 설명한 Trainer 클래스를 사용할 것인데 학습 과정은 다음과 같습니다.

> 3.4.1_train.py를 확인하세요.

- 학습 데이터에서 미니배치를 선택하고 신경망에 입력해 기울기를 구한 다음 Optimizer에서 매개변수를 갱신하는 과정을 거치게됩니다.

#### Note

> 앞으로 신경망 학습은 Trainer 클래스를 사용하는데 이를 사용하면 복잡해지기 쉬운 코드를 깔끔하게 유지할 수 있습니다.

- 이제 이 파일의 실행 결과는 다음과 같습니다.

<img src="README.assets/fig 3-21.png" alt="fig 3-21" style="zoom:50%;" />

- 보다시피 학습을 거듭할수록 손실이 줄어드는데 학습이 순조로움을 알 수 있습니다.

  - 여기서 학습이 끝난 후의 가중치 매개변수를 살펴보자면 입력 측 MatMul 계층의 가중치를 꺼내 확인하면 됩니다.

  - 이를 실행하면 각 행에 대응하는 단어 ID의 분산 표현이 저장되어있어 이를 확인하면 됩니다.

- 이제 단어를 밀집벡터로 나타낼 수 있게 되었으며 이것이 단어의 분산 표현입니다.

  - 이 분산 표현은 '단어의 의미'를 잘 파악한 벡터 표현일 것으로 기대할 수 있습니다.

- 하지만 이렇게 작은 말뭉치는 말뭉치가 워낙 작기 때문에 좋은 결과를 얻을 수 없습니다. 그렇다고 큰 말뭉치는 결과가 좋아지지만 처리 속도에 문제가 생깁니다.

  - 따라서 이 문제점은 다음 장에서는 '진짜' CBOW 모델로 구현하여 해결하도록 하겠습니다.

## 3.5 word2vec 보충

- 지금까지 word2vec의 CBOW 모델을 자세히 살펴보았습니다. 이번 절에서는 word2vec에 관한 중요한 주제를 보충학습하겠습니다.

  - 우선 CBOW 모델을 '확률'관점에서 다시 살펴보겠습니다.

### 3.5.1 CBOW 모델과 확률

- 먼저 '확률'의 표기법을 간단히 알아보면 확률은 **P()**라고 작성하며 P(A)는 A가 일어날 확률을 P(A, B)는 A와 B가 동시에 일어날 **동시 확률**입니다.

- 반면 B가 먼저 일어나고 A가 일어나는 **사후 확률**은 P(A|B)로 작성하며 말 그대로 '**사건**이 일어난 **후**의 **확률**'입니다.

- 그럼 CBOW 모델을 확률 표기법으로 기술해볼텐데 CBOW 모델은 맥락을 주면 타깃 단어가 출현할 확률을 나타냅니다.

  - 여기서 말뭉치를 w<sub>i</sub>처럼 단어 시퀀스로 나타내고 t번째 단어에 대해 윈도우 크기가 1인 맥락을 고려하면 다음과 같습니다.

  <img src="README.assets/fig 3-22.png" alt="fig 3-22" style="zoom:50%;" />

  - 그럼 맥락으로 그 이전과 이후 단어가 주어졌을 때 타깃이 w<sub>t</sub>가 될 확률은 다음과 같이 쓸 수 있습니다.

  <img src="README.assets/e 3-1.png" alt="e 3-1" style="zoom:50%;" />

  - 위의 식은 'w<sub>t-1</sub>과 w<sub>t+1</sub>이 일어난 후 w<sub>t</sub>가 일어날 확률'을 의미합니다. 즉, CBOW는 위의 식을 모델링하고 있습니다.

- 위의 식을 이용하면 CBOW 모델의 손실 함수도 간결하게 표현할 수 있습니다. 여기서 1장에서 나온 교차 엔트로피 오차를 적용합니다.

  - 교차 엔트로피 오차의 y<sub>k</sub>은 'k번째에 해당하는 사건이 일어날 확률'을 의미합니다. 그리고 t<sub>k</sub>은 정답 레이블이며 원핫 벡터로 표현됩니다.

  - 여기서 문제의 정답은 'w<sub>t</sub>가 발생'하는 것이므로 해당 원소만 1이고 나머지는 0인 원핫 레이블이됩니다. 이를 감안하면 다음의 식이 유도됩니다.

  <img src="README.assets/e 3-2.png" alt="e 3-2" style="zoom:50%;" />

  - 이 식에서 볼 수 있든 CBOW 모델의 손실함수는 단순히 식 3.1의 확률에 log를 취하고 마이너스를 붙이면 됩니다. 이를 **음의 로그 기능도**라고 합니다. 이 식은 샘플 데이터 하나에 대한 것이므로 이를 말뭉치 전체로 확장하면 다음과 같습니다.

  <img src="README.assets/e 3-3.png" alt="e 3-3" style="zoom:50%;" />

- CBOW 모델의 학습이 수행하는 일은 위의 식의 값을 가능한 작게 만드는 것입니다. 그리고 이 가중치 매개변수가 우리가 얻고자하는 단어의 분산 표현인 것입니다.

  - 여기서는 윈도우 크기가 1인 경우만 생각했지만, 다른 크기도 수식으로 쉽게 나타낼 수 있습니다.
