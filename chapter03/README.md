# chapter03. word2vec

- 앞 장에 이어 이번 장의 주제도 단어의 분산 표현이지만 앞 장에서 '통계 기반 기법'을 사용한 것과 달리 더 강력한 '추론 기반 기법'을 사용할 것입니다.

- 이름에서 보듯 '추론 기반 기법'은 추론을 하는 기법으로 신경망을 사용하는데 그 신경망이 word2vec입니다.

  - 이번 장에서는 word2vec의 구조를 알아보고 이를 직접 구현해보며 확실하게 이해할 것입니다.

- 이번 장의 목표는 단순한 word2vec 구현하기로 처리 효율을 희생한 대신 이해하기 쉬운 word2vec입니다.

  - 따라서 큰 데이터셋에서는 어렵지만 작은 데이터셋은 문제없이 처리할 수 있습니다.

  - 또한, 다음 장에서 이 word2vec에 여러 개선사항을 더해 '진짜' word2vec을 구현할 예정입니다.

## 3.1 추론 기반 기법과 신경망

- 단어를 벡터로 표현하는 방법 중 가장 성공적인 기법은 크게 '통계 기반 기법'과 '추론 기반 기법'입니다.

  - 이 둘은 단어의 의미를 얻는 방식이 서로 크게 다르지만 그 배경에는 모두 분포 가설이 있습니다.

- 이번 절에서는 통계 기반 기법의 문제를 지적하고 그 대인아니 추론 기반 기법의 이점을 거시적인 관점에서 설명합니다.

  - 또한 word2vec의 전처리를 위한 신경망으로 '단어'를 처리하는 예시를 살펴볼 것입니다.

### 3.1.1 통계 기반 기법의 문제점

- 지금까지 본 것처럼 통계 기반 기법은 주변 단어의 빈도를 기초로 단어를 표현합니다.

  - 구체적으로 단어의 동시발생 행렬을 만들고 그 행렬에 SVD를 적용하여 밀집 벡터로 재구성합니다. 하지만 이 방식은 대규모 말뭉치를 다룰 때 문제가 발생합니다.

- 현실에서 다루는 말뭉치의 어휘 수는 어마어마합니다. 예컨대 영어의 어휘 수는 100만을 훌쩍 넘는데 어휘가 100만 개라면 통계 기반 기법에서는 그 제곱의 행렬을 만들게 됩니다. 이는 비현실적입니다.

#### Note

> SVD를 n by n 행렬에 적용하는 비용은 O(n<sup>3</sup>)입니다.
>
> 이는 계산 시간이 n의 세제곱에 비례한다는 뜻으로 슈퍼컴퓨터를 동원해도 처리하기 힘든 수준입니다.
>
> 실제로 근사적 기법과 희소행렬의 성질 등으로 이를 개선할 수 있지만 그렇다고 해도 여전히 상당한 컴퓨팅 자원을 소모합니다.

- 통계 기반 기법은 말뭉치 전체의 통계를 이용해 단 1회의 처리만에 단어의 분산 표현을 얻습니다.

- 한편, 추론 기반 기법에서는 신경망을 이용한다면 미니배치로 학습하는 것이 일반적입니다.

  - 미니배치 학습에서는 신경망이 한 번에 소량의 학습 샘플씩 반복해서 학습하여 가중치를 갱신해갑니다.

  <img src="README.assets/fig 3-1.png" alt="fig 3-1" style="zoom:50%;" />

- 위의 그림처럼 통계 기반 기법은 학습 데이터를 한 번에 처리합니다.(배치 학습) 반면에 추론 기반 기법은 학습 데이터의 일부를 사용해 순차적으로 학습합니다.(미니배치 학습)

  - 이는 말뭉치의 어휘 수가 많아 SVD 등 계산량이 큰 작업을 처리하기 어려운 경우에도 신경망을 학습시킬 수 있다는 것으로 데이터를 작게 나눠 학습시킵니다.

  - 게다가 여러 GPU를 사용한 병렬 계산도 가능하므로 학습 속도를 높일 수도 있습니다.

  - 추론 기반 기법이 통계 기반 기법보다 매력적인 점은 이 외에도 여러 가지가 있으며 이는 추론 기반 기법을 학습한 뒤 '3.5.3 통계 기반 vs 추론 기반'에서 알아보겠습니다.

### 3.1.2 추론 기반 기법 개요

- 추론 기반 기법은 '추론'이 주된 작업인데 추론은 주변 단어가 주어졌을 때 빈 칸에 들어갈 단어를 추측하는 작업입니다.

<img src="README.assets/fig 3-2.png" alt="fig 3-2" style="zoom:50%;" />

- 이처럼 추론 문제를 풀고 학습하는 것이 '추론 기반 기법'이 다루는 문제로 이 문제를 반복해서 풀면서 단어의 출현 패턴을 학습합니다.

  - 이를 '모델 관점'에서 바라보면 다음과 같습니다.

<img src="README.assets/fig 3-3.png" alt="fig 3-3" style="zoom:50%;" />

- 이처럼 추론 기반 기법에는 어떤 모델이 등장하는데 이 모델로 신경망을 사용할 것입니다.

  - 모델은 맥락의 정보를 입력받아 출현할 수 있는 각 단어의 출현 확률을 출력하고 그 안애서 말뭉치를 사용해 올바른 추측을 내릴 수 있도록 학습합니다.

  - 그리고 그 학습의 결과로 단어의 분산 표현을 얻는 것이 추론 기반 기법의 전체 그림입니다.

#### Note

> 추론 기반 기법도 통계 기반 기법처럼 분포 가설이 존재합니다.
>
> 분포 가설이란 '단어의 의미는 주변 단어에 의해 형성된다'는 가설로 추론 기반 기법에서는 이를 앞과 같은 추측 문제로 귀결시켰습니다.
>
> 이처럼 두 기법 모두 분포 가설에 근거하는 '단어의 동시발생 가능성'을 얼마나 잘 모델링하는가가 중요한 연구입니다.

### 3.1.3 신경망에서의 단어 처리

- 지금부터 신경망을 이용해 단어를 처리합니다만 신경망은 단어를 그래도 처리할 수 없으니 단어를 '고정 길이의 벡터'로 변환합니다. 이 방법이 **원핫 표현(벡터)**입니다.

  - 원핫 표현이란 벡터의 원소 중 하나만 1이고 나머지는 0인 벡터를 의미합니다.

- 원핫 표현은 자세하게 다음과 같이 'You say goodbye and I say hello'라는 문장이 있을 때 이를 다음과 같이 처리합니다.

<img src="README.assets/fig 3-4.png" alt="fig 3-4" style="zoom:50%;" />

- 이처럼 단어는 텍스트, 단어 ID 그리고 원핫 표현 형태로 나타낼 수 있고 단어를 원핫 표현으로 변환하는 방법은 다음과 같습니다.

  - 먼저 총 어휘 수만큼 원소를 갖는 벡터를 준비하고 인덱스가 단어 ID와 같은 원소를 1로 나머지를 0으로 설정합니다.

  - 그럼 단어를 고정 길이 벡터로 변환하면 신경망 입력층은 뉴런의 수를 고정할 수 있습니다.

<img src="README.assets/fig 3-5-1585116923977.png" alt="fig 3-5" style="zoom:50%;" />

- 위의 예시처럼 입력층의 뉴런은 총 7개이고 각 뉴런은 각 단어에 대응합니다. 이제 이를 사용해 단어를 벡터로 나타낼 수 있고 신경망을 구성하는 '계층'들을 벡터로 처리할 수 있습니다.

  - 다시 말해, 단어를 신경망으로 처리할 수 있다는 뜻으로 다음처럼 원핫 표현의 단어 하나를 완전연결계층을 통해 변환할 수 있습니다.

<img src="README.assets/fig 3-6.png" alt="fig 3-6" style="zoom:50%;" />

- 위 신경망은 완전연결계층이므로 각 노드가 이웃 층의 모든 노드와 연결되어 있습니다.

  - 이 화살표에는 가중치(매개변수)가 존재하며 입력층 뉴런과 가중치의 합이 은닉층 뉴런이 됩니다. 참고로 이번 장의 완전연결 계층에서는 편향을 생략합니다.

#### Note

> 편향을 이용하지 않는 완전연결계층은 '행렬 곱' 연산에 해당합니다.
>
> 그래서 이 책에서 완전연결계층은 1장에서 구현한 MatMul 계층과 같아집니다.
>
> 참고로 딥러닝 프레임워크들은 일반적으로 완전연결계층을 생성할 때 편향을 이용할지 선택할 수 있습니다.

- 위에서 본 그림에서 가중치를 명확히 표현하기 위해 바꿔 그리면 다음과 같습니다.

<img src="README.assets/fig 3-7.png" alt="fig 3-7" style="zoom:50%;" />

- 그럼 이를 코드로 살펴보고 넘어가겠습니다. 거두절미하고 이 완전연결 계층에 의한 변환은 python으로 다음과 같습니다.

```python
import numpy as np

c = np.array([[1, 0, 0, 0, 0, 0, 0]])  # 입력
W = np.random.randn(7, 3)  # 가중치
h = np.matmul(c, W)  # 중간 노드
print(h)
```

- 이 코드는 단어가 0인 단어를 원핫 표현으로 표현하고 완전연결계층을 통과시켜 변환하는 모습입니다.

  - 복습해보자면, 완전연결계층의 계산은 행렬곱으로 수행할 수 있고 이 연산은 np.matmul()이 해결합니다.

#### Warning

> 이 코드에서 입력 데이터 c의 차원 수(ndim)은 2입니다.
>
> 이는 미니배치 처리를 고려한 것으로 최초의 차원(0번째 차원)에 각 데이터를 저장합니다.

- 앞선 코드에서 주목할 부분은 c와 W의 행렬 곱 부분으로 c는 원핫 표현이라서 단어 ID에 대응하는 원소만 1이고 나머지는 0인 벡터입니다. 따라서 앞선 c와 W의 연산은 결국 가중치의 행벡터 하나를 '뽑아낸' 것과 같습니다.

<img src="README.assets/fig 3-8.png" alt="fig 3-8" style="zoom:50%;" />

- 그저 가중치에 행벡터 하나를 뽑아낼 뿐인데 행렬 곱을 계산하는 건 비효율적으로 보일 수 있는데 이는 '4.1 word2vec의 개선 1'에서 개선할 예정입니다. 다음처럼.

```python
from commons.layers import MatMul

c = np.array([[1, 0, 0, 0, 0, 0, 0]])
W = np,random.randn(7, 3)

layer = MatMul(W)
h = layer.forward(c)
print(h)
```

- 위 코드는 commons 디렉토리 안에 있는 MatMul 계층을 import하여 사용합니다. 그런 다음 해당 계층에 가중치 W를 설정하고 forward를 사용해 순전파를 계산합니다.

## 3.2 단순한 word2vec

- 앞 절에서는 추론 기반 기법을 배우고 신경망으로 단어를 처리하는 방법을 코드로 알아봤습니다. 이제 이번에는 word2vec을 구현해보겠습니다.

  - 이번 절에서 사용할 신경망은 word2vec에서 제안하는 **CBOW** 모델입니다.

#### Warning

> word2vec이라는 용어는 원래 프로그램이나 도구를 가리키는 데 사용되지만 용어가 유명해지며 신경망 모델을 가리키는 경우도 많아졌습니다.
>
> CBOW 모델과 skip-gram 모델은 word2vec에서 사용되는 신경망으로 이번 절에서는 CBOW를 이야기하며 두 모델의 차이는 '3.5.2 skip-gram 모델'에서 설명하겠습니다.

### 3.2.1 CBOW 모델의 추론 처리

- CBOW 모델은 맥락에서 타깃을 추측하는 용도의 신경망입니다. 이 모델이 가능한 정확하게 추론하도록 훈련하여 단어의 분산 표현을 얻는 것이 목표입니다.

- CBOW 모델의 입력은 맥락으로 앞서 본 'you'나 'goodbye'와 같은 단어들의 목록입니다.

  - 가장 먼저 이 맥락을 원핫 표현으로 변환하여 CBOW 모델이 처리할 수 있도록 준비합니다. 이는 다음과 같이 표현됩니다.

  <img src="README.assets/fig 3-9.png" alt="fig 3-9" style="zoom:50%;" />

- 이 그림이 CBOW 모델의 신경망으로 입력층이 2개에 은닉층을 거쳐 출력층에 도달하는 형태입니다.

  - 두 입력층에서 은닉층으로의 변환은 똑같은 완전연결게층이 처리하고 은닉층에서 출력층으로의 변환은 다른 완전연결계층이 처리합니다.

#### Warning

> 이 그림에서 입력층이 2개인 이유는 맥락으로 고려할 단어를 2개로 정했기 때문입니다.
>
> 즉, 맥락에 포함시킬 단어가 N개라면 입력층도 N개가 됩니다.

- 이제 이 모델의 은닉층에 주목하면 은닉층의 뉴런은 입력층의 완전연결계층에 의해 변환된 값이 되는데 입력층이 여러 개라면 전체를 '평균'하면 됩니다.

  - 앞선 예에 대입하면 완전연결계층에 의한 첫 번째 입력층이 h<sub>1</sub>이 되고 두 번째 입력층이 h<sub>2</sub>가 되면 은닉층은 0.5 \* (h<sub>1</sub> + h<sub>2</sub>)가 됩니다.

  - 마지막으로 출력층의 뉴런은 7개인데 이 뉴런 하나가 각 단어에 대응합니다.

    - 출력층의 뉴런은 각 단어의 '점수'를 뜻하며 값이 높을수록 대응 단어의 출현 확률도 높아집니다.

    - 여기서 점수란 확률로 해석되기 전의 값이고 이 점수에 소프트맥스 함수를 적용하면 '확률'을 얻을 수 있습니다.

#### Warning

> 점수를 Softmax 계층에 통과시킨 후의 뉴런을 출력층이라고 합니다. 이 책에서는 점수를 출력하는 층을 출력층이라고 하겠습니다.

- 이제 입력층에서 은닉층으로의 변환은 완전연결계층에 의해 이루어지는데 이 예시에서 완전연결계층의 가중치 W<sub>in</sub>은 7 by 3 행렬로 단어의 분산 표현의 정체입니다.

<img src="README.assets/fig 3-10.png" alt="fig 3-10" style="zoom:50%;" />

- 위 그림에서 보듯 가중치의 각 행에는 해당 단어의 분산 표현이 담겨 있습니다. 따라서 학습을 진행할수록 맥락에 출현하는 단어를 잘 추측하는 방향으로 분산 표현들이 갱신될 것입니다.

  - 그리고 이렇게 얻은 벡터에는 '단어의 의미'도 잘 녹아들어 있는데 이것이 word2vec의 전체 그림입니다.

#### Note

> 은닉층의 뉴런 수를 입력층의 뉴런 수보다 적게하는 것이 중요한 핵심입니다.
>
> 이렇게 해야 은닉층에는 단어 예측에 필요한 정보를 '간결하게' 담게 되며, 결과적으로 밀집벡터 표현을 얻을 수 있습니다.
>
> 이 때 은닉층의 정보는 사람이 이해할 수 없는 코드로 쓰여있는데 이것이 바로 '인코딩'입니다.
>
> 한 편 은닉층의 정보에서 원하는 결과를 얻는 작업은 '디코딩'입니다. 즉, 디코딩이랑 인코딩된 정보를 우리 인간이 이해할 수 있는 표현으로 복원하는 작업입니다.

- 지금까지 CBOW 모델을 '뉴런 관점'에서 그려왔다면 이제 이를 '계층 관점'에서 그려보겠습니다. 그 모습은 다음과 같습니다.

<img src="README.assets/fig 3-11.png" alt="fig 3-11" style="zoom:50%;" />

- 위 그림에서 보듯 CBOW 모델 가장 앞에는 2개의 MatMul 계층이 있고 이어 두 계층의 출력을 더한 뒤 0.5를 곱해 '평균'을 계산합니다. 이는 은닉층 뉴런이 됩니다.

  - 마지막으로 은닉층 뉴런에 또 다른 MatMul 계층이 적용되면 '점수'가 출력됩니다.

#### Warning

> 편향을 사용하지 않는 완전연결계층의 처리는 MatMul 계층의 순전파와 같습니다. 이 계층은 내부에서 행렬 곱을 계산합니다.

- 그럼 이를 참고해 CBOW 모델의 추론 처리를 python으로 구현하겠습니다.

> 3.2.1_cbow_predict.py를 확인하세요.

- 해당 파일의 과정이 CBOW 모델의 추론 과정입니다. 여기서 보듯 CBOW 모델은 활성화 함수를 사용하지 않는 간단한 구성의 신경망입니다.

  - 다만 차이는 입력층이 여러 개가 있고 입력층들의 가중치가 공유된다는 점입니다.

### 3.2.2 CBOW 모델의 학습

- 지금까지 설명한 CBOW 모델은 출력층에서 각 단어의 점수를 출력하는데 여기에 소프트맥스 함수를 적용하면 '확률'을 얻을 수 있습니다.

  - 이 확률은 맥락이 주어졌을 때 그 중앙에 어떤 단어가 출현하는지를 나타냅니다.

- 다음 예처럼 맥락이 'you'와 'goodbye'이고 정답 레이블이 'say'일 때 가중치가 적절히 설정된 신경망이라면 확률을 나타내는 뉴런들 중 정답에 해당하는 뉴런의 값이 클 것입니다.

<img src="README.assets/fig 3-12.png" alt="fig 3-12" style="zoom:50%;" />

- CBOW 모델의 학습은 올바른 예측을 할 수 있도록 가중치를 조정하는 일을 하는데 그 결과 가중치 W<sub>in</sub>에 단어의 출현 패턴을 파악한 벡터가 학습됩니다.

  - 물론 정확하게는 in, out 모두에 이를 기록하며 CBOW(+skip-gram) 모델로 얻을 수 있는 단어의 분산 표현은 단어의 의미 면에서나 문법 면에서 모두 우리의 직관에 부합하는 경우가 많습니다.

  - 이 분산 표현은 위키백과 등 대규모 말뭉치를 사용해 얻을 수 있는 단어의 분산 표현이라면 더더욱 그렇습니다.

#### Note

> CBOW 모델은 단어의 출현 패턴을 학습 시 사용한 말뭉치에서 배웁니다.
>
> 따라서 말뭉치가 다르다면 학습 후 얻게 되는 단어의 분산 표현도 달라집니다.
>
> 예를 들어 '스포츠' 기사만을 다룬 것과 '음악' 기사만을 다룬 것은 단어의 분산 표현이 크게 달라질 것입니다.

- 다시 앞에서 본 신경망의 학습에 대해 생각해보겠습니다. 우리가 다루는 모델은 다중 클래스 분류를 수행하는 신경망으로 이를 학습하려면 소프트맥스와 교차 엔트로피 오차만 이용하면 됩니다.

  - 여기에서는 소프트맥스 함수를 이용해 점수를 확률로 반환하고 그 확률과 정답 레이블의 교차 엔트로피 오차를 구한 후 그 값을 손실로 사용합니다.

  <img src="README.assets/fig 3-13.png" alt="fig 3-13" style="zoom:50%;" />

  - 이처럼 추론 처리를 수행하는 CBOW 모델에 Softmax 계층과 Cross Entropy Error 계층을 추가했을 뿐으로 손실을 얻을 수 있습니다.

  - 이상이 CBOW 모델의 손실을 구하는 계산의 흐름이자 순방향 전파입니다.

- 덧붙여 앞 선 그림은 Softmax와 Cross Entropy 계층을 사용했지만 이 두 계층을 합쳐 Softmax with Loss라는 하나의 계층으로 구현할 수 있습니다. 그 모습은 다음과 같습니다.

<img src="README.assets/fig 3-14.png" alt="fig 3-14" style="zoom:50%;" />

### 3.2.3 word2vec의 가중치와 분산 표현

- 지금까지 설명한 것처럼 word2vec에서 사용되는 신경망에는 두 가지 가중치가 있습니다.

  - 바로 입력 측 완전연결계층과 출력 측 완전연결계층이고 입력 측의 경우 각 행이 각 단어의 분산 표현에 해당합니다.

  - 또한 출력 측 가중치도 단어의 의미가 인코딩된 벡터가 저장되고 있다고 생각할 수 있는데 출력 측의 가중치는 다음과 같이 각 단어의 분산 표현이 열 방향으로 저장됩니다.

  <img src="README.assets/fig 3-15.png" alt="fig 3-15" style="zoom:50%;" />

- 여기서 최종적으로 이용하는 단어의 분산 표현으로 어느 쪽이 더 나은지 고민해야 합니다. 가능한 경우는 입력 측 가중치, 출력 측 가중치, 둘 다입니다.

  - 여기서 둘 다 사용하는 경우는 각 가중치를 어떻게 조합하느냐에 따라 몇 가지 방법이 생기는 데 그 중 하나는 두 가중치를 단순히 더한다는 것입니다.

- wrod2vec(특히 skip-gram) 모델에서는 입력 측의 가중치만 이용하는 것이 대중적인 선택입니다. 많은 연구도 그러한 방향으로 진행되므로 우리도 이를 따라 W<sub>in</sub>만 단어의 분산 표현으로 활용하겠습니다.

#### Note

> word2vec과 skip-gram의 경우 W<sub>in</sub>을 사용한 쪽이 더 나은 성과를 보여줍니다.
>
> 반면에 word2vec과 비슷한 기법인 GloVe에서는 두 가중치를 더했을 때 좋은 결과를 얻었습니다.
