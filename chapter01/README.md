# chapter01. Review Neural Network

- 이 책은 전편에 이어 딥러닝의 가능성을 한층 더 깊이 탐험할 것입니다.

  - 물론, 전편과 동일하게 라이브러리나 프레임워크 는 최대한 사용하지 않고 밑바닥부터 구현할 것입니다.

- 이번 장에서는 신경망을 복습합니다. 전편의 내용을 요약한 장이기도 합니다.

  - 다만 차이점은 효율 향상을 위해 전편의 구현 규칙을 일부 변경했습니다.

## 1.1 수학과 python 복습

- 먼저 수학, 즉 신경망 계산에 필요한 '벡터'나 '행렬'에 대해 복습하겠습니다. 또, 신경망을 원할히 구현하기 위한 python, 특히 numpy 코드도 되새겨볼 것입니다.

### 1.1.1 벡터와 행렬

- 신경망은 벡터와 행렬(또는 텐서)가 도처에서 등장하니 이를 가볍게 정리하겠습니다.

  - 벡터는 크기와 방향을 가진 양으로 숫자가 일렬로 늘어선 집합으로 표현할 수 있습니다.

    - 즉, 파이썬에서는 1차원 배열로 취급할 수 있습니다.

  - 반면에 행렬을 숫자가 2차원 형태로 늘어선 것으로 다음과 같은 차이를 가집니다.

  <img src="README.assets/fig 1-1.png" alt="fig 1-1" style="zoom:50%;" />

- 이처럼 벡터는 1차원, 행렬은 2차원 배열로 표현할 수 있으며 행렬의 가로를 행, 세로를 열이라고 합니다.

#### Note

> 벡터와 행렬을 확장해 숫자 집합을 N차원으로 표현한 것은 일반적으로 텐서라고 합니다

- 벡터는 단순한 개념이지만 이를 표현하는 방법이 행과 열, 두 가지로 나뉩니다.

<img src="README.assets/fig 1-2.png" alt="fig 1-2" style="zoom:50%;" />

- 수학과 딥러닝 등 많은 분야에서 '열벡터' 방식을 선호하지만, 이 책에서는 구현의 편의를 고려해 '행벡터'로 다루겠습니다.

  - 또한, 수식에서 벡터와 행렬은 **x**나 **W**처럼 굵게 강조하여 단일 원소로 구성된 스칼라와 구분하겠습니다.

#### Warning

> python으로 벡터를 행벡터로 구현할 때, 벡터를 가로 방향 행렬로 변환해 사용하면 보다 명확해집니다.
>
> 예를 들어 원소 수가 N개인 벡터는 1 \* N 형상의 행렬로 처리합니다.

- 그럼 python을 통해 벡터와 행렬에 대해 알아보겠습니다.

```python
import numpy as np

x = np.array([1, 2, 3])
print('class:', x.__class__)
print('shape:', x.shape)
print('dimension:', x.dim)

W = np.array([[1, 2, 3], [4, 5, 6]])
print('shape:', W.shape)
print('dimension:', W.dim)
```

- 이처럼 벡터와 행렬 모두 `np.array()`로 생성할 수 있습니다.

  - 이 메서드는 `np.ndarray` 클래스를 생성하며 이 클래스에는 다양한 메서드와 인스턴스 변수가 내장되어 있습니다.

    - 앞선 예제는 인스턴스 변수 중 `shape`와 `dim`을 사용했고 각각 다차원 배열의 형상과 차원의 수를 의미합니다.

- 결과를 보면 x는 1차원 배열에 원소 수가 3개인 벡터이고 W는 2차원 배열에 2행 3열의 행렬임을 알 수 있습니다.

### 1.1.2 행렬의 원소별 연산

- 수의 집합을 벡터나 행렬로 표현했다면 이를 간단히 계산하는 과정은 다음과 같습니다. 우선 원소별 연산입니다.

```python
X = np.array([[1, 2, 3], [4, 5, 6]])
W = np.array([[2, 4, 6], [1, 3, 5]])
print(X + W)
print(X * W)
```

- 다차원 numpy의 더하기와 곱하기를 위와 같이 하면 피연산자인 다차원 배열들에서 서로 대응하는 원소끼리 독립적인 연산이 이루어집니다.

### 1.1.3 브로드캐스트

- numpy의 다차원 배열에서 형상이 다른 배열끼리도 연산할 수 있는데 이를 브로드캐스트라고 합니다.

```python
A = np.array([[1, 2], [3, 4]])
print(A * 10)
```

- 위 계산에서 2행 2열의 행렬에 스칼라 10을 곱하면 스칼라 값이 동일한 크기의 행렬로 확장된 후 원소별 연산을 수행합니다.

<img src="README.assets/fig 1-3.png" alt="fig 1-3" style="zoom:50%;" />

- 또 다른 예시는 다음과 같습니다.

```python
A = np.array([[1, 2], [3, 4]])
b = np.array([10, 20])
print(A * b)
```

- 위 계산도 1차원 배열이 2차원 배열로 형상이 같아지도록 확장된 수 이를 계산합니다.

<img src="README.assets/fig 1-4.png" alt="fig 1-4" style="zoom:50%;" />

#### Warning

> numpy의 브로드캐스트가 효과적으로 동작하려면 다차원 배열의 형상이 몇 가지 규칙을 충족해야합니다.
>
> 자세한 내용은 문헌을 참고하세요.

### 1.1.4 벡터의 내적과 행렬의 곱

- 다음은 벡터의 내적과 행렬의 곱입니다. 우선 벡터의 내적은 다음과 같이 진행됩니다.

<img src="README.assets/e 1-1.png" alt="e 1-1" style="zoom:50%;" />

- 이처럼 2개의 벡터가 존재할 때 백터의 내적은 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것과 같습니다.

#### Note

> 벡터의 내적은 직관적으로 '두 벡터가 얼마나 같은 방향을 향하고 있는가'를 의미합니다.
>
> 벡터의 길이가 1인 경우, 완전히 같은 방향을 향하는 두 벡터의 내적은 1이지만 반대를 향하는 두 벡터의 내적은 -1입니다.

- 계속해서 행렬의 곱은 다음과 같은 수서로 계산됩니다.

<img src="README.assets/fig 1-5.png" alt="fig 1-5" style="zoom:50%;" />

- 이처럼 행렬의 곱은 왼쪽 행렬의 행벡터와 오른쪽 행렬의 열벡터의 내적으로 계산하고 그 결과를 새로운 행렬의 대응하는 원소에 저장합니다.

  - 예를 들어 A의 1행과 B의 1열의 연산 결과는 새로운 행렬의 1행 1열의 원소입니다.

- 이들을 python으로 구현하는 방식은 `np.dot()`과 `np.matmul()`입니다.

```python
# inner calculation of vectors
a = np.array([1, 2, 3])
b = np.array([1, 2, 3])
print(np.dot(a, b))

# multiple of matrix
A = np.array([[1, 2], [3, 4]])
B = np.array([[1, 2], [3, 4]])
print(np.matmul(A, B))
```

- 사실 두 연산 모두 `np.dot()`을 사용할 수 있으나 가능하면 둘을 구분하여 코드의 논리와 의도를 명확히하는 것이 좋습니다.

  - `np.dot()`과 `np.matmul()` 외에도 행렬 계산을 도와주는 편의 메서드는 많습니다. 이를 적절히 활용한다면 신경망 구현에 어려움이 없을 것입니다.

#### Note

> numpy를 익히는 가장 좋은 방법은 실제 코딩하며 연습하는 것이 제일입니다.
>
> numpy 경험을 쌓고 싶다면 '100 numpy exercises'를 추천합니다. 말 그대로 100개의 연습문제를 도전하며 실력을 쌓을 수 있습니다.

### 1.1.5 행렬 형상 확인

- 행렬이나 벡터를 사용해 계산할 때는 그 '형상'에 주의해야 합니다. 이는 행렬의 곱을 위한 것으로 다음처럼 '형상 확인'이 중요합니다.

<img src="README.assets/fig 1-6.png" alt="fig 1-6" style="zoom:50%;" />

- 위 그림은 (3, 2) 행렬 A와 (2, 4) 행렬 B를 곱해 (3, 4) 행렬 C를 만드는 예시로 이처럼 두 행렬의 대응하는 차원의 원소 수가 같아야 합니다.

  - 그래야 그 결과인 행렬의 형상은 피연산 행렬의 행의 수와 연산 행렬의 열의 수를 가지게 됩니다. 이게 '형상 확인'입니다.

#### Note

> 행렬의 곱 등 행렬 계산에 형상 확인을 해야만 신경망 구현을 부드럽게 진행할 수 있습니다.

## 1.2 신경망의 추론

- 이제 신경망을 복습해보겠습니다. 신경망의 작업은 크게 두 단계로 '학습'과 '추론'입니다.

> 우선 이번 절은 '추론'입니다.

### 1.2.1 신경망 추론 전체 그림

- 신경망은 간단히 말해 단순한 '함수'입니다. 무언가를 입력하면 반환하는 함수처럼 신경망도 입력에 대한 출력을 반환합니다.

- 이번 절에서는 2차원 데이터를 3차원으로 출력하는 함수를 예로 들겠습니다.

  - 이 함수를 신경망으로 구현하려면 **입력층**에 뉴런 2개를, **출력층**에 뉴런 3개를 준비합니다.

  - 그리고 **은닉층** 혹은 **중간층**에도 적당한 뉴런을 배치합니다.

  > 은닉층에 뉴런을 4개 둔 예시를 그리면 다음과 같습니다.

  <img src="README.assets/fig 1-7.png" alt="fig 1-7" style="zoom:50%;" />

- 위 그림은 뉴런을 원으로, 그 사이 연결을 화살표로 표현했습니다.

  - 화살표에는 **가중치**가 존재하여 가중치와 뉴런의 곱을 합쳐 다음 뉴런의 입력으로 사용합니다.

  > 정확하게는 그 합에 활성화 함수를 거쳐진 결과가 다음 뉴런의 입력이 됩니다.

  - 또한, 각 층에는 이전 뉴런의 값에 영향을 받지 않는 '정수'도 더해지는데 이를 **편향**이라고 합니다.

  - 덧붙여 위 그림의 신경망은 인접하는 층의 모든 뉴런이 서로 연결되어 있으므로 **완전연결계층**이라고 합니다.

#### Note

> 위 그림의 신경망은 3층 구성이지만 사실 가중치를 지나는 층은 2층 뿐입니다
>
> 그래서 이 책에서 위 신경망을 2층 신경망으로 부르겠습니다. 다만 문헌에 따라 부르는 방식은 다르니 유의해주세요.

- 그럼 위 신경망이 계산하는 수식에 대해 살펴보겠습니다.

  > 입력층 데이터를 (x<sub>1</sub>, x<sub>2</sub>), 가중치를 (w<sub>11</sub>, w<sub>21</sub>), 편향을 b<sub>1</sub>으로 쓰겠습니다.

  - 우선 은닉층 첫 번째 뉴런의 식은 다음과 같습니다.

  <img src="README.assets/e 1-2-1581685151846.png" alt="e 1-2" style="zoom:50%;" />

  - 이처럼 은닉층 뉴런은 가중치의 합으로 계산되며 이런 식으로 가중치와 편향 값을 바꿔가며 위 계산을 뉴런의 수만큼 반복해 은닉층에 속한 모든 뉴런의 값을 구합니다.

  - 가중치와 편향에는 인덱스가 붙는데 중요한 것은 이들이 가중치 합으로 계산되고 이는 행렬의 곱으로 한 번에 계산할 수 있다는 것입니다.

  > 실제 완전연결계층이 수행하는 변환은 다음과 같습니다.

  <img src="README.assets/e 1-3.png" alt="e 1-3" style="zoom:50%;" />

  - 은닉층 뉴런의 값이 좌변의 (1, 4) 행렬이며 입력은 (1, 2) 행렬, 가중치는 (2, 4) 행렬, 편향을 (1, 4) 행렬임을 알 수 있습니다.

  - 이를 더 간소화하면 다음과 같습니다.

  <img src="README.assets/e 1-4.png" alt="e 1-4" style="zoom:50%;" />

  - 여기서 **x**는 입력, **h**는 은닉층 뉴런, **W**는 가중치, **b**는 편향을 의미하고 모두 행렬입니다.

  - 이들의 형상은 모두 다음과 같이 변환됩니다.

  <img src="README.assets/fig 1-8.png" alt="fig 1-8" style="zoom:50%;" />

  - 이처럼 행렬의 곱은 대응하는 차원의 원소 수가 같아야 합니다.

#### Note

> 행렬의 곱 계산은 행렬의 형상 확인이 중요합니다. 형상을 보면 이 계산이 올바른지, 적어도 계산이 성립하는지를 알 수 있습니다.

- 이것으로 완전연결계층의 변환을 행렬로 정리하고 계산해봤습니다. 그러나 이는 하나의 입력 데이터만을 대상으로 한 변환입니다.

- 하지만 신경망의 추론과 학습은 다수의 샘플 데이터, 즉 미니배치를 한 번에 처리합니다.

- 이렇게 하기 위해서는 행렬 **x**의 행 각각에 샘플 데이터를 하나씩 저장해야합니다. 즉, N개의 미니배치에 대한 연산은 다음과 같습니다.

<img src="README.assets/fig 1-9.png" alt="fig 1-9" style="zoom:50%;" />

- 이처럼 형상 확인을 통해 각 미니배치가 정상적으로 변환되었는지 알 수 있습니다.

- 이 때 미니배치가 한 번에 완전연결계층에 의해 변환되고 은닉층에는 이만큼의 뉴런이 함께 계산됩니다.

> 이를 python으로 구현하면 다음과 같습니다.

```python
import numpy as np

W1 = np.random.randn(2, 4)  # weight
b1 = np.random.randn(4)  # bias
x = np.random.randn(10, 2)  # input
h = np.matmul(x, W1) + b1  # output
```

- 이처럼 10개의 미니배치를 완전연결계층으로 변환할 때 x의 헛 번째 차원이 각 샘플 데이터에 해당합니다.

- 마찬가지로 h의 각 인덱스에 인덱스에 해당하는 데이터의 은닉층 뉴런이 저장됩니다.

#### Warning

> 위 코드 마지막의 편향 b1은 브로드캐스드되어 더해집니다.

- 그런데 완전연결계층에 의한 변환은 '선형' 변환입니다. 여기에 '비선형' 효과를 부여하는 것이 바로 **활성화 함수**입니다.

- 더 자세히 말하자면 비선형 활성화 함수를 사용해 신경망의 표현력을 높일 수 있습니다.

  - 활성화 함수의 종류는 다양하나 대표적으로 **시그모이드 함수**를 사용하기도 합니다.

  <img src="README.assets/e 1-5.png" alt="e 1-5" style="zoom:50%;" />

  - 이 함수의 그래프는 다음과 같은 S자형을 그립니다.

  <img src="README.assets/fig 1-10.png" alt="fig 1-10" style="zoom:50%;" />

  - 시그모이드 함수는 임의의 실수를 입력받아 0부터 1사이의 실수를 출력합니다. 이를 구현하면 다음과 같습니다.

  ```python
  def sigmoid(x):
      return 1 / (1 + np.exp(-x))
  ```

- 이제 위 함수를 사용해 은닉층의 뉴런을 변환하여 비선형 변환이 가능해집니다.

> `a = sigmoid(h)`

- 이어서 이 활성화 함수의 출력인 **활성화**를 다른 완전연결계층에 통과시켜 변환합니다.

  - 위의 예시에 따르면 은닉층의 뉴런이 4, 출력층의 뉴런이 3이므로 완전연결계층에 사용되는 가중치의 행렬은 (4, 3) 형상입니다.

  > 이를 종합해 구현하면 다음과 같습니다.

  ```python
  import numpy as np

  def sigmoid(x):
  return 1 / (1 + np.exp(-x))

  x = np.random.randn(10, 2) # input
  W1 = np.random.randn(2, 4) # weight for 1st layer
  b1 = np.random.randn(4) # bias for 1st layer
  W2 = np.random.randn(4, 3) # 2nd layer
  b1 = np.random.randn(3) # 2nd layer

  h = np.matmul(x, W1) + b1
  a = sigmoid(h) # activation
  s = np.matmul(a, W2) + b2

  ```

- 위 코드를 해석하자면 x의 형상이 (10, 2), 즉 2차원 데이터가 10개 씩 미니배치로 처리된다는 의미입니다.

- 또한 최종 형상은 (10, 3)으로 3차원 데이터가 10개 씩 미니배치로 처리된 결과가 나온다는 의미입니다.

- 이 신경망은 3차원 데이터를 출력하는데 각 차원의 값을 이용해 세 클래스로 분류할 수 있습니다.

- 이 경우 출력된 3차원 벡터의 각 차원은 각 클래스에 대응하는 점수(첫 번째 뉴런이 첫 번째 클래스이듯)로 실제 분류한다면 출력층에서 가장 큰 값을 내는 뉴런에 해당하는 클래스가 예측 결과가 될 것입니다.

#### Note

> 점수란 '확률'이 되기 전의 값으로 점수가 높을수록 그 뉴런에 해당하는 클래스의 확률도 올라갑니다.
>
> 덧붙여 점수를 소프트맥스 함수에 입력하면 확률로 얻을 수 있습니다.

- 이상이 신경망의 추론이며 다음 절은 지금까지 수행한 처리를 하나의 '계층'으로 추상화한 python class를 구현할 것입니다.

### 1.2.2 계층으로 클래스화 및 순전파 구현

- 그럼 신경망에서 하는 처리를 계층으로 구현해봅시다.

  - 여기서는 완전연결계층에 의한 변환을 Affine 계층으로, 시그모이드 함수에 의한 변환을 Sigmoid 계층으로 구현할 것입니다.

    - 참고로 완전연결계층에 의한 변환은 기하학에서 Affine 변환에 해당하여 Affine 계층으로 부릅니다.

- 또한, 각 계층은 python class로 구현할 것이며 기본적인 변환을 수행하는 메서드의 이름은 forward()입니다.

#### Note

> 신경망 추론 과정에서 처리를 신경망의 **순전파**라고 합니다.
>
> 순전파란 말 그대로 입력층에서 출력층으로 향하는 전파로 이 때 신경망을 구성하는 각 계층이 입력에서 출력 방향으로 처리 결과를 차례로 전파합니다.
>
> 나중에 살펴볼 신경망 학습에서는 데이터(기울기)를 순전파와 반대 방향으로 보내는 데 이를 **역전파**라고 합니다.

- 신경망에서는 다양한 계층이 등장하는 데 이들을 모두 python class로 구현할 것입니다.

- 이토록 모듈화한 계층을 블록 조합하듯 신경망 구축에 사용할 수 있고 이를 위해 다음 '구현 규칙'을 따르겠습니다.

  - 모든 계층은 forward()와 backward() 메서드를 가지고 인스턴스 변수인 params와 grads를 가진다.

- 이 규칙을 간단히 설명하면 다음과 같습니다.

  - forward()와 backward()는 각각 순전파와 역전파를 의미하며 params는 가중치와 편향같은 매개변수를, grads는 params에 대응하는 기울기를 저장한 list입니다.

#### Note

> 이 규칙에 따라 구현하면 일관되고 확장성이 좋아집니다. 왜 이를 따르는 게 좋은 지는 나중에 밝혀집니다.

- 이번 절에서는 순전파만 구현할 것이므로 각 계층은 forward() 메서드와 params 인스턴스 변수를 가질 것입니다.

- 이를 통해 가장 먼저 구현할 계층은 Sigmoid 입니다.

> 1.2.2_forward_net.py의 class Sigmoid를 참고하세요.

- 시그모이드 함수는 주 변환 처리를 forward()에서 처리하며 해당 계층은 학습할 매개변수가 없으므로 빈 리스트로 초기화 합니다.

- 다음은 Affine 입니다.

> 1.2.2_forward_net.py의 class Affine을 참고하세요.

- Affine 계층은 초기화할 때 가중치와 편향을 받습니다. 즉, 이들은 매개변수로 신경망이 학습할 때 수시로 갱신됩니다. 그리고 forward로 순전파를 처리합니다.

#### Note

> 이 책의 예제 코드는 '구현 규칙'을 따르므로 모든 계층에 학습 매개변수가 params list에 들어갑니다.
>
> 덕분에 모든 매개변수를 간단히 정리할 수 있고 자연스럽게 매개변수 갱신이나 파일을 저장하는 것이 쉬워집니다.

- 그럼 이 계층들을 사용해 신경망의 추론을 처리해보겠습니다. 예시는 다음과 같습니다.

<img src="README.assets/fig 1-11.png" alt="fig 1-11" style="zoom:50%;" />

- 이처럼 입력 **x**가 Affine, Sigmoid, Affine 계층을 차례로 거쳐 점수 **s**를 출력합니다. 이를 TwoLayerNet이라는 클래스로 추상화하고 추론은 predict()로 합니다.

#### Note

> 앞으로 신경망 그림은 '뉴런 관점'의 그림이 아닌 '계층 관점'으로 생각해주세요.

- 그럼 TwoLayerNet의 구현은 다음과 같습니다.

> 1.2.2_forward_net.py의 class TwoLayerNet을 참고하세요.

- 이처럼 초기화를 통해 가중치 초기화와 계층 생성을 진행하고 학습할 매개변수들을 params에 저장합니다.

- 그리고 그 결과를 바탕으로 신경망의 추론을 진행하여 입력 데이터 x에 대한 점수 s를 구할 수 있습니다.

- 이처럼 학습할 모든 매개변수가 model, params에 모여있으므로 이어질 신경망 학습이 한결 쉬워지게 됩니다.

## 1.3 신경망의 학습

- 학습되지 않은 신경망은 '좋은 추론'을 할 수 없습니다. 그래서 학습을 먼저 수행하고 학습된 매개변수를 사용해 추론을 수행하는 흐름이 일반적입니다.

  - 추론이란 앞에서 본 것처럼 다중 클래스 분류 등의 문제에 답을 구하는 작업입니다.

- 한편, 신경망의 학습은 최적 매개변수를 찾는 작업으로 이번 절에서 이를 학습하는 방법을 살펴보겠습니다.

### 1.3.1 손실 함수

- 신경망 학습에는 학습이 얼마나 잘 되고 있는지를 알기 위한 '척도'가 필요합니다.

- 일반적으로 학습 단계의 특정 시점에서 신경망의 성능을 나타내는 척도로 **손실**을 사용합니다.

  - 손실이란 학습 데이터와 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출하는 단일 값입니다.

- 신경망의 손실은 **손실 함수**를 사용해 구합니다.

  - 다중 클래스 분류 신경망에서는 주로 **교차 엔트로피 오차**를 사용합니다.

  - 교차 엔트로피 오차란 신경망이 출력하는 각 클래스의 확률과 정답 레이블을 이용해 구할 수 있습니다.

- 그럼 지금껏 다뤄온 신경망에서 손실을 구해보겠습니다. 그에 앞서 신경망에 Softmax 계층과 Cross Entropy Error 계층을 새로 추가하면 '계층 관점'에서 다음과 같이 그려집니다.

<img src="README.assets/fig 1-12.png" alt="fig 1-12" style="zoom:50%;" />

- 입력 데이터 **x**, 정답 레이블 **t**, 손실 **L**을 기점으로 Softmax 계층의 출력은 확률로 다음 계층인 Cross Entropy Error 계층에 이 확률과 정답 레이블이 입력됩니다.

- 이어서 소프트맥스 함수와 교차 엔트로피 오차에 관해 알아보겠습니다. 우선 소프트맥스 함수의 식은 다음과 같습니다.

  <img src="README.assets/e 1-6.png" alt="e 1-6" style="zoom:50%;" />

  - 총 n개의 출력 중 k번째 출력 y<sub>k</sub>를 구하는 위 식은 k번째 클래스의 소프트맥스 함수의 출력입니다.

  - 이 식에서 보듯 소프트맥스 함수의 분자는 점수 s<sub>k</sub>의 지수 함수이고 분모는 모든 입력 신호의 지수 함수의 총합입니다.

  - 이를 통해서 소프트맥스 함수의 각 원소의 출력은 0과 1사이의 실수로 이 값을 모두 더하면 1이 나옵니다. 즉, 이를 통해 소프트맥스 함수의 출력을 '확률'로 해석할 수 있는 것입니다.

- 이제 이 입력값을 교차 엔트로피 오차에 입력하는데 그 수식은 다음과 같습니다.

  <img src="README.assets/e 1-7.png" alt="e 1-7" style="zoom:50%;" />

  - 여기서 t<sub>k</sub>는 k번째 클래스에 해당하는 정답 레이블로 log는 자연상수 e를 밑으로 하며 정답 레이블은 `t = [0, 0, 1]`과 같이 one-hot 벡터입니다.

#### Note

> one-hot 벡터란 단 하나의 원소만 1이고 나머지는 0인 벡터로 여기서 1인 원소가 정답 클래스에 해당합니다.
>
> 따라서 위의 식은 정답 레이블이 1의 원소에 해당하는 출력의 자연로그를 계산할 뿐입니다.

- 나아가 미니배치를 고려하면 교차 엔트로피 오차 식은 다음과 같이 확장되며 이 식에서 N개의 데이터를 입력해 n번째 데이터의 k번째 차원 값을 계산합니다.

  <img src="README.assets/e 1-8.png" alt="e 1-8" style="zoom:50%;" />

  - 위의 식은 조금 복잡해 보이지만 하나의 데이터에 대한 손실 함수를 나타낸 기존의 식을 N개의 데이터로 확장한 결과입니다.

  - 다만, N으로 나누어 1개당 '평균 손실 함수'를 구할 뿐입니다. 이렇게 평균을 구하면 미니배치의 크기와 상관없이 일관된 오차를 얻을 수 있습니다.

- 이 책에서 소프트맥스 함수와 교차 엔트로피 오차를 계산하는 계층을 모아 Softmax with Loss 계층으로 합쳐 계산합니다. 따라서 최종 신경망은 다음과 같습니다.

<img src="README.assets/fig 1-13.png" alt="fig 1-13" style="zoom:50%;" />

- 이처럼 이 책에서는 Softmax with Loss 계층을 사용하지만 그 구현은 따로 설명하지 않습니다.

  - 다만 궁금하신 분들은 chapter01/commons/layers.py의 해당 부분을 참고하시던가 이전 책의 4.2 손실 함수에서 확인하시면 됩니다.

### 1.3.2 미분과 기울기

- 신경망 학습의 목표는 손실을 최소화하는 매개변수를 찾는 것입니다. 이 때 중요한 것이 '미분'과 '기울기'로 이번 절에서는 이에 대해 간략히 설명하겠습니다.

- 어떤 함수 y = f(x)가 존재할 때 x에 대한 y의 미분은 dy/dx입니다. 이 값은 x가 조금 변할 때 y가 얼마나 변하는 가를 나타내는 '변화의 정도'입니다.

  - y = x<sup>2</sup> 함수를 예를 들면 dy/dx = 2x입니다. 즉, 다음과 같이 함수의 기울기를 의미합니다.

  <img src="README.assets/fig 1-14.png" alt="fig 1-14" style="zoom:50%;" />

- 이처럼 x라는 변수 하나에 대해 미분을 구한 것 처럼 여러개의 변수라도 미분을 할 수 있습니다.

  - **L**이라는 스칼라와 **x**라는 벡터가 있는 L = f(x)가 있을 때 x의 i번째 원소에 대한 미분은 dL/dx<sub>i</sub>로 쓸 수 있고 이를 다른 원소에 대해서도 정리하면 다음과 같습니다.

  <img src="README.assets/e 1-9.png" alt="e 1-9" style="zoom:50%;" />

- 이처럼 벡터의 각 원소에 대한 미분을 정리한 것이 **기울기**로 벡터와 마찬가지로 행렬에서도 기울기를 생각할 수 있습니다.

  - **W**라는 (m, n) 행렬이 존재하고 L = g(W) 함수의 기울기는 다음과 같습니다.

  <img src="README.assets/e 1-10.png" alt="e 1-10" style="zoom:50%;" />

- 즉, 이처럼 L의 W에 대한 기울기를 행렬로 정이할 수 있고 여기서 중요한 것은 W와 dL/dW의 형상이 같다는 점입니다.

- 그리고 이 성질을 이용하면 매개변수 갱신과 연쇄 법칙을 구현할 수 있습니다.

#### Warning

> 엄밀하게 말해 이 책에서 의미하는 '기울기'는 수학의 기울기와 다릅니다.
>
> 수학의 기울기가 벡터에 대한 미분으로 한정되는 반면 딥러닝은 행렬이나 텐서에 대해서도 미분을 정의하고 이를 기울기라 부릅니다.

### 1.3.3 연쇄 법칙

- 학습 시 신경망은 학습 데이터에 대한 손실을 출력합니다. 여기서 우리가 원하는 것은 각 매개변수에 대한 손실의 기울기입니다.

  - 그 기울기를 얻을 수 있다면 이를 통해 매개변수를 갱신할 수 있기 때문입니다. 이 때 이를 구하는 방법이 바로 **오차역전파법**입니다.

- 오차역전파법은 **연쇄 법칙**에 기인하는데 이 법칙은 합성 함수에 대한 미분의 법칙입니다.

  - 연쇄 법칙을 더 자세히 설명하면 y = f(x)와 z = g(y)라는 함수가 존재할 때 이 둘은 z = g(f(x))로 쓸 수 있으며 z는 두 함수를 조합해 계산할 수 있습니다.

  - 이 때 이 합성 함수의 미분은 다음과 같습니다.

  <img src="README.assets/e 1-11.png" alt="e 1-11" style="zoom:50%;" />

- 즉, 이처럼 x에 대한 z의 미분은 y = f(x)의 미분과 z = g(y)의 미분을 곱하면 구할 수 있으며 이를 연쇄 법칙이라고 합니다.

- 연쇄 법칙이 중요한 이유는 함수가 아무리 복잡하더라도 그 미분은 개별 함수의 미분들을 이용해 구할 수 있기 때문입니다.

  - 다르게 말해 각 함수의 국소 미분을 계산할 수 있다면 그를 곱해 전체 미분을 구할 수 있습니다.

#### Note

> 신경망은 여러 '함수'가 연결된 것입니다. 따라서 오차역전파법은 여러 함수에 대해 연쇄 법칙을 효율적으로 적용해 기울기를 계산합니다.

### 1.3.4 계산 그래프

- 곧이어 오차역전파법을 살펴보기 전에 '계산 그래프'를 설명하겠습니다. 계산 그래프는 계산 과정을 시각적으로 보여주는 것이며 다음과 같습니다.

<img src="README.assets/fig 1-15.png" alt="fig 1-15" style="zoom:50%;" />

- 이처럼 계산 그래프는 노드와 화살표로 그리며 위의 예시처럼 더하기를 '+' 노드로 나타내고 변수 x, y를 화살표 위에 작성했습니다.

- 계산 그래프는 연산을 노드로 나타내고 처리 결과가 순서대로 흐릅니다. 이것이 계산 그래프의 '순전파'입니다.

- 게다가 기울기도 직관적으로 구할 수 있으며 여기서 중요한 점은 기울기가 순전파와 반대 방향으로 전파된다는 것인데 이것이 바로 '역전파'입니다.

- 역전파를 설명하기 앞서 역전파가 이뤄지는 전체 그림을 더 명확하게 하기 위해 계산 앞 뒤로 어떤 계산이 존재하는 z = x + y 노드에 대해 알아보겠습니다.

  - 이 계산의 최종 결과는 스칼라 **L**으로 가정할 것입니다.

  <img src="README.assets/fig 1-16.png" alt="fig 1-16" style="zoom:50%;" />

  - L의 미분을 각 변수에 대해 구하기 위한 계산 그래프의 역전파는 다음과 같습니다.

  <img src="README.assets/fig 1-17.png" alt="fig 1-17" style="zoom:50%;" />

  - 역전파는 두꺼운 화살표로 그리고 그 아래에 전파되는 값을 씁니다. 이 때 전파되는 값은 L에 대한 각 변수의 미분입니다.

    - 그리고 여기서 다시 연쇄 법칙이 등장하는데 역전파로 흐르는 미분 값은 상류에서 흘러온 미분에 각 연산 노드의 국소 미분을 곱해 계산할 수 있습니다.

    - 이 예에서는 dL/dx = dL/dz _ dz/dx, dL/dy = dL/dz _ dz/dy 인데 z = x + y의 덧셈 노드에 대한 연산은 dz/dx = dz/dy = 1 입니다.

    - 즉, 덧셈 노드는 상류에서 받은 값에 1을 곱해 하류로 기울기를 전파합니다. 즉, 상류의 기울기를 흘립니다.

    <img src="README.assets/fig 1-18.png" alt="fig 1-18" style="zoom:50%;" />

- 이처럼 계산 그래프는 계산을 시각적으로 보여줍니다. 그리고 역전파에 의한 기울기 흐름을 살펴보아 도출 과정을 이해하는 데 도움을 줍니다.

- 이제 대표적인 연산 노드와 그 계산을 소개하겠습니다.

#### 곱셈 노드

- 곱셈 노드는 z = x \* y를 수행하며 dz/dx = y, dz/dy = x로 곱셈 노드의 역전파는 상류의 기울기에 순전파 시 입력을 서로 바꾼 값을 곱해 흘려 보냅니다.

<img src="README.assets/fig 1-19.png" alt="fig 1-19" style="zoom:50%;" />

- 참고로 덧셈 노드와 곱셈 노드 모두 벡터나 행렬, 텐서와 같은 다변수를 흘려도 문제가 없습니다. 이 경우 각 원소를 독립적으로 계산하는 '원소별 연산'을 사용합니다.

#### 분기 노드

- 분기 노드는 다음과 같이 분기되는 노드입니다.

<img src="README.assets/fig 1-20.png" alt="fig 1-20" style="zoom:50%;" />

- 분기 노드는 따로 그리지 않고 선이 두 개로 나뉘도록 그립니다. 이 때 같은 값이 복제되어 분기합니다.

- 따라서 분기 노드는 복제 노드라고 할 수 있으며 그 역전파는 상류의 기울기들의 합입니다.

#### Repeat 노드

- N개로 분기되는 노드를 Repeat 노드라고 합니다. 이는 다음과 같습니다.

<img src="README.assets/fig 1-21.png" alt="fig 1-21" style="zoom:50%;" />

- Repeat 노드는 N개의 분기 노드로 볼 수 있으므로 역전파는 N개의 기울기를 모두 더하면 됩니다. 이를 코드로 구현하면 다음과 같습니다.

```python
import numpy as np

D, N = 8, 7
x = np.random.randn(1, D)  # input
y = np.repeat(x, N, axis=0)  # forward

dy = np.random.randn(N, D)  # random grad
dx = np.sum(dy, axis=0, keepdims=True)  # backpropagation
```

- 여기서 np.repeat() 메서드가 원소 복제를 수행하고 axis를 지정해 어느 축으로 복제할 지 결정합니다.

- 반면 역전파에서 np.sum()을 통해 총합을 구하고 keepdims=True를 사용해 2차원 배열의 차원 수를 유지합니다.

#### Note

> numpy의 브로드캐스트는 배열의 원소를 복제하여 Repeat 노드를 사용해 이 기능을 표현할 수 있습니다.

#### Sum 노드

- Sum 노드는 범용 덧셈 노드로 (N, D) 배열에 대해 총합을 0축에 구하는 계산의 예를 통해 이를 알아볼 수 있습니다.

<img src="README.assets/fig 1-22.png" alt="fig 1-22" style="zoom:50%;" />

- 이처럼 Sum 노드의 역전파는 상류의 기울기를 모두 분재하는 것으로 덧셈 노드의 역전파를 확장한 것입니다. 코드로는 다음과 같습니다.

```python
import numpy as np

D, N = 8, 7
x = np.random.randn(N, D)  # input
y = np.sum(x, axis=0, keepdims=True)  # forward

dy = np.random.randn(1, D)  # random grad
dx = np.repeat(dy, N, axis=0)  # backpropagation
```

- Sum 노드의 순전파는 np.sum(), 역전파는 np.repeat() 메서들 구현하며 이는 Sum 노드가 Repeat 노드의 반대 관계임을 보여줍니다.

#### MatMul 노드

- 행렬의 곱셈인 MatMul 노드는 역전파 과정이 복잡하므로 일반적인 설명 후에 직관적인 이해를 돕는 설명을 더하겠습니다.

  - **y = x \* W**라는 계산을 예로 들겠습니다. 여기서 **x, W, y**의 형상은 (1, D), (D, H), (1, H)입니다.

<img src="README.assets/fig 1-23.png" alt="fig 1-23" style="zoom:50%;" />

- 이 때 **x**의 i번째 원소에 대한 미분 dL/dx<sub>i</sub>는 다음과 같습니다.

<img src="README.assets/e 1-12.png" alt="e 1-12" style="zoom:50%;" />

- 이처럼 x<sub>i</sub>를 변화시키면 벡터 **y**의 모든 원소가 변하고 최종적으로 **L**이 변합니다. 따라서 이들의 총합이 미분이 됩니다.

  - 여기서 dy<sub>j</sub>/dx<sub>i</sub> = W<sub>ij</sub>이 되므로 이를 사용해 식을 다음과 같이 고칠 수 있습니다.

<img src="README.assets/e 1-13.png" alt="e 1-13" style="zoom:50%;" />

- 즉, L에 대한 미분이 dL/d**y**와 **W**의 i행 벡터의 내적으로 구해짐을 알 수 있으므로 이처럼 바꿀 수 있습니다.

<img src="README.assets/e 1-14.png" alt="e 1-14" style="zoom:50%;" />

- 여기서 **W<sup>T</sup>**는 전치행렬을 의미하며 '형상 확인'을 하면 다음과 같습니다.

<img src="README.assets/fig 1-24.png" alt="fig 1-24" style="zoom:50%;" />

- 행렬의 형상이 올바르므로 위 식은 성립함을 알 수 있고 이를 역으로 취해 역전파의 수식을 유도할 수 있습니다.

  - 동일하게 **y = x \* W**를 예로 들고 미니배치를 고려해 **x**에 N개의 데이터가 있다고 하면 **x, W, y**의 형상은 (N, D), (D, H), (N, H)가 되고 역전파는 다음과 같습니다.

<img src="README.assets/fig 1-25.png" alt="fig 1-25" style="zoom:50%;" />

- 이제 dL/d**x**를 구하는 방법은 dL/d**y**와 **W**를 연결지어 생각하면 됩니다.

  - 이는 곱셈의 역전파처럼 행렬의 역전파에서도 순전파 시 입력을 서로 바꾼 행렬을 사용한다는 점에 주목해 정합성이 유지되도록 행렬 곱을 조합하면 됩니다.

<img src="README.assets/fig 1-26.png" alt="fig 1-26" style="zoom:50%;" />

- 이 노드를 하나의 계층으로 구현하면 완성입니다.

> chapter01/commons/layers.py의 class MatMul을 확인하세요.

- MatMul 계층은 학습하는 매개변수를 params에 보관하고 그에 대응하는 형태로 기울기를 grads에 보관합니다.

- 반면 역전파에서 dx와 dW를 구해 가중치의 기울기를 인스턴스 변수인 grads에 저장합니다.

> 해당 코드에 사용된 생략기호 ...는 numpy 배열이 가리키는 메모리 위치를 고정해 덮어씌웁니다.

#### Warning

> 할당을 하지 않고 덮어쓰기를 진행하는 이유는 얕은 복사가 아닌 깊은 복사를 하기 위함입니다.

- 생략 기호에 대해 더 자세히 설명하기 위해 다음 코드를 통해 살펴보겠습니다.

```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
```

- 이 경우 a = b와 a[...] = b 모두 a에 [4, 5, 6]이 할당되지만 두 경우 a가 가리키는 메모리 위치가 다릅니다.

<img src="README.assets/fig 1-27.png" alt="fig 1-27" style="zoom:50%;" />

- a = b는 둘 모두 가리키는 메모리 위치가 같아지는 얕은 복사가 수행됩니다. 반면 a[...] = b는 a의 메모리에 b가 복제되는 깊은 복사가 수행됩니다.

  - 즉, 생략 기호를 사용해 변수의 메모리 주소를 고정할 수 있고 이를 통해 인스턴스 변수 grads를 다루기 더 쉬워집니다.

#### Note

> grads 리스트에는 각 매개변수의 기울기를 저장하며 각 원소는 numpy 배열로 계층을 생성할 때 한 번만 생성합니다.
>
> 그 이후로 항상 생략 기호를 사용하므로 메모리 주소가 변하는 일 없이 항상 값을 덮어씁니다. 이 경우 기울기를 그룹화하는 작업을 최초 한 번만 하면 됩니다.

### 1.3.5 기울기 도출과 역전파 구현

- 이제 실용적인 계층인 Sigmoid, Affine, Softmax with Loss 계층을 구현합니다.

#### Sigmoid 계층

- 시그모이드 함수의 수식에 따른 미분은 다음과 같습니다.

<img src="README.assets/e 1-15.png" alt="e 1-15" style="zoom:50%;" />

- 이에 따른 계산 그래프를 그리면 다음과 같습니다.

<img src="README.assets/fig 1-28.png" alt="fig 1-28" style="zoom:50%;" />

#### Note

> 여기에서는 시그모이드 함수의 미분 도출 과정을 생략했습니다. 자세한 과정은 'Appedix A'에서 확인해주세요.

- 그럼 Sigmoid 계층을 python으로 구현다면 다음과 같습니다.

> 자세한 내용은 chapter01/commons/layers.py의 class Sigmoid를 확인하세요.

#### Affine 계층

- Affine 계층의 순전파는 y = np.matmul(x, W) + b로 구현할 수 있으며 편향은 numpy의 브로드캐스트 기능으로 계산됩니다. 이를 토대로 그린 Affine 계층의 계산 그래프는 다음과 같습니다.

<img src="README.assets/fig 1-29.png" alt="fig 1-29" style="zoom:50%;" />

- 이처럼 MatMul 노드로 행렬의 곱을 계산하고 편향이 Repeat 노드에 의해 복제된 후 더해집니다. 이를 코드로 작성하면 다음과 같습니다.

> chapter01/commons/layers.py의 class Affine을 확인하세요.

- 이 책의 규칙에 따라 인스턴스 변수 params에 매개변수를, grads에 기울기를 저장하고 각 노드의 역전파를 통해 역전파를 계산합니다.

  - Repeat 노드의 역전파는 np.sum()으로 계산하는데 이 때 행렬의 형상을 잘 보도 어느 axis로 합을 구할지 명시해야합니다.

#### Warning

> Affine 계층은 이미 구현한 MatMul 계층을 이용하면 더 쉽게 구현할 수 있습니다.
>
> 이번 절에서는 미리 구현한 MatMul 계층을 이용하지 않고 numpy의 메서드를 사용했습니다.

#### Softmax with Loss 계층

- 소프트맥스 함수와 교차 엔트로피 오차 함수는 Softmax with Loss라는 하나의 계층으로 구현합니다. 이 계산 그래프는 다음과 같습니다.

<img src="README.assets/fig 1-30.png" alt="fig 1-30" style="zoom:50%;" />

- 여기서 소프트맥스 함수는 Softmax 계층으로, 교차 엔트로피 오차는 Cross Entropy Error 계층으로 구현되었습니다.

  - Softmax 계층은 입력을 정규화해서 출력하고 Cross Entropy Error 계층은 Softmax 계층의 출력과 정답 레이블을 비교해 손실을 구해 출력합니다.

#### Note

> 여기서 주목할 부분은 역전파의 결과입니다.
>
> Softmax 계층의 역전파는 출력값과 정답 레이블의 차이로 깔끔하게 계산됩니다.
>
> 이처럼 신경망의 역전파는 이 차이를 앞 계층에 전해주는 신경망 학습에 아주 중요한 성질입니다.

- 이를 구현한 코드는 chapter01/commons/layers.py에 수록되어 있으며 역전파 유도 과정은 이전 책의 'Appedix A'에 수록되어 있습니다.

### 1.3.6 가중치 갱신

- 오차역전파법으로 기울기를 구했다면 이를 이용해 신경망의 매개변수를 갱신하는데 그 순서는 다음과 같습니다.

  1. 미니배치: 훈련 데이터 중 무작위로 다수의 데이터를 고릅니다.

  2. 기울기 계산: 오차역전파법으로 각 가중치 매개변수에 대한 손실 함수의 기울기를 구합니다.

  3. 매개변수 갱신: 기울기를 이용해 매개변수를 갱신합니다.

  4. 반복: 1 ~ 3단계를 필요한 만큼 반복합니다.

- 이를 거쳐 신경망 학습이 이루어집니다. 여기서 기울기는 현재 가중치 매개변수에서 손실을 가장 크게 하는 방향을 의미합니다.

  - 따라서, 매개변수를 그 기울기와 반대 방향으로 갱신하면 손실을 줄일 수 있는데 이를 **경사하강법**이라고 합니다.

- 3단계에서 수행하는 가중치 갱신 기법의 종류는 다양한데 그중 가장 단순한 **확률적경사하강법** 즉, **SGD**룰 구현하겠습니다.

  - SGD는 단순한 방법으로 현재 가중치를 기울기 방향으로 일정 거리만큼 갱신함을 의미합니다.

  <img src="README.assets/e 1-16.png" alt="e 1-16" style="zoom:50%;" />

  - 이 식에서 가중치 매개변수가 **W**, 손실 함수의 기울기가 dL/d**W**이며 eta는 학습률을 의미합니다.

- 이를 python으로 구현해보겠습니다. 자세한 내용은 chapter01/commons/optimizer.py의 class SGD에 기록되어 있습니다.

  - 매개변수를 갱신하는 클래스는 update(params, grads)라는 메서드를 통해 신경망의 가중치를 갱신합니다.

  - 여기서 lr은 학습률로 이를 사용해서 매개변수의 갱신을 결정합니다.

- 이를 사용해 신경망의 매개변수 갱신을 행하면 다음과 같습니다.

```python
from .layers import TwoLayerNet
from .optimizers import SGD

model = TwoLayerNet()
optimizer = SGD()

# here are some codes
from i in range(10000):
    # here are some codes
    x_batch, t_batch = get_mini_batch(...)  # set mini batch
    loss = model.forward(x_batch, t_batch)
    model.backward()
    optimizer.update(model.params, models.grads)
    # here are some codes
```

- 이처럼 최적화를 수행하는 클래스를 분리해 구현하면 기능을 쉽게 모듈화할 수 있습니다.

  - 이 외에도 다양한 최적화 기법들이 존재하며 자세한 내용은 이전 책의 '6.1 매개변수 갱신'을 참고하세요.

## 1.4 신경망으로 문제를 풀다

### 1.4.1 스파이럴 데이터셋

- 이 책에서는 데이터셋을 다루는 편의 클래스 몇 개를 datasets 디렉터리에 준비했습니다. 이번 절에서는 그 중 datasets/spiral.py를 사용합니다.

  - 이 파일에는 나선형 데이터, 즉 스파이럴을 읽는 클래스가 구현되어있습니다.

  > 자세한 내용은 chapter01/1.4.1_show_spiral_dataset.py를 확인하세요.

  - 해당 파일의 결과는 다음과 같습니다.

  <img src="README.assets/fig 1-31.png" alt="fig 1-31" style="zoom:50%;" />

  - 2차원 입력 데이터에 분류할 클래스가 3개인 데이터셋으로 이를 보면 직선만으로 이들을 분류할 수 없음을 알 수 있습니다.

- 그렇다면 비선형 분리를 진행하면 올바르게 진행되는지 확인해보도록 하겠습니다.

#### Warning

> 실전에서는 데이터셋을 훈련용, 테스트용, 검증용으로 분리하여 학습과 평가를 수행하지만 이 단계에서는 간단한 실험을 위해 생략했습니다.

### 1.4.2 신경망 구현

- 그렇다면 신경망을 구현해보겠습니다. 이번 절의 신경망은 은닉층이 하나인 신경망으로 다음과 같습니다.

> 자세한 내용은 chapter01/1.4.2_two_layer_net.py를 확인하세요.

#### Warning

> Softmax with Loss 계층은 다른 계층과 다르게 취급하여 layers 리스트가 아닌 loss_layer에 따로 저장합니다.

### 1.4.3 학습용 코드

- 이어서 학습을 수행해보겠습니다.

  - 여기서는 학습 데이터를 읽고 신경망과 옵티마이저를 생성하여 앞 선 네 단계의 절차대로 학습을 수행합니다.

> 자세한 내용은 chapter01/1.4.3_train_custom_loop.py를 확인하세요.

#### Warning

> epoch은 학습 단위로 학습 데이터를 모두 살펴본 시점(데이터셋을 한 바퀴 돌아본 시점)을 의미합니다.

- 중간에 사용되는 np.random.permutation()은 데이터를 뒤섞어 무작위 순서를 반환합니다.

```python
import numpy as np

print(np.random.permutation(10))  # 이 결과와
print(np.random.permutation(10))  # 이 결과는 같을 수도 아닐 수도 있습니다.
```

#### Warning

> 여기서 구현한 신경망의 학습 코드는 다른 곳에서도 활용되어 Trainer class로도 만들어져 있습니다.
>
> 자세한 내용은 1.4.4 Trainer 클래스에서 확인하세요.

- 이제 이 코드를 실행해보면 손실 값이 다음처럼 내려가는 것을 확인할 수 있습니다.

<img src="README.assets/fig 1-32.png" alt="fig 1-32" style="zoom:50%;" />

- 이는 신경망이 올바른 방향으로 학습됨을 알려주는 것입니다. 그렇다면 **결정 경계**를 통해 신경망이 영역을 분리한 모습도 확인해보세요.

<img src="README.assets/fig 1-33.png" alt="fig 1-33" style="zoom:50%;" />

- 이처럼 신경망이 나선형 패턴을 올바르게 학습했음을 알 수 있습니다.

  - 여기세 신경망에 은닉층을 추가할수록 더 복잡한 표현이 가능해지며 표현력이 더 풍부해집니다.

### 1.4.4 Trainer 클래스

- 앞에서 언급했듯이 이 책에서 반복되어 사용되는 코드가 많습니다. 그 중 하나인 Trainer를 클래스로 만들고 새로운 기능을 조금 추가하면 다음과 같습니다.

> 자세한 내용은 chapter01/commons/trainer.py를 확인하세요.

- 해당 클래스는 fit() 메서드를 활용해 학습을 시작하는데 해당 메서드가 받는 인수는 다음과 같습니다.

<img src="README.assets/table 1-1.png" alt="table 1-1" style="zoom:50%;" />

- 또한 해당 클래스는 plot() 메서드를 제공하는데 이는 fit()에서 기록한 손실을 그래프로 보여줍니다.

  - 그렇다면 이를 실제로 실행하는 코드를 확인해보세요.

  > 자세한 내용은 chapter01/1.4.4_train.py를 확인하세요.

- 이렇게 작성하면 이전보다 신경망 학습이 더 깔끔한 코드를 사용해 이루어집니다. 앞으로는 이런 식으로 학습을 진행하겠습니다.
