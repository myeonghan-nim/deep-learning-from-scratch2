# chapter01. Review Neural Network

- 이 책은 전편에 이어 딥러닝의 가능성을 한층 더 깊이 탐험할 것입니다.

  - 물론, 전편과 동일하게 라이브러리나 프레임워크 는 최대한 사용하지 않고 밑바닥부터 구현할 것입니다.

- 이번 장에서는 신경망을 복습합니다. 전편의 내용을 요약한 장이기도 합니다.

  - 다만 차이점은 효율 향상을 위해 전편의 구현 규칙을 일부 변경했습니다.

## 1.1 수학과 python 복습

- 먼저 수학, 즉 신경망 계산에 필요한 '벡터'나 '행렬'에 대해 복습하겠습니다. 또, 신경망을 원할히 구현하기 위한 python, 특히 numpy 코드도 되새겨볼 것입니다.

### 1.1.1 벡터와 행렬

- 신경망은 벡터와 행렬(또는 텐서)가 도처에서 등장하니 이를 가볍게 정리하겠습니다.

  - 벡터는 크기와 방향을 가진 양으로 숫자가 일렬로 늘어선 집합으로 표현할 수 있습니다.

    - 즉, 파이썬에서는 1차원 배열로 취급할 수 있습니다.

  - 반면에 행렬을 숫자가 2차원 형태로 늘어선 것으로 다음과 같은 차이를 가집니다.

  <img src="README.assets/fig 1-1.png" alt="fig 1-1" style="zoom:50%;" />

- 이처럼 벡터는 1차원, 행렬은 2차원 배열로 표현할 수 있으며 행렬의 가로를 행, 세로를 열이라고 합니다.

#### Note

> 벡터와 행렬을 확장해 숫자 집합을 N차원으로 표현한 것은 일반적으로 텐서라고 합니다

- 벡터는 단순한 개념이지만 이를 표현하는 방법이 행과 열, 두 가지로 나뉩니다.

<img src="README.assets/fig 1-2.png" alt="fig 1-2" style="zoom:50%;" />

- 수학과 딥러닝 등 많은 분야에서 '열벡터' 방식을 선호하지만, 이 책에서는 구현의 편의를 고려해 '행벡터'로 다루겠습니다.

  - 또한, 수식에서 벡터와 행렬은 **x**나 **W**처럼 굵게 강조하여 단일 원소로 구성된 스칼라와 구분하겠습니다.

#### Warning

> python으로 벡터를 행벡터로 구현할 때, 벡터를 가로 방향 행렬로 변환해 사용하면 보다 명확해집니다.
>
> 예를 들어 원소 수가 N개인 벡터는 1 \* N 형상의 행렬로 처리합니다.

- 그럼 python을 통해 벡터와 행렬에 대해 알아보겠습니다.

```python
import numpy as np

x = np.array([1, 2, 3])
print('class:', x.__class__)
print('shape:', x.shape)
print('dimension:', x.dim)

W = np.array([[1, 2, 3], [4, 5, 6]])
print('shape:', W.shape)
print('dimension:', W.dim)
```

- 이처럼 벡터와 행렬 모두 `np.array()`로 생성할 수 있습니다.

  - 이 메서드는 `np.ndarray` 클래스를 생성하며 이 클래스에는 다양한 메서드와 인스턴스 변수가 내장되어 있습니다.

    - 앞선 예제는 인스턴스 변수 중 `shape`와 `dim`을 사용했고 각각 다차원 배열의 형상과 차원의 수를 의미합니다.

- 결과를 보면 x는 1차원 배열에 원소 수가 3개인 벡터이고 W는 2차원 배열에 2행 3열의 행렬임을 알 수 있습니다.

### 1.1.2 행렬의 원소별 연산

- 수의 집합을 벡터나 행렬로 표현했다면 이를 간단히 계산하는 과정은 다음과 같습니다. 우선 원소별 연산입니다.

```python
X = np.array([[1, 2, 3], [4, 5, 6]])
W = np.array([[2, 4, 6], [1, 3, 5]])
print(X + W)
print(X * W)
```

- 다차원 numpy의 더하기와 곱하기를 위와 같이 하면 피연산자인 다차원 배열들에서 서로 대응하는 원소끼리 독립적인 연산이 이루어집니다.

### 1.1.3 브로드캐스트

- numpy의 다차원 배열에서 형상이 다른 배열끼리도 연산할 수 있는데 이를 브로드캐스트라고 합니다.

```python
A = np.array([[1, 2], [3, 4]])
print(A * 10)
```

- 위 계산에서 2행 2열의 행렬에 스칼라 10을 곱하면 스칼라 값이 동일한 크기의 행렬로 확장된 후 원소별 연산을 수행합니다.

<img src="README.assets/fig 1-3.png" alt="fig 1-3" style="zoom:50%;" />

- 또 다른 예시는 다음과 같습니다.

```python
A = np.array([[1, 2], [3, 4]])
b = np.array([10, 20])
print(A * b)
```

- 위 계산도 1차원 배열이 2차원 배열로 형상이 같아지도록 확장된 수 이를 계산합니다.

<img src="README.assets/fig 1-4.png" alt="fig 1-4" style="zoom:50%;" />

#### Warning

> numpy의 브로드캐스트가 효과적으로 동작하려면 다차원 배열의 형상이 몇 가지 규칙을 충족해야합니다.
>
> 자세한 내용은 문헌을 참고하세요.

### 1.1.4 벡터의 내적과 행렬의 곱

- 다음은 벡터의 내적과 행렬의 곱입니다. 우선 벡터의 내적은 다음과 같이 진행됩니다.

<img src="README.assets/e 1-1.png" alt="e 1-1" style="zoom:50%;" />

- 이처럼 2개의 벡터가 존재할 때 백터의 내적은 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것과 같습니다.

#### Note

> 벡터의 내적은 직관적으로 '두 벡터가 얼마나 같은 방향을 향하고 있는가'를 의미합니다.
>
> 벡터의 길이가 1인 경우, 완전히 같은 방향을 향하는 두 벡터의 내적은 1이지만 반대를 향하는 두 벡터의 내적은 -1입니다.

- 계속해서 행렬의 곱은 다음과 같은 수서로 계산됩니다.

<img src="README.assets/fig 1-5.png" alt="fig 1-5" style="zoom:50%;" />

- 이처럼 행렬의 곱은 왼쪽 행렬의 행벡터와 오른쪽 행렬의 열벡터의 내적으로 계산하고 그 결과를 새로운 행렬의 대응하는 원소에 저장합니다.

  - 예를 들어 A의 1행과 B의 1열의 연산 결과는 새로운 행렬의 1행 1열의 원소입니다.

- 이들을 python으로 구현하는 방식은 `np.dot()`과 `np.matmul()`입니다.

```python
# inner calculation of vectors
a = np.array([1, 2, 3])
b = np.array([1, 2, 3])
print(np.dot(a, b))

# multiple of matrix
A = np.array([[1, 2], [3, 4]])
B = np.array([[1, 2], [3, 4]])
print(np.matmul(A, B))
```

- 사실 두 연산 모두 `np.dot()`을 사용할 수 있으나 가능하면 둘을 구분하여 코드의 논리와 의도를 명확히하는 것이 좋습니다.

  - `np.dot()`과 `np.matmul()` 외에도 행렬 계산을 도와주는 편의 메서드는 많습니다. 이를 적절히 활용한다면 신경망 구현에 어려움이 없을 것입니다.

#### Note

> numpy를 익히는 가장 좋은 방법은 실제 코딩하며 연습하는 것이 제일입니다.
>
> numpy 경험을 쌓고 싶다면 '100 numpy exercises'를 추천합니다. 말 그대로 100개의 연습문제를 도전하며 실력을 쌓을 수 있습니다.

### 1.1.5 행렬 형상 확인

- 행렬이나 벡터를 사용해 계산할 때는 그 '형상'에 주의해야 합니다. 이는 행렬의 곱을 위한 것으로 다음처럼 '형상 확인'이 중요합니다.

<img src="README.assets/fig 1-6.png" alt="fig 1-6" style="zoom:50%;" />

- 위 그림은 (3, 2) 행렬 A와 (2, 4) 행렬 B를 곱해 (3, 4) 행렬 C를 만드는 예시로 이처럼 두 행렬의 대응하는 차원의 원소 수가 같아야 합니다.

  - 그래야 그 결과인 행렬의 형상은 피연산 행렬의 행의 수와 연산 행렬의 열의 수를 가지게 됩니다. 이게 '형상 확인'입니다.

#### Note

> 행렬의 곱 등 행렬 계산에 형상 확인을 해야만 신경망 구현을 부드럽게 진행할 수 있습니다.

## 1.2 신경망의 추론

- 이제 신경망을 복습해보겠습니다. 신경망의 작업은 크게 두 단계로 '학습'과 '추론'입니다.

> 우선 이번 절은 '추론'입니다.

### 1.2.1 신경망 추론 전체 그림

- 신경망은 간단히 말해 단순한 '함수'입니다. 무언가를 입력하면 반환하는 함수처럼 신경망도 입력에 대한 출력을 반환합니다.

- 이번 절에서는 2차원 데이터를 3차원으로 출력하는 함수를 예로 들겠습니다.

  - 이 함수를 신경망으로 구현하려면 **입력층**에 뉴런 2개를, **출력층**에 뉴런 3개를 준비합니다.

  - 그리고 **은닉층** 혹은 **중간층**에도 적당한 뉴런을 배치합니다.

  > 은닉층에 뉴런을 4개 둔 예시를 그리면 다음과 같습니다.

  <img src="README.assets/fig 1-7.png" alt="fig 1-7" style="zoom:50%;" />

- 위 그림은 뉴런을 원으로, 그 사이 연결을 화살표로 표현했습니다.

  - 화살표에는 **가중치**가 존재하여 가중치와 뉴런의 곱을 합쳐 다음 뉴런의 입력으로 사용합니다.

  > 정확하게는 그 합에 활성화 함수를 거쳐진 결과가 다음 뉴런의 입력이 됩니다.

  - 또한, 각 층에는 이전 뉴런의 값에 영향을 받지 않는 '정수'도 더해지는데 이를 **편향**이라고 합니다.

  - 덧붙여 위 그림의 신경망은 인접하는 층의 모든 뉴런이 서로 연결되어 있으므로 **완전연결계층**이라고 합니다.

#### Note

> 위 그림의 신경망은 3층 구성이지만 사실 가중치를 지나는 층은 2층 뿐입니다
>
> 그래서 이 책에서 위 신경망을 2층 신경망으로 부르겠습니다. 다만 문헌에 따라 부르는 방식은 다르니 유의해주세요.

- 그럼 위 신경망이 계산하는 수식에 대해 살펴보겠습니다.

  > 입력층 데이터를 (x<sub>1</sub>, x<sub>2</sub>), 가중치를 (w<sub>11</sub>, w<sub>21</sub>), 편향을 b<sub>1</sub>으로 쓰겠습니다.

  - 우선 은닉층 첫 번째 뉴런의 식은 다음과 같습니다.

  <img src="README.assets/e 1-2-1581685151846.png" alt="e 1-2" style="zoom:50%;" />

  - 이처럼 은닉층 뉴런은 가중치의 합으로 계산되며 이런 식으로 가중치와 편향 값을 바꿔가며 위 계산을 뉴런의 수만큼 반복해 은닉층에 속한 모든 뉴런의 값을 구합니다.

  - 가중치와 편향에는 인덱스가 붙는데 중요한 것은 이들이 가중치 합으로 계산되고 이는 행렬의 곱으로 한 번에 계산할 수 있다는 것입니다.

  > 실제 완전연결계층이 수행하는 변환은 다음과 같습니다.

  <img src="README.assets/e 1-3.png" alt="e 1-3" style="zoom:50%;" />

  - 은닉층 뉴런의 값이 좌변의 (1, 4) 행렬이며 입력은 (1, 2) 행렬, 가중치는 (2, 4) 행렬, 편향을 (1, 4) 행렬임을 알 수 있습니다.

  - 이를 더 간소화하면 다음과 같습니다.

  <img src="README.assets/e 1-4.png" alt="e 1-4" style="zoom:50%;" />

  - 여기서 **x**는 입력, **h**는 은닉층 뉴런, **W**는 가중치, **b**는 편향을 의미하고 모두 행렬입니다.

  - 이들의 형상은 모두 다음과 같이 변환됩니다.

  <img src="README.assets/fig 1-8.png" alt="fig 1-8" style="zoom:50%;" />

  - 이처럼 행렬의 곱은 대응하는 차원의 원소 수가 같아야 합니다.

#### Note

> 행렬의 곱 계산은 행렬의 형상 확인이 중요합니다. 형상을 보면 이 계산이 올바른지, 적어도 계산이 성립하는지를 알 수 있습니다.

- 이것으로 완전연결계층의 변환을 행렬로 정리하고 계산해봤습니다. 그러나 이는 하나의 입력 데이터만을 대상으로 한 변환입니다.

- 하지만 신경망의 추론과 학습은 다수의 샘플 데이터, 즉 미니배치를 한 번에 처리합니다.

- 이렇게 하기 위해서는 행렬 **x**의 행 각각에 샘플 데이터를 하나씩 저장해야합니다. 즉, N개의 미니배치에 대한 연산은 다음과 같습니다.

<img src="README.assets/fig 1-9.png" alt="fig 1-9" style="zoom:50%;" />

- 이처럼 형상 확인을 통해 각 미니배치가 정상적으로 변환되었는지 알 수 있습니다.

- 이 때 미니배치가 한 번에 완전연결계층에 의해 변환되고 은닉층에는 이만큼의 뉴런이 함께 계산됩니다.

> 이를 python으로 구현하면 다음과 같습니다.

```python
import numpy as np

W1 = np.random.randn(2, 4)  # weight
b1 = np.random.randn(4)  # bias
x = np.random.randn(10, 2)  # input
h = np.matmul(x, W1) + b1  # output
```

- 이처럼 10개의 미니배치를 완전연결계층으로 변환할 때 x의 헛 번째 차원이 각 샘플 데이터에 해당합니다.

- 마찬가지로 h의 각 인덱스에 인덱스에 해당하는 데이터의 은닉층 뉴런이 저장됩니다.

#### Warning

> 위 코드 마지막의 편향 b1은 브로드캐스드되어 더해집니다.

- 그런데 완전연결계층에 의한 변환은 '선형' 변환입니다. 여기에 '비선형' 효과를 부여하는 것이 바로 **활성화 함수**입니다.

- 더 자세히 말하자면 비선형 활성화 함수를 사용해 신경망의 표현력을 높일 수 있습니다.

  - 활성화 함수의 종류는 다양하나 대표적으로 **시그모이드 함수**를 사용하기도 합니다.

  <img src="README.assets/e 1-5.png" alt="e 1-5" style="zoom:50%;" />

  - 이 함수의 그래프는 다음과 같은 S자형을 그립니다.

  <img src="README.assets/fig 1-10.png" alt="fig 1-10" style="zoom:50%;" />

  - 시그모이드 함수는 임의의 실수를 입력받아 0부터 1사이의 실수를 출력합니다. 이를 구현하면 다음과 같습니다.

  ```python
  def sigmoid(x):
      return 1 / (1 + np.exp(-x))
  ```

- 이제 위 함수를 사용해 은닉층의 뉴런을 변환하여 비선형 변환이 가능해집니다.

> `a = sigmoid(h)`

- 이어서 이 활성화 함수의 출력인 **활성화**를 다른 완전연결계층에 통과시켜 변환합니다.

  - 위의 예시에 따르면 은닉층의 뉴런이 4, 출력층의 뉴런이 3이므로 완전연결계층에 사용되는 가중치의 행렬은 (4, 3) 형상입니다.

  > 이를 종합해 구현하면 다음과 같습니다.

  ```python
  import numpy as np

  def sigmoid(x):
  return 1 / (1 + np.exp(-x))

  x = np.random.randn(10, 2) # input
  W1 = np.random.randn(2, 4) # weight for 1st layer
  b1 = np.random.randn(4) # bias for 1st layer
  W2 = np.random.randn(4, 3) # 2nd layer
  b1 = np.random.randn(3) # 2nd layer

  h = np.matmul(x, W1) + b1
  a = sigmoid(h) # activation
  s = np.matmul(a, W2) + b2

  ```

- 위 코드를 해석하자면 x의 형상이 (10, 2), 즉 2차원 데이터가 10개 씩 미니배치로 처리된다는 의미입니다.

- 또한 최종 형상은 (10, 3)으로 3차원 데이터가 10개 씩 미니배치로 처리된 결과가 나온다는 의미입니다.

- 이 신경망은 3차원 데이터를 출력하는데 각 차원의 값을 이용해 세 클래스로 분류할 수 있습니다.

- 이 경우 출력된 3차원 벡터의 각 차원은 각 클래스에 대응하는 점수(첫 번째 뉴런이 첫 번째 클래스이듯)로 실제 분류한다면 출력층에서 가장 큰 값을 내는 뉴런에 해당하는 클래스가 예측 결과가 될 것입니다.

#### Note

> 점수란 '확률'이 되기 전의 값으로 점수가 높을수록 그 뉴런에 해당하는 클래스의 확률도 올라갑니다.
>
> 덧붙여 점수를 소프트맥스 함수에 입력하면 확률로 얻을 수 있습니다.

- 이상이 신경망의 추론이며 다음 절은 지금까지 수행한 처리를 하나의 '계층'으로 추상화한 python class를 구현할 것입니다.

### 1.2.2 계층으로 클래스화 및 순전파 구현

- 그럼 신경망에서 하는 처리를 계층으로 구현해봅시다.

  - 여기서는 완전연결계층에 의한 변환을 Affine 계층으로, 시그모이드 함수에 의한 변환을 Sigmoid 계층으로 구현할 것입니다.

    - 참고로 완전연결계층에 의한 변환은 기하학에서 Affine 변환에 해당하여 Affine 계층으로 부릅니다.

- 또한, 각 계층은 python class로 구현할 것이며 기본적인 변환을 수행하는 메서드의 이름은 forward()입니다.

#### Note

> 신경망 추론 과정에서 처리를 신경망의 **순전파**라고 합니다.
>
> 순전파란 말 그대로 입력층에서 출력층으로 향하는 전파로 이 때 신경망을 구성하는 각 계층이 입력에서 출력 방향으로 처리 결과를 차례로 전파합니다.
>
> 나중에 살펴볼 신경망 학습에서는 데이터(기울기)를 순전파와 반대 방향으로 보내는 데 이를 **역전파**라고 합니다.

- 신경망에서는 다양한 계층이 등장하는 데 이들을 모두 python class로 구현할 것입니다.

- 이토록 모듈화한 계층을 블록 조합하듯 신경망 구축에 사용할 수 있고 이를 위해 다음 '구현 규칙'을 따르겠습니다.

  - 모든 계층은 forward()와 backward() 메서드를 가지고 인스턴스 변수인 params와 grads를 가진다.

- 이 규칙을 간단히 설명하면 다음과 같습니다.

  - forward()와 backward()는 각각 순전파와 역전파를 의미하며 params는 가중치와 편향같은 매개변수를, grads는 params에 대응하는 기울기를 저장한 list입니다.

#### Note

> 이 규칙에 따라 구현하면 일관되고 확장성이 좋아집니다. 왜 이를 따르는 게 좋은 지는 나중에 밝혀집니다.

- 이번 절에서는 순전파만 구현할 것이므로 각 계층은 forward() 메서드와 params 인스턴스 변수를 가질 것입니다.

- 이를 통해 가장 먼저 구현할 계층은 Sigmoid 입니다.

> 1.2.2_forward_net.py의 class Sigmoid를 참고하세요.

- 시그모이드 함수는 주 변환 처리를 forward()에서 처리하며 해당 계층은 학습할 매개변수가 없으므로 빈 리스트로 초기화 합니다.

- 다음은 Affine 입니다.

> 1.2.2_forward_net.py의 class Affine을 참고하세요.

- Affine 계층은 초기화할 때 가중치와 편향을 받습니다. 즉, 이들은 매개변수로 신경망이 학습할 때 수시로 갱신됩니다. 그리고 forward로 순전파를 처리합니다.

#### Note

> 이 책의 예제 코드는 '구현 규칙'을 따르므로 모든 계층에 학습 매개변수가 params list에 들어갑니다.
>
> 덕분에 모든 매개변수를 간단히 정리할 수 있고 자연스럽게 매개변수 갱신이나 파일을 저장하는 것이 쉬워집니다.

- 그럼 이 계층들을 사용해 신경망의 추론을 처리해보겠습니다. 예시는 다음과 같습니다.

<img src="README.assets/fig 1-11.png" alt="fig 1-11" style="zoom:50%;" />

- 이처럼 입력 **x**가 Affine, Sigmoid, Affine 계층을 차례로 거쳐 점수 **s**를 출력합니다. 이를 TwoLayerNet이라는 클래스로 추상화하고 추론은 predict()로 합니다.

#### Note

> 앞으로 신경망 그림은 '뉴런 관점'의 그림이 아닌 '계층 관점'으로 생각해주세요.

- 그럼 TwoLayerNet의 구현은 다음과 같습니다.

> 1.2.2_forward_net.py의 class TwoLayerNet을 참고하세요.

- 이처럼 초기화를 통해 가중치 초기화와 계층 생성을 진행하고 학습할 매개변수들을 params에 저장합니다.

- 그리고 그 결과를 바탕으로 신경망의 추론을 진행하여 입력 데이터 x에 대한 점수 s를 구할 수 있습니다.

- 이처럼 학습할 모든 매개변수가 model, params에 모여있으므로 이어질 신경망 학습이 한결 쉬워지게 됩니다.

## 1.3 신경망의 학습

- 학습되지 않은 신경망은 '좋은 추론'을 할 수 없습니다. 그래서 학습을 먼저 수행하고 학습된 매개변수를 사용해 추론을 수행하는 흐름이 일반적입니다.

  - 추론이란 앞에서 본 것처럼 다중 클래스 분류 등의 문제에 답을 구하는 작업입니다.

- 한편, 신경망의 학습은 최적 매개변수를 찾는 작업으로 이번 절에서 이를 학습하는 방법을 살펴보겠습니다.

### 1.3.1 손실 함수

- 신경망 학습에는 학습이 얼마나 잘 되고 있는지를 알기 위한 '척도'가 필요합니다.

- 일반적으로 학습 단계의 특정 시점에서 신경망의 성능을 나타내는 척도로 **손실**을 사용합니다.

  - 손실이란 학습 데이터와 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜가를 산출하는 단일 값입니다.

- 신경망의 손실은 **손실 함수**를 사용해 구합니다.

  - 다중 클래스 분류 신경망에서는 주로 **교차 엔트로피 오차**를 사용합니다.

  - 교차 엔트로피 오차란 신경망이 출력하는 각 클래스의 확률과 정답 레이블을 이용해 구할 수 있습니다.

- 그럼 지금껏 다뤄온 신경망에서 손실을 구해보겠습니다. 그에 앞서 신경망에 Softmax 계층과 Cross Entropy Error 계층을 새로 추가하면 '계층 관점'에서 다음과 같이 그려집니다.

<img src="README.assets/fig 1-12.png" alt="fig 1-12" style="zoom:50%;" />

- 입력 데이터 **x**, 정답 레이블 **t**, 손실 **L**을 기점으로 Softmax 계층의 출력은 확률로 다음 계층인 Cross Entropy Error 계층에 이 확률과 정답 레이블이 입력됩니다.

- 이어서 소프트맥스 함수와 교차 엔트로피 오차에 관해 알아보겠습니다. 우선 소프트맥스 함수의 식은 다음과 같습니다.

  <img src="README.assets/e 1-6.png" alt="e 1-6" style="zoom:50%;" />

  - 총 n개의 출력 중 k번째 출력 y<sub>k</sub>를 구하는 위 식은 k번째 클래스의 소프트맥스 함수의 출력입니다.

  - 이 식에서 보듯 소프트맥스 함수의 분자는 점수 s<sub>k</sub>의 지수 함수이고 분모는 모든 입력 신호의 지수 함수의 총합입니다.

  - 이를 통해서 소프트맥스 함수의 각 원소의 출력은 0과 1사이의 실수로 이 값을 모두 더하면 1이 나옵니다. 즉, 이를 통해 소프트맥스 함수의 출력을 '확률'로 해석할 수 있는 것입니다.

- 이제 이 입력값을 교차 엔트로피 오차에 입력하는데 그 수식은 다음과 같습니다.

  <img src="README.assets/e 1-7.png" alt="e 1-7" style="zoom:50%;" />

  - 여기서 t<sub>k</sub>는 k번째 클래스에 해당하는 정답 레이블로 log는 자연상수 e를 밑으로 하며 정답 레이블은 `t = [0, 0, 1]`과 같이 one-hot 벡터입니다.

#### Note

> one-hot 벡터란 단 하나의 원소만 1이고 나머지는 0인 벡터로 여기서 1인 원소가 정답 클래스에 해당합니다.
>
> 따라서 위의 식은 정답 레이블이 1의 원소에 해당하는 출력의 자연로그를 계산할 뿐입니다.

- 나아가 미니배치를 고려하면 교차 엔트로피 오차 식은 다음과 같이 확장되며 이 식에서 N개의 데이터를 입력해 n번째 데이터의 k번째 차원 값을 계산합니다.

  <img src="README.assets/e 1-8.png" alt="e 1-8" style="zoom:50%;" />

  - 위의 식은 조금 복잡해 보이지만 하나의 데이터에 대한 손실 함수를 나타낸 기존의 식을 N개의 데이터로 확장한 결과입니다.

  - 다만, N으로 나누어 1개당 '평균 손실 함수'를 구할 뿐입니다. 이렇게 평균을 구하면 미니배치의 크기와 상관없이 일관된 오차를 얻을 수 있습니다.

- 이 책에서 소프트맥스 함수와 교차 엔트로피 오차를 계산하는 계층을 모아 Softmax with Loss 계층으로 합쳐 계산합니다. 따라서 최종 신경망은 다음과 같습니다.

<img src="README.assets/fig 1-13.png" alt="fig 1-13" style="zoom:50%;" />

- 이처럼 이 책에서는 Softmax with Loss 계층을 사용하지만 그 구현은 따로 설명하지 않습니다.

  - 다만 궁금하신 분들은 chapter01/commons/layers.py의 해당 부분을 참고하시던가 이전 책의 4.2 손실 함수에서 확인하시면 됩니다.
