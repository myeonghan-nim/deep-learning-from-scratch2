# chapter04. Speed Improvement of word2vec

- 앞의 3장에서 word2vec의 구조를 배우고 CBOW 모델을 구현했습니다.

  - CBOW 모델은 단순한 2층 신경망이라서 간단하게 구현할 수 있지만 말뭉치에 포함된 어휘 수가 많아지면 계산량이 커진다는 단점이 있습니다.

- 그래서 이번 장에서는 word2vec의 속도를 개선할 것이며 구체적으로는 두 가지 개선을 추가할 것입니다.

  - 첫 번째로 embedding이라는 새로운 계층을 도입합니다.

  - 두 번째로 네거티브 샘플링이라는 새로운 손실 함수를 도입합니다.

  - 이 두 가지 개선으로 '진짜' word2vec을 구현할 수 있으며 이를 PTB 데이터셋을 가지고 학습을 수행하고 결과로 얻은 단어의 분산 표현의 장점을 평가할 것입니다.

## 4.1 word2vec 개선 ①

- 우선 앞 장에서 구현한 CBOW 모델은 다음과 같습니다.

<img src="README.assets/fig 4-1.png" alt="fig 4-1" style="zoom:50%;" />

- 앞 장의 CBOW 모델은 단어 2개를 맥락으로 사용해 이를 바탕으로 하나의 단어(타깃)을 추측합니다.

  - 이 때 입력 측 가중치(W<sub>in</sub>)와 행렬 곱으로 은닉층이 계산되고 다시 출력 측 가중치(W<sub>out</sub>)와 행렬 곱으로 각 단어의 점수를 얻습니다.

  - 그리고 이 점수에 소프트맥스 함수를 적용해 단어의 출현 확률을 얻고 이 확률을 정답 레이블과 비교하여 손실을 구합니다.(정확히는 교차 엔트로피 오차가 활용됩니다.)

#### Warning

> 앞 장에서는 맥락의 윈도우 크기를 1로 한정하여 다깃 앞뒤 한 단어씩만 사용했습니다.
>
> 이번 장에서는 나중에 어떤 크기의 맥락도 다룰 수 있도록 기능을 추가할 것입니다.

- 이전에 구현한 CBOW는 작은 말뭉치를 다룰 때는 문제될 게 없지만 거대한 말뭉치를 다루게 되면 몇 가지 문제가 발생합니다.

  - 그 예시를 위해 어휘가 100만 개, 은닉층 뉴런이 100개인 CBOW 모델을 구현하면 다음과 같습니다.

  <img src="README.assets/fig 4-2.png" alt="fig 4-2" style="zoom:50%;" />

  - 이처럼 입력층과 출력층에 100만 개의 뉴런이 존재하고 이 때문에 중간 계산에 많은 시간이 소요되는데 정확히는 다음 두 계산 때문입니다.

    - 입력층의 원핫 표현과 가중치 행렬 W<sub>in</sub>의 계산

    - 은닉층과 가중치 행렬 W<sub>out</sub>의 곱 및 Softmax 계층의 계산

- 첫 번째로 입력층의 원핫 표현과 관련된 문제는 단어를 원핫 표현으로 다루기 때문에 어휘 수가 많아지면 원핫 표현의 벡터 크기가 커진다는 점입니다. 게다가 이 원핫 벡터와 가중치 행렬 W<sub>in</sub>을 곱하는 것은 상당한 계산 자원을 소모합니다.

  - 이 문제는 embedding 계층을 도입하여 해결할 것입니다.

- 두 번째로 은닉층 이후 은닉층과 가중치 행려려 W<sub>out</sub>의 곱도 계산량이 상당한데 Softmax 계층도 다루는 어휘가 많아져 계산량이 증가합니다.

  - 이 문제는 네거티브 샘플링이라는 새로운 손실 함수를 도입해 해결할 것입니다.

#### Note

> 개선 전의 word2vec 파일은 chapter03의 simple_cbow.py로 구현되어 있습니다.
>
> 반면에 개선 후의 word2vec 파일은 chapter04의 cbow.py로 구현되어 있습니다.

### 4.1.1 Embedding 계층

- 앞 장의 word2vec 구현은 단어를 원핫 표현으로 바꾸었습니다. 그리고 MatMul 계층에 입력해 가중치 행렬을 곱했습니다. 만일 어휘 수가 100만 개에 은닉층 뉴런 수가 100개라면 MatMul의 계산은 다음과 같습니다.

<img src="README.assets/fig 4-3.png" alt="fig 4-3" style="zoom:50%;" />

- 어휘가 100만 개이므로 원핫 표현도 100만 차원이 되는데 여기에 가중치 행렬을 곱해야하는 것입니다. 하지만 결과적으로 이는 단지 행렬의 특정 행을 추출하는 일입니다. 따라서 원핫 표현으로의 변환과 MatMul 계층의 행렬 곱 계산은 사실 필요가 없습니다.

- 그러면 다중치 매개변수에서 '단어 ID에 해당하는 행(벡터)'를 추출하는 계층을 만들텐데 이를 Embedding 계층이라고 부르겠습니다. 이 계층은 단어의 분산 표현을 저장합니다.

#### Note

> 자연어 처리 분야에서 단어의 밀집벡터를 **단어 임베딩** 혹은 단어의 **분산 표현**이라고 합니다.
>
> 참고로 통계 기반 기법으로 얻은 단어 벡터는 distributional representation이며 추론 기반 기법은 distributed representation이라 합니다.

### 4.1.2 Embedding 계층 구현

- 행렬에서 특정 행을 추출하는 방법은 매우 쉽습니다. 예를 들어 가중치 W가 2차원 numpy 배열일 때 특정 행은 원하는 행을 명시하면 됩니다.

```python
import numpy as np

W = np.arange(21).reshape(7, 3)
print(W)  # 가중치 확인
print(W[2])  # 가중치 행렬의 2번째(0이 시작점) 행 추출
```

- 또한 가중치 W에서 여러 행을 한꺼번에 추출하는 것은 원하는 행 번호들을 배열에 멍시하면 됩니다.

```python
idx = np.array([1, 0, 3, 0])
print(W[idx])
```

- 이처럼 인수에 배열을 사용하면 여러 행을 한 번에 추출할 수 있으며 이는 미니배치 처리를 가정한 경우입니다.

- 그럼 Embedding 계층의 forward 메서드를 구현하겠습니다.

> chapter04/commons의 layer.py의 Embedding 클래스를 확인하세요.

- 이 책의 규칙에 따라 인스턴스 변수 params와 grads를 사용하며 인스턴스 변수 idx에는 추출하는 행의 인덱스(단어 ID)를 배열로 저장합니다.

- 이어서 역전파를 생각해볼텐데 순전파에서 가중치 W의 특정 행을 추출한 것은 단순히 가중치의 특정 행 뉴런만을 다음 층으로 흘려보낸 것입니다.

  - 따라서 역전파에서는 앞 층에서 전해진 기울기를 다음 층(입력 측 층)으로 그대로 흘려주면 됩니다.

  - 다만, 앞 층에서 전해진 기울기를 가중치 기울기 dW의 특정 행(idx번째 행)에 설정합니다. 이는 다음과 같습니다.

<img src="README.assets/fig 4-4.png" alt="fig 4-4" style="zoom:50%;" />

- 이를 구현하면 다음과 같습니다.

```python
    def backward(self, dout):
        dW, = self.grads
        dw[...] = 0
        dW[self.idx] = dout  # 실은 안 좋은 예시입니다.
        return None
```

- 이는 가중치 기울기 dW를 떠내어 dW의 원소를 0으로 덮어씌우지만 dW의 형상을 유지합니다. 그리고 앞 층에서 전해진 기울기 dout을 idx번째 행에 할당합니다.

#### Warning

> 여기에서 가중치 W와 크기가 같은 행렬 dW를 만들어 dW의 특정 행에 기울기를 할당했습니다.
>
> 그러나 최종적으로 가중치 W를 갱신하기 위해서 일부러 dW와 같은 W와 같은 형상의 행렬을 만들 필요는 없습니다.
>
> 갱신하려는 행 번호와 그 기울기를 따로 저장하면 이 정보로 가중치 W의 특정 행만 갱신할 수 있습니다.
>
> 다만 여기에서는 이미 구현한 갱신 클래스인 Optimizer와의 조합을 고려해 이처럼 구현했습니다.

- 다만, 이 구현에는 문제가 하나 있는데 idx의 원소가 중복되면 다음과 같은 문제가 발생합니다.

<img src="README.assets/fig 4-5.png" alt="fig 4-5" style="zoom:50%;" />

- 이처럼 동일한 idx가 주어지면 먼저 쓰여진 값을 나중에 쓰여진 값이 덮어씌웁니다. 이를 해결하려면 '할당'이 아닌 '더하기'를 해야합니다. 이를 올바르게 구현하면 다음과 같습니다.

```python
    def backward(self, dout):
        dW, = self.grads
        dW[...] = 0

        for i, word_idx in enumerate(self.idx):
            dW[word_idx] += dout[i]
        # 또는 위의 for 문을 np.add.at(dW, self.idx, dout)으로 바꿔주세요.

        return None
```

- 이처럼 for 문을 사용해 해당 인덱스에 기울기를 더하거나 numpy의 np.add.at(A, idx, B) 메서드를 사용해 B를 A의 idx번째 행에 더해도 됩니다.

#### Note

> 위의 구현에서 python의 for 문보다 numpy의 내장 메서드를 사용하는 편이 더 빠릅니다.
>
> 이는 해당 메서드에 속도와 효율을 높여주는 최적화가 적용되어 있기 때문입니다.

- 이상으로 Embedding 계층을 구현했으며 이를 사용해 word2vec의 입력 측 MatMul 계층을 Embedding 계층으로 전환할 것입니다. 그를 통해 쓸데없는 계산을 생략해 메모리 사용량을 줄일 수 있습니다.
