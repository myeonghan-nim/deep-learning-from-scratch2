# chapter04. Speed Improvement of word2vec

- 앞의 3장에서 word2vec의 구조를 배우고 CBOW 모델을 구현했습니다.

  - CBOW 모델은 단순한 2층 신경망이라서 간단하게 구현할 수 있지만 말뭉치에 포함된 어휘 수가 많아지면 계산량이 커진다는 단점이 있습니다.

- 그래서 이번 장에서는 word2vec의 속도를 개선할 것이며 구체적으로는 두 가지 개선을 추가할 것입니다.

  - 첫 번째로 embedding이라는 새로운 계층을 도입합니다.

  - 두 번째로 네거티브 샘플링이라는 새로운 손실 함수를 도입합니다.

  - 이 두 가지 개선으로 '진짜' word2vec을 구현할 수 있으며 이를 PTB 데이터셋을 가지고 학습을 수행하고 결과로 얻은 단어의 분산 표현의 장점을 평가할 것입니다.

## 4.1 word2vec 개선 ①

- 우선 앞 장에서 구현한 CBOW 모델은 다음과 같습니다.

<img src="README.assets/fig 4-1.png" alt="fig 4-1" style="zoom:50%;" />

- 앞 장의 CBOW 모델은 단어 2개를 맥락으로 사용해 이를 바탕으로 하나의 단어(타깃)을 추측합니다.

  - 이 때 입력 측 가중치(W<sub>in</sub>)와 행렬 곱으로 은닉층이 계산되고 다시 출력 측 가중치(W<sub>out</sub>)와 행렬 곱으로 각 단어의 점수를 얻습니다.

  - 그리고 이 점수에 소프트맥스 함수를 적용해 단어의 출현 확률을 얻고 이 확률을 정답 레이블과 비교하여 손실을 구합니다.(정확히는 교차 엔트로피 오차가 활용됩니다.)

#### Warning

> 앞 장에서는 맥락의 윈도우 크기를 1로 한정하여 다깃 앞뒤 한 단어씩만 사용했습니다.
>
> 이번 장에서는 나중에 어떤 크기의 맥락도 다룰 수 있도록 기능을 추가할 것입니다.

- 이전에 구현한 CBOW는 작은 말뭉치를 다룰 때는 문제될 게 없지만 거대한 말뭉치를 다루게 되면 몇 가지 문제가 발생합니다.

  - 그 예시를 위해 어휘가 100만 개, 은닉층 뉴런이 100개인 CBOW 모델을 구현하면 다음과 같습니다.

  <img src="README.assets/fig 4-2.png" alt="fig 4-2" style="zoom:50%;" />

  - 이처럼 입력층과 출력층에 100만 개의 뉴런이 존재하고 이 때문에 중간 계산에 많은 시간이 소요되는데 정확히는 다음 두 계산 때문입니다.

    - 입력층의 원핫 표현과 가중치 행렬 W<sub>in</sub>의 계산

    - 은닉층과 가중치 행렬 W<sub>out</sub>의 곱 및 Softmax 계층의 계산

- 첫 번째로 입력층의 원핫 표현과 관련된 문제는 단어를 원핫 표현으로 다루기 때문에 어휘 수가 많아지면 원핫 표현의 벡터 크기가 커진다는 점입니다. 게다가 이 원핫 벡터와 가중치 행렬 W<sub>in</sub>을 곱하는 것은 상당한 계산 자원을 소모합니다.

  - 이 문제는 embedding 계층을 도입하여 해결할 것입니다.

- 두 번째로 은닉층 이후 은닉층과 가중치 행려려 W<sub>out</sub>의 곱도 계산량이 상당한데 Softmax 계층도 다루는 어휘가 많아져 계산량이 증가합니다.

  - 이 문제는 네거티브 샘플링이라는 새로운 손실 함수를 도입해 해결할 것입니다.

#### Note

> 개선 전의 word2vec 파일은 chapter03의 simple_cbow.py로 구현되어 있습니다.
>
> 반면에 개선 후의 word2vec 파일은 chapter04의 cbow.py로 구현되어 있습니다.

### 4.1.1 Embedding 계층

- 앞 장의 word2vec 구현은 단어를 원핫 표현으로 바꾸었습니다. 그리고 MatMul 계층에 입력해 가중치 행렬을 곱했습니다. 만일 어휘 수가 100만 개에 은닉층 뉴런 수가 100개라면 MatMul의 계산은 다음과 같습니다.

<img src="README.assets/fig 4-3.png" alt="fig 4-3" style="zoom:50%;" />

- 어휘가 100만 개이므로 원핫 표현도 100만 차원이 되는데 여기에 가중치 행렬을 곱해야하는 것입니다. 하지만 결과적으로 이는 단지 행렬의 특정 행을 추출하는 일입니다. 따라서 원핫 표현으로의 변환과 MatMul 계층의 행렬 곱 계산은 사실 필요가 없습니다.

- 그러면 다중치 매개변수에서 '단어 ID에 해당하는 행(벡터)'를 추출하는 계층을 만들텐데 이를 Embedding 계층이라고 부르겠습니다. 이 계층은 단어의 분산 표현을 저장합니다.

#### Note

> 자연어 처리 분야에서 단어의 밀집벡터를 **단어 임베딩** 혹은 단어의 **분산 표현**이라고 합니다.
>
> 참고로 통계 기반 기법으로 얻은 단어 벡터는 distributional representation이며 추론 기반 기법은 distributed representation이라 합니다.

### 4.1.2 Embedding 계층 구현

- 행렬에서 특정 행을 추출하는 방법은 매우 쉽습니다. 예를 들어 가중치 W가 2차원 numpy 배열일 때 특정 행은 원하는 행을 명시하면 됩니다.

```python
import numpy as np

W = np.arange(21).reshape(7, 3)
print(W)  # 가중치 확인
print(W[2])  # 가중치 행렬의 2번째(0이 시작점) 행 추출
```

- 또한 가중치 W에서 여러 행을 한꺼번에 추출하는 것은 원하는 행 번호들을 배열에 멍시하면 됩니다.

```python
idx = np.array([1, 0, 3, 0])
print(W[idx])
```

- 이처럼 인수에 배열을 사용하면 여러 행을 한 번에 추출할 수 있으며 이는 미니배치 처리를 가정한 경우입니다.

- 그럼 Embedding 계층의 forward 메서드를 구현하겠습니다.

> chapter04/commons의 layer.py의 Embedding 클래스를 확인하세요.

- 이 책의 규칙에 따라 인스턴스 변수 params와 grads를 사용하며 인스턴스 변수 idx에는 추출하는 행의 인덱스(단어 ID)를 배열로 저장합니다.

- 이어서 역전파를 생각해볼텐데 순전파에서 가중치 W의 특정 행을 추출한 것은 단순히 가중치의 특정 행 뉴런만을 다음 층으로 흘려보낸 것입니다.

  - 따라서 역전파에서는 앞 층에서 전해진 기울기를 다음 층(입력 측 층)으로 그대로 흘려주면 됩니다.

  - 다만, 앞 층에서 전해진 기울기를 가중치 기울기 dW의 특정 행(idx번째 행)에 설정합니다. 이는 다음과 같습니다.

<img src="README.assets/fig 4-4.png" alt="fig 4-4" style="zoom:50%;" />

- 이를 구현하면 다음과 같습니다.

```python
    def backward(self, dout):
        dW, = self.grads
        dw[...] = 0
        dW[self.idx] = dout  # 실은 안 좋은 예시입니다.
        return None
```

- 이는 가중치 기울기 dW를 떠내어 dW의 원소를 0으로 덮어씌우지만 dW의 형상을 유지합니다. 그리고 앞 층에서 전해진 기울기 dout을 idx번째 행에 할당합니다.

#### Warning

> 여기에서 가중치 W와 크기가 같은 행렬 dW를 만들어 dW의 특정 행에 기울기를 할당했습니다.
>
> 그러나 최종적으로 가중치 W를 갱신하기 위해서 일부러 dW와 같은 W와 같은 형상의 행렬을 만들 필요는 없습니다.
>
> 갱신하려는 행 번호와 그 기울기를 따로 저장하면 이 정보로 가중치 W의 특정 행만 갱신할 수 있습니다.
>
> 다만 여기에서는 이미 구현한 갱신 클래스인 Optimizer와의 조합을 고려해 이처럼 구현했습니다.

- 다만, 이 구현에는 문제가 하나 있는데 idx의 원소가 중복되면 다음과 같은 문제가 발생합니다.

<img src="README.assets/fig 4-5.png" alt="fig 4-5" style="zoom:50%;" />

- 이처럼 동일한 idx가 주어지면 먼저 쓰여진 값을 나중에 쓰여진 값이 덮어씌웁니다. 이를 해결하려면 '할당'이 아닌 '더하기'를 해야합니다. 이를 올바르게 구현하면 다음과 같습니다.

```python
    def backward(self, dout):
        dW, = self.grads
        dW[...] = 0

        for i, word_idx in enumerate(self.idx):
            dW[word_idx] += dout[i]
        # 또는 위의 for 문을 np.add.at(dW, self.idx, dout)으로 바꿔주세요.

        return None
```

- 이처럼 for 문을 사용해 해당 인덱스에 기울기를 더하거나 numpy의 np.add.at(A, idx, B) 메서드를 사용해 B를 A의 idx번째 행에 더해도 됩니다.

#### Note

> 위의 구현에서 python의 for 문보다 numpy의 내장 메서드를 사용하는 편이 더 빠릅니다.
>
> 이는 해당 메서드에 속도와 효율을 높여주는 최적화가 적용되어 있기 때문입니다.

- 이상으로 Embedding 계층을 구현했으며 이를 사용해 word2vec의 입력 측 MatMul 계층을 Embedding 계층으로 전환할 것입니다. 그를 통해 쓸데없는 계산을 생략해 메모리 사용량을 줄일 수 있습니다.

## 4.2 word2vec 개선 ②

- 이어서 word2vec의 두 번째 개선을 진행하겠습니다. 이제 남은 병목은 은닉층 이후의 행렬 곱과 Softmax 계층의 계산입니다. 바로 이를 해결하는 것이 이번 절의 목표입니다.

- 이를 해결하는 방법은 **네거티브 샘플링**으로 Softmax을 대신해 낮은 수준의 계산량을 유지할 수 있습니다.

  - 이번 이야기는 조금 복잡하지 한 단계씩 천천히 확인하며 진행하겠습니다.

### 4.2.1 은닉층 이후의 계산의 문제점

- 은닉층 이후 계산의 문제점을 알아보기 위해 앞 절과 같이 어휘가 100만 개, 은닉층 뉴런이 100개일 때의 word2vec 모델의 동작을 알아보겠습니다.

<img src="README.assets/fig 4-6.png" alt="fig 4-6" style="zoom:50%;" />

- 이처럼 입력층과 출력층에 100만 개에 달하는 뉴런이 존재하는데 앞에서 Embedding 계층을 통해 입력층의 계산 낭비를 줄였습니다. 남은 부분은 은닉층 이후로 다음 두 부분이 문제입니다.

  - 은닉층의 뉴런과 가중치 행렬(W<sub>out</sub>)의 곱

  - Softmax 계층의 계산

- 첫 번째는 거대한 행렬의 곱 문제로 은닉층 벡터의 크기가 100, 가중치 행렬의 크기가 100 X 100 만입니다. 이렇게 큰 행렬의 계산은 시간과 메모리가 크게 소모됩니다. 따라서 이를 '가볍게' 만들어야 합니다.

- 두 번째로 Softmax 계층도 어휘가 많아질수록 계산량이 증가합니다. 이 사실은 다음 수식을 통해 명확하게 알 수 있습니다.

<img src="README.assets/e 4-1.png" alt="e 4-1" style="zoom:50%;" />

- k번째 원소(단어)를 타깃으로 했을 때 계산식인데 위에서 처럼 100만 개의 어휘에 대한 분모의 값을 얻으려면 exp 연산을 100만 번 수행해야합니다. 따라서 이를 대신할 '가벼운' 연산이 필요합니다.

### 4.2.2 다중 분류에서 이진 분류로

- 따라서 네거티브 샘플링에 대해 설명하겠습니다. 이 기법의 핵심 아이디어는 '이진 분류'로 더 자세하게 '다중 분류'를 '이진 분류'로 근사하는 것이 네거티브 샘플링의 중요 포인트입니다.

  - 이는 지금까지 수많은 단어 중 옳은 단어 하나를 선택하는 '다중 분류' 문제를 '이진 분류' 문제로 다루는 방법, 즉 근사하는 방법을 찾아내는 것이 핵심입니다.

#### Note

> 이진 분류는 Yes/No로 답하는 문제를 다루는데 예를 들어 '이 숫자는 7입니까?', '타깃 단어는 say입니까?' 같은 문제가 이진 분류입니다.

- 지금까지 맥락이 주어졌을 때 정답이 되는 단어를 높은 확률로 추측하도록 만드는 일을 했습니다. 그리고 학습이 잘 이뤄지면 그 신경망은 올바른 추측을 할 수 있습니다.

- 이제 '다중 분류'를 '이진 분류'로 바꾸는 방법을 통해 이 문제를 해결할 것입니다. 예를 들어 '맥락이 you와 goodbye일 때 타깃 단어는 say입니까?'라는 질문에 답하는 신경망을 생각해야 하는 것입니다.

  - 이 방식을 사용하면 출력층에는 뉴런이 하나만 있으면 됩니다. 바로 그 뉴런이 say의 점수를 출력할 것입니다. 이 형태를 그림으로 그리면 다음과 같습니다.

  <img src="README.assets/fig 4-7.png" alt="fig 4-7" style="zoom:50%;" />

- 이처럼 출력층의 뉴런은 단 하나로 은닉층과 출력 측의 가중치 행렬의 내적이 'say'에 해당하는 열만 추출하고 그 벡터와 은닉층 뉴런과의 내적을 계산합니다. 더 자세하게는 다음과 같습니다.

<img src="README.assets/fig 4-8.png" alt="fig 4-8" style="zoom:50%;" />

- 이처럼 출력 측 가중치 W<sub>out</sub>에서는 각 단어 ID의 단어 벡터가 각각 열로 저장되어 있고 예시처럼 'say'에 해당하는 단어 벡터를 추출합니다. 그리고 그 벡터와 은닉층 뉴런과의 내적을 구하면 그 값이 최종 점수입니다.

#### Note

> 이전까지 출려쳑층에서는 모든 단어를 대상으로 계산을 수행했습니다.
>
> 하지만 여기서는 단어 하나에 주목하여 그 점수만을 계산하는 것이 차이입니다.
>
> 그리고 그 결과를 시그모이드 함수를 사용해 점수를 확률로 변환합니다.

### 4.2.3 시그모이드 함수와 교차 엔트로피 오차

- 이진 분류 문제를 신경망으로 풀려면 시그모이드 함수를 통해 점수를 확률로 변환하고 손실을 교차 엔트로피 함수로 구합니다. 이들은 이진 분류 신경망에서 가장 흔하게 사용되는 조합입니다.

#### Note

> 다중 분류의 경우, 출력층에서는 점수를 확률로 변환할 때 소프트맥스를 손실은 교차 엔트로피 오차를 사용해 구합니다.
>
> 이진 분류의 경우, 출력층에서 시그모이드 함수를 사용하며 손실은 다중 분류와 동일하게 계산합니다.

- 시그모이드 함수를 복습하면 다음과 같습니다.

<img src="README.assets/e 4-2.png" alt="e 4-2" style="zoom:50%;" />

- 이 함수의 계산 결과는 S자 곡선 형태로 입력값이 0과 1사이의 실수로 변환됩니다. 여기서 핵심은 출력값을 '확률'로 해석할 수 있다는 점입니다.

  - 이 함수의 계산 그래프와 그 결과는 다음과 같습니다.

  <img src="README.assets/fig 4-9.png" alt="fig 4-9" style="zoom:50%;" />

- 시그모이드 함수를 적용해 확률을 얻으면 이 확률로 손실을 구합니다. 이 때 사용되는 게 '교차 엔트로피 오차'로 다음과 같습니다.

<img src="README.assets/e 4-3.png" alt="e 4-3" style="zoom:50%;" />

- 여기서 y는 시그모이드 함수의 출력을, t는 정답 레이블(1일 때 Yes)을 의미합니다. 따라서 t가 1이면 -logy가 t가 0이면 -log(1-y)가 출력됩니다.

#### Warning

> 이진 분류와 다중 분류 모두 손실 함수로 '교차 엔트로피 오차'를 사용하나 다른 수식에 같은 의미를 가집니다.
>
> 정확히 말해 다중 분류에서 출력층에 뉴런을 2개만 사용한다면 이진 분류와 동일한 식을 가지게 됩니다.
>
> 따라서 Softmax with Loss 계층을 조금만 수정하면 Sigmoid with Loss 계층을 만들 수 있습니다.

- 이어서 Sigmoid 계층과 Corss Entropy Error 계층의 계산 그래프를 살펴보겠습니다.

<img src="README.assets/fig 4-10.png" alt="fig 4-10" style="zoom:50%;" />

- 여기서 주목할 점은 역전파 값인 y - t로 역전파로 정답 레이블과 출력의 차이가 흘러갑니다.

  - 이 오차가 앞 계층에 흘러가므로, 오차가 크면 '크게' 학습하고, 오차가 작으면 '작게' 학습하게 됩니다.

#### Note

> '시그모이드 함수'와 '교차 엔트로피 오차'를 조합하여 역전파 값이 y - t라는 깔끔한 결과를 도출합니다.
>
> 마찬가지로 '소프트맥스 함수'와 '교차 엔트로피 오차', '항등 함수'와 '거듭 제곱 오차'의 조합도 역전차 시 y - t가 됩니다.

### 4.2.4 다중 분류에서 이진 분류로 (구현)

- 이제 지금까지 이야기한 것을 구현 관점에서 정리하겠습니다.

- 지금껏 다룬 다중 분류 문제는 출력층에 어휘 수만큼 뉴런을 준비하고 이 뉴런들이 출력한 값을 Softmax 계층에 통과시켰습니다. 이 때 사용되는 신경망을 '계층'과 '연산'을 중심으로 그리면 다음과 같습니다.

<img src="README.assets/fig 4-11.png" alt="fig 4-11" style="zoom:50%;" />

- 이 예시는 맥락이 you와 goodbye이고 타깃이 say인 경우로 입력층에 각각 대응하는 단어 ID의 분산 표현을 추출하기 위한 Embedding 계층을 사용했습니다.

#### Warning

> 앞 절에서 Embedding 계층을 구현했으며 이 계층은 단어 ID의 분산 표현을 추출합니다.
>
> 이전에는 MatMul 계층을 사용했었습니다.

- 이제 이 신경망을 이진 분류 신경망으로 변환하겠습니다. 그 구성을 다음과 같습니다.

<img src="README.assets/fig 4-12.png" alt="fig 4-12" style="zoom:50%;" />

- 여기에서는 은닉층 뉴런 h와 출력 측의 가중치 W<sub>out</sub>에서 단어 say에 해당하는 단어 벡터와 내적을 계산합니다. 그리고 그 출력을 Sigmoid with Loss 계층에 입력해 최종 손실을 얻습니다.

#### Warning

> 위의 예제에서는 Sigmoid with Loss 계층에 정답 레이블을 1로 입력하고 있는데 이는 Yes를 의미합니다.
>
> 반면에 답이 No라면 정답 레이블로 0을 입력해야 합니다.

- 이제 이 신경망의 후반부를 더 단순하게 만들어 더 이야기하기 쉽게 바꾸겠습니다.

  - 이를 위해 Embedding Dot 계층을 도입합니다. 이 계층은 Embedding 계층과 dot 연산의 처리를 합친 계층입니다.

<img src="README.assets/fig 4-13.png" alt="fig 4-13" style="zoom:50%;" />

- 은닉층 뉴런 h는 Embedding Dot 계층을 거쳐 Sigmoid with Loss 계층을 통과합니다. 보다시피 Embedding Dot 계층을 사용하면서 은닉층 이후의 처리가 간단해졌습니다. 이제 이 계층의 구현을 살펴보겠습니다.

> 4.2.4_negative_sampling_layer.py를 확인하세요.

- EmbeddingDot class에는 총 4개의 인스턴스 변수가 존재하는데 params에 매개변수를, grads에 기울기를, embed에 Embedding 계층을, cache에 순전파의 계산 결과를 잠시 유지합니다.

  - 순전파를 담당하는 forward 메서드는 인수로 은닉층 뉴런(h)과 numpy 배열(idx)을 받는데 여기에서 idx는 단어 ID의 배열로 이는 '미니배치 처리'를 가정했기 때문에 배열을 입력받습니다.

  - forward 메서드는 Embedding 계층의 forward를 호출한 다음 내적을 계산합니다. 이 구현을 제대로 이해하기 위해서는 다음 표를 참고하면 됩니다.

  <img src="README.assets/fig 4-14.png" alt="fig 4-14" style="zoom:50%;" />

  - 위의 예시처럼 적당한 W와 h 그리고 idx를 준비합니다. 이 때 idx는 3개의 데이터를 미니배치로 한 번에 처리하는 예임을 뜻합니다. 따라서 idx의 행을 추출하고 이를 h와 각 원소별 곱을 수행한 뒤 이 결과를 행 단위로(axis=1) 전부 더해 결과를 출력합니다.

- 이상이 Embedding Dot 계층의 순전파로 역전파는 순전파의 반대 순서로 기울기를 전달하도록 구현합니다.

### 4.2.5 네거티브 샘플링

- 지금까지 배운 것으로 주어진 문제를 '다중 분류'에서 '이진 분류'로 변환할 수 있지만 이것만으로 문제가 해결되지 않습니다. 이는 지금까지 긍정적인 예(정답)에 대해서만 학습했기 때문으로 부정적인 예(오답)에 대한 학습이 필요합니다.

  - 여기서 앞의 예제를 다시 생각하면 긍정적인 예인 say만을 대상으로 이진 분류를 했을 때 '좋은 가중치'가 준비되어 있다면 Sigmoid 계층의 출력은 1에 가까울 것입니다.

  <img src="README.assets/fig 4-15.png" alt="fig 4-15" style="zoom:50%;" />

- 현재 신경망은 긍정적인 예에 대해서만 학습하여 부정적인 예에 대해서는 어떤 지식도 없습니다. 따라서 긍정적인 예에 대해서는 Sigmoid 출력을 1에 가깝게 만들고 부정적인 예에 대해서는 Sigmoid 출력을 0에 가깝게 만드는 것이 목표입니다. 이는 다음과 같습니다.

<img src="README.assets/fig 4-16.png" alt="fig 4-16" style="zoom:50%;" />

#### Note

> 다중 분류 문제를 이진 분류로 다루려면 '정답'과 '오답'에 대해 각각 바르게 분류할 수 있어야합니다.
>
> 따라서 긍정적 예와 부정적 예 모두를 대상으로 문제를 생각해야 합니다.

- 그렇다고 모든 부정적 예에 대해 이진 분류를 학습하는 것도 옳지 않습니다. 이 방식은 어휘 수가 늘어날수록 감당할 수 없기 때문입니다. 따라서 근사적인 해법으로 부정적 예를 선택하여 샘플링합니다. 이것이 바로 '네거티브 샘플링'이 의미하는 바입니다.

- 정리하면, 네거티브 샘플링 기법은 긍정적 예를 타깃으로 한 경우의 손실을 구하는 동시에 부정적 예를 몇 개 샘플링하여 그에 대한 손실을 구합니다. 그리고 각 데이터의 손실을 더한 값을 최종 손실로 사용합니다.

- 이 이야기들을 구체적인 예를 들어 살펴보겠습니다. 긍정적 예인 say와 샘플링된 부정적 예인 hello와 I에 대해 CBOW 모델의 은닉층 이후에 대해 주목하면 계산 그래프는 다음과 같습니다.

<img src="README.assets/fig 4-17.png" alt="fig 4-17" style="zoom:50%;" />

- 여기서 주의할 부분은 긍정적 예와 부정적 예를 다루는 방식으로 긍정적 예는 Sigmoid with Loss 계층에 정답 레이블로 1을, 부정적 예는 0을 입력하는 것입니다. 그런 다음 각 데이터의 손실을 모두 더해 최종 손실을 출력합니다.

### 4.2.6 네거티브 샘플링의 샘플링 기법

- 네거티브 샘플링에 관해 설명할 것이 하나 남았는데 그것은 어떻게 부정적 예를 샘플링하느냐 하는 것입니다. 단순히 무작위로 샘플링하는 것보다 좋은 방법이 있는데 말뭉치의 통계 데이터를 기초로 샘플링하는 것입니다.

  - 구체적으로 말뭉치에 자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어를 적게 추출하는 것입니다. 이를 기준으로 샘플링하려면 각 단어의 출현 횟수를 '확률분포'로 나타내고 그 분포에 따라 단어를 샘플링합니다.

<img src="README.assets/fig 4-18.png" alt="fig 4-18" style="zoom:50%;" />

- 말뭉치에서 단어별 출현 횟수를 바탕으로 확률분포를 구한 다음, 그 확률분포에 따라서 샘플링을 수행하면 되는데 자주 등장하는 단어가 선택될 가능성이 높고 '희소한 단어'는 선택되기 어렵습니다.

#### Note

> 네거티브 샘플링에서는 부정적 예를 가능한 한 많이 다루는 것이 좋지만 계산량 문제 때문에 적은 수로 한정합니다.
>
> 그런데 우연히 '희소한 단어'만 선택되었다면 결과가 나빠질 것입니다. 이는 실제로 희소한 단어가 거의 출현하지 않기 때문입니다.
>
> 즉, 드문 단어를 잘 처리하는 것보다 흔한 단어를 잘 처리하는 것이 좋은 결과로 이어집니다.

- 그러면 확률분포에 따라 샘플링하는 예를 python 코드로 설명하겠습니다. 이 때 numpy의 np.random.choice 메서드를 사용할 것이며 그 예는 다음과 같습니다.

```python
import numpy as np

# 0부터 9까지 숫자 중 무작위 하나를 선택합니다.
print(np.random.choice(10))

# words 중 무작위 하나를 선택합니다.
words = ['apple', 'samsung', 'intel', 'nvidia']
print(np.random.choice(words))

# 임의의 갯수를 무작위로 샘플링합니다.
print(np.random.choice(words, size=2))  # 중복이 가능합니다.
print(np.random.choice(words, size=2, replace=False))  # 중복이 불가능합니다.

# 확률분포에 따라 샘플링합니다.
p = [0.4, 0.1, 0.3, 0.2]
print(np.random.choice(words, p=p))
```

- 위의 코드는 np.random.choice의 무작위 샘플링 용도를 보여주는데 size를 통해 샘플링 크기를, replace를 통해 중복 여부를, p를 통해 확률분포에 근거해 샘플링을 수행합니다.

- 이제 이에 따라 부정적 예를 샘플링하는데 word2vec은 확률분포에 한 가지 수정을 권고합니다. 바로 기본 확률분포에 0.75를 제곱하는 것입니다.

<img src="README.assets/e 4-4.png" alt="e 4-4" style="zoom:50%;" />

- 여기서 P는 i번째 단어의 확률로 원래 확률분포의 각 요소에 0.75 제곱하는 것입니다. 다만, 수정 후 확률의 총합은 1이 되어야 하므로 분모도 수정 후 확률분포의 총합이 필요합니다.

- 다만, 이렇게 수정하는 이유는 출현 확률이 낮은 단어를 '버리지 않기' 위해서로 0.75 제곱을 통해 낮은 단어의 확률을 살짝 높일 수 있습니다. 이는 다음의 예시를 통해서도 확인할 수 있습니다.

```python
p = [0.7, 0.29, 0.01]
mod_p = np.power(p, 0.75)
mod_p /= np.sum(mod_p)
print(mod_p)  # [0.64, 0.33, 0.27]
```

- 이처럼 낮은 확률의 단어를 조금 더 쉽게 샘플링되도록 하기 위한 구제 조피로 0.75 제곱을 수행합니다. 저 수치는 이론적 의미가 없으니 다른 수치로 수정해도 됩니다.

- 이처럼 네거티브 샘플링은 말뭉치에서 단어의 확률분포를 만들고 0.75 제곱을 한 다음 부정적 예를 샘플링합니다. 이 책에서는 UnigramSampler라는 이름으로 이를 구현했습니다.

> 자세한 내용은 4.2.4_negative_sampling_layer.py의 UnigramSampler class를 확인하세요.

#### Note

> Unigram은 '하나의 연속된 단어'를 의미합니다. 이에 수가 늘어나면 Bigram, Trigram이 됩니다.
>
> 따라서 해당 class는 한 단어를 대상으로 확률분포를 만든다는 점을 의미합니다. 만약 Bigram이라면 두 단어로 구성된 대상에 대한 확률분포를 만듭니다.

- UnigramSampler class는 초기화 시 3개의 인수를 받는데 단어 ID 목록인 corpus, 확률분포에 제곱할 값인 power, 부정적 예 샘플링을 하는 횟수인 sample_size입니다.

  - 해당 클래스는 get_negative_sample(target) 메서드를 제공하는데 해당 메서드는 target 인수로 지정한 단어를 긍정적 예로 해석하고 그 외 단어 ID를 샘플링해 부정적 예를 고릅니다.

- 이 클래스를 사용하는 예시는 다음과 같습니다.

```python
corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])
power, sample_size = 0.75, 2

sampler = UnigramSampler(corpus, power, sample_size)
target = np.array([1, 3, 0])
negative_samples = sampler.get_negative_sample(target)
print(negative_samples)
```

- 위 예시는 긍정적 예로 3개의 데이터를 미니배치로 다뤘고 각 데이터에 대해 부정적 예를 2개씩 샘플링합니다. 이제 이를 기반으로 부정적 예를 샘플링할 수 있습니다.

### 4.2.7 네거티브 샘플링 구현

- 이제 네거티브 샘플링을 구현하겠습니다. 해당 클래스는 NagativeSamplingLoss라는 이름으로 구현되어 있습니다.

> 자세한 내용은 4.2.4_negative_sampling_layer.py의 NegativeSamplingLoss 클래스를 확인하세요.

- 초기화 메서드의 인수로 출력 측 가중치를 나타내는 W, 말뭉치를 뜻하는 corpus, 확률분포에 제곱할 값 power, 부정적 예 샘플링 횟수인 sample이 있습니다.

  - 여기에 앞에서 설명한 UnigramSampler를 sampler로 지정해 저장하고 부정적 예 샘플링 횟수는 sample_size에 저장합니다.

  - 인스턴스 변수인 loss_layers와 embed_dot_layers는 원하는 계층을 리스트로 보관하는데 부정적 예의 sample_size개에 긍정적 예 1개를 더해 sample_size + 1개의 계층을 생성합니다. 여기에서는 0번째 계층이 긍정적 예를 다룰 것입니다.

  - 마지막으로 매개변수와 기울기를 각각 배열로 저장합니다.

- 이어서 순전파인 forward 메서드는 은닉층 뉴런 h와 긍정적 예의 타깃인 target입니다.

  - 이 메서드는 self.sampler를 사용해 부정적 예를 샘플링하여 negative_sample에 저장합니다. 그 다음 긍정적 예와 부정적 예 각각의 데이터에 대해서 순전파를 수행하고 손실들을 더합니다.

  - 구체적으로 Embedding Dot 계층의 forward 점수를 구하고 그 점수와 레이블을 Sigmoid with Loss 계층으로 흘려 손실을 수합니다. 여기에서 긍정적 예의 정답 레이블은 1, 부정적 예의 정답 레이블은 0입니다.

- 마지막으로 역전파는 순전파의 역순으로 각 계층의 backward 메서드를 호출하면 됩니다. 은닉층의 뉴런은 순전파 시에 여러 개로 복사되었는데 이는 '1.3.4 계산 그래프'에서 설명한 Repeat 노드에 해당합니다. 따라서 역전파 때는 여러 기울기 값을 더해줍니다. 이상으로 네거티브 샘플링을 모두 구형했습니다.
