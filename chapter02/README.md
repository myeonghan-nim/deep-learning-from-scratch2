# chapter02. Dispersion Representation of Natural Language and Word

- 자연어 처리란 컴퓨터가 사람의 말을 알아듣게 만드는 것을 의미합니다.

- 이번 장에서는 컴퓨터에 말을 이해시킨다는 것이 무슨 의미인지, 어떤 방법들이 존재하는지를 중심으로 알아보겠습니다.

- 또한, 어떤 방법들이 존재하는지 알아보되 고전적인 기법들부터 딥러닝 기반 기법들까지 알아보겠습니다.

- 이번 장에서는 python으로 텍스트를 다루는 연습도 겸합니다.

  - 텍스트를 단어로 분할, 단어를 단어 ID로 변환 등을 구현하고 이 구현된 함수들은 다음 장에서도 사용됩니다.

## 2.1 자연어 처리란

- 우리가 평소에 쓰는 말을 **자연어**라고 합니다. 따라서 **자연어 처리(NLP)**는 자연어를 처리하는 분야입니다.

  - 더 쉽게 말해 '우리가 사용하는 말을 컴퓨터에게 이해시키기 위한 기술'을 의미하고 이를 통해 컴퓨터가 우리에게 도움이 되는 일을 수행하게 하는 것입니다.

- 그런데 컴퓨터가 이해할 수 있는 언어는 '프로그래밍 언어'나 '마크업 언어' 같은 것으로 코드의 의미를 고유하게 해석할 수 있도록 문법이 정의된 언어를 의미합니다.

- 즉, 일반적인 프로그래밍 언어는 기계적이고 고정되어 있습니다. 반면에 자연어는 부드러운 언어로 동일한 문장도 여러 형태로 표현 가능하는 등 의미나 형태가 유연하게 바뀝니다.

  - 이처럼 자연어는 살아 있는 언어로 컴퓨터에게 이를 이해시키는 것은 어려운 도전입니다.

  - 하지만, 이를 성공한다면 수많은 사람에게 도움이 되는 일을 컴퓨터에게 시킬 수 있습니다.

    - 예를 들어 검색 엔진, 기계 번역, 질의응답 시스템, IME, 문장 자동요약, 감정분석 등이 있습니다.

#### Note

> 자연어 처리를 응용한 예로 '질의응답 시스템'이 있고 가장 대표주자로 IBM의 Watson이 있습니다.
>
> 미국의 TV 퀴즈쇼에서 어떤 사람보다도 정확히 대답하여 우승한 이 AI는 '의사결정 지원 시스템'으로 분야를 넓혔습니다.
>
> 최근에 과거의 방대한 의료 데이터를 활용해 난치병 환자에게 올바른 치료법을 제안해 목숨을 구한 사례가 보고되기도 했습니다.

### 2.1.1 단어의 의미

- 우리의 말은 '문자'로 구성되며 말의 의미는 '단어'로 구성됩니다.

  - 단어는 최소 단위로 자연어를 컴퓨터를 이해시키는 데 무엇보다 '단어의 의미'를 이해시키는 것이 중요합니다.

- 이번 장의 주제는 컴퓨터에게 '단어의 의미' 이해시키기로 이를 잘 파악하는 표현 방법에 관해 알아볼 것입니다.

  - 이번 장과 다음 장에 걸쳐 나오는 대표적인 기법은 **시소러스**, **통계 기반 기법**, **추론 기반 기법**이 있습니다.

- 가장 먼저 사람의 손으로 만든 시소러스(유의어 사전)를 이용하는 방법을 간단히 살펴보고 통계 정보로 단어를 표현하는 '통계 기반 기법"을 배우겠습니다.

- 다음 장에서는 신경망을 활용한 '추론 기반 기법'을 배울 것이며 구체적으로는 word2vec 기법입니다.

## 2.2 시소러스

- '단어의 의미'를 나타내는 방법은 사람이 직접 단어의 의미를 정의하는 방식을 생각할 수 있습니다.

  - 그 중 한 방법으로 사전처럼 각 단어에 그 의미를 설명해 넣을 수 있습니다.

- 자연어 처리의 역사에서 단어의 의미를 인력을 동원해 정의하려는 시도는 수없이 있었으나 사람이 이용하는 일반적인 사전이 아닌 **시소러스**를 애용했습니다.

  - 시소러스는 유의어 사전으로 동의어나 유의어가 한 그룹으로 분류되어 있습니다.

  <img src="README.assets/fig 2-1.png" alt="fig 2-1" style="zoom:50%;" />

  - 또한 자연어 처리에 이용되는 시소러스에서는 단어 사이의 상하관계, 포함관례 등 더 세세한 관계까지 정의한 경우도 있으며 그 관계는 다음과 같습니다.

  <img src="README.assets/fig 2-2.png" alt="fig 2-2" style="zoom:50%;" />

- 이처럼 모든 단어에 대한 유의어 집합을 만들고 단어들의 관계를 그래프로 표현하여 단어 사이의 연결을 정의할 수 있습니다.

  - 그럼 이 '단어 네트워크'를 컴퓨터에 입력해 가르칠 수 있고 간접적으로 단어의 의미를 이해시킬 수 있습니다.

#### Note

> 시소러스를 어떻게 사용하는가는 자연어 처리 애플리케이션에 따라 다릅니다.
>
> 검색엔진의 예시에서는 car와 automobile이 유의어임을 안다면 car의 검색 결과에 automobile의 검색 결과도 보여줄 것입니다.

### 2.2.1 WordNet

- 자연어 처리 분야에서 가장 유명한 시소러스는 **WordNet**으로 1985년부터 구축된 전통적인 시소러스입니다. 현재도 많은 연구와 다양한 자연어 처리 애플리케이션에 활용됩니다.

- WordNet을 사용하면 유의어를 얻거나 '단어 네트워크'를 사용할 수 있으며 단어 사의 유사도를 단어 네트워크를 통해 구할 수 있습니다.

  - 이 책에서는 자세히 설명하지 않으나 'Appedix B. WordNet 맛보기'에 일부가 설명되어 있습니다.

  - 해당 부록에서는 WordNet, 정확히는 NLTK 모듈을 설치하고 몇 가지 간단한 실험을 합니다.

#### Note

> Appedix B에서는 실제 WordNet을 사용한 단어 유사도 구하기 실험을 합니다.
>
> 사람이 정의한 '단어 네트워크'를 기초로 단어 사이의 유사도를 구하면 '단어의 의미'를 이해하는 첫걸음을 내딛었다고 할 수 있습니다.

### 2.2.2 시소러스의 문제점

- WordNet과 같은 시소러스에는 수많은 단어에 대한 동의어와 계층 구조 등 관계가 정의되어이으며 단어의 의미를 컴퓨터에 전달할 수 있습니다.

- 하지만 사람이 수작업으로 레이블링하는 방식에는 큰 결점이 존재하는데 다음은 시소러스 방식의 대표적 문제점들입니다.

  - 시대 변화에 대응하기 어렵다.

    - 우리가 사용하는 말은 살아있어 신조어가 등장하고 옛말이 잊혀집니다.

    - 또한 시대에 따라 언어의 의미가 변하기도 하는데 대표적으로 'heavy'는 '심각하다'는 의미도 있지만 과거에는 없었던 의미입니다.

    - 이에 대응하려면 사람이 시소러스를 끊임없이 갱신해야 합니다.

  - 사람을 쓰는 비용이 크다.

    - 시소러스를 만드는 데 엄청난 인적 비용이 발생합니다. 이상적으로 방대한 단어 모두에 대해 관계를 정의하고 뜻을 기록하는 것은 매우 힘든 일입니다.

  - 단어의 미묘한 차이를 표현할 수 없다.

    - 시소러스는 뜻이 비슷한 단어를 묶지만 실제로 비슷한 단어들이라도 미묘한 차이가 있습니다. 이를 수작업으로 하기엔 상당히 곤란한 일입니다.

- 이처럼 시소러스를 사용하는 기법은 커다란 문제가 있습니다.

  - 이 문제를 해결하기 위해 '통계 기반 기법'과 신경망을 사용한 '추론 기반 기법'이 등장합니다.

  - 이 두 기법은 대량의 텍스트 데이터에서 '단어의 의미'를 자동으로 추출하여 사람이 수작업으로 하지 않습니다.

#### Note

> 자연어 처리뿐 아니라 이미지 인식도 특징을 사람이 수동으로 설계하는 일이 오랜 세월 계속되었습니다.
>
> 그러다 딥러닝이 실용화되며 이미지에서 원하는 결과를 바로 얻을 수 있게 되었습니다.
>
> 이는 자연어 처리에서 벌어지는 일과 마찬가지도 사람의 개입을 최소한으로 줄이고 텍스트 데이터 만으로 원하는 결과를 얻어내는 방향으로 변화 중입니다.

## 2.3 통계 기반 기법

- 이제부터 통계 기반 기법을 살펴보며 **말뭉치**를 이용할 것입니다.

  - 말 뭉치란 간단히 말해 대량의 텍스트 데이터로 맹목적으로 수집된 데이터가 아닌 자연어 처리 연구나 애플리케이션을 염두에 두고 수집된 데이터를 의미합니다.

- 결국 말뭉치란 텍스트 데이터에 지나지 않지만 그 안의 문장들은 사람이 쓴 글이므로 다른 시각에서 자연어에 대한 사람의 지식이 충분히 담겨 있다고 볼 수 있습니다.

  - 문장을 쓰는 방법, 단어를 선택하는 방법, 단어의 의미 등 사람이 알고 있는 자연어에 대한 지식을 바탕으로 통계 기반 기법은 말뭉치에서 자동으로, 효율적으로 핵심을 추출하는 것입니다.

#### Warning

> 자연어 처리에 사용되는 말뭉치에는 텍스트 데이터에 대한 추가 정보가 포함되는 경우가 있습니다.
>
> 예를 들어 데이터의 단어 각각에 '품사'가 레이블링될 수 있습니다.
>
> 이 경우 말뭉치는 컴퓨터가 다루기 쉬운 형태(트리 등)로 가공되어 주어집니다.
>
> 이 책에서는 이러한 추가 레이블 없이 단순한 텍스트 데이터로 주어졌다고 가정합니다.

### 2.3.1 Python으로 말뭉치 전처리하기

- 자연어 처리에는 다양한 말뭉치가 사용되는데 유명한 것으로 위키백과나 구글 뉴스 등이 있으며 세익스피어와 같은 대문호의 작품들도 사용됩니다.

  - 이번 장에서는 우선 문장 하나로 이루어진 단순한 텍스트를 사용하고 그 뒤 실용적인 말뭉치를 다뤄보겠습니다.

- 우선 python을 이용해 매우 작은 텍스트 데이터에 전처리를 하겠습니다.

  - 여기서 전처리란 데이터를 단어로 분할하고 그 분할된 단어들을 ID 목록으로 변환하는 것을 의미합니다.

```python
# 이번 절에서 사용할 말뭉치는 문장 하나로 이루어진 텍스트입니다.
text = 'You say goodbye and I say hello.'

text = text.lower()  # 소문자로 만듭니다.
text = text.replace('.', ' .')  # 문장 부호에 띄어쓰기를 넣습니다.
print(text)

words = text.split(' ')  # 띄어쓰기를 기준으로 나눕니다.
print(words)
```

#### Warning

> 여기서 단어를 분할할 때 문장부호 앞에 공백을 넣었지만 더 현명하고 범용적인 방법으로 '정규표현식'이 있습니다.
>
> 정규표현식 라이브러리 re를 import하고 `re.split('(\W+)?'.text)`를 호출하면 단어 단위로 쉽게 분할할 수 있습니다.
>
> 더 자세한 내용은 정규표현식 관련 도서 등을 참고하세요.

- 이제 원래 문장을 단어 목록으로 이용할 수 있으나 단어를 텍스트 그대로 조작하기란 여러 면에서 불편합니다.

- 따라서 단어에 ID를 부여하고 ID 리스트로 이용할 수 있도록 손질할 것이며 그 사전 준비로 python의 dictionary를 이용해 단어 ID와 단어를 짝지어 주겠습니다.

```python
word_to_id, id_to_word = {}, {}
for word in words:
    if word not in word_to_id:
        new_id = len(word_to_id)
        word_to_id[word], id_to_word[new_id] = new_id, word
```

- 여기에서 단어 ID에서 단어로 변환은 id_to_word가 담당하며 단어에서 단어 ID로 변환은 word_to_id가 담당합니다.

  - 이를 통해 단어 ID와 단어 대응표가 만들어졌습니다.

- 마지막으로 '단어 목록'을 '단어 ID 목록'으로 바꾸어 보겠습니다.

```python
import numpy as np

corpus = np.array([word_to_id[word] for word in words])
print(corpus)
```

#### Note

> 내포(comprehension)은 반복문 처리를 간단히 쓰기 위한 기법으로 위의 예시처럼 사용됩니다.

- 이것으로 말뭉치를 사용하기 위한 사전 준비는 끝났습니다. 이 처리를 한 데 모아 함수로 구현하면 다음과 같습니다.

> 자세한 내용은 chapter02/commons/util.py의 preprocess() 함수를 확인하세요.

- 여기서 준비한 corpus, word_to_id, id_to_word는 앞으로 이 책 곳곳에서 등장하며 각각 단어 ID 목록, 단어에서 단어 ID 변환, 단어 ID에서 단어로 변환을 의미합니다.

- 이상으로 말뭉치를 다룰 준비를 마쳤으니 이제 이를 사용해 '단어의 의미'를 추출하겠습니다. 그 한 방법으로 이번 절에서는 '통계 기반 기법'을 살펴볼 것입니다.

  - 이 기법은 단어를 벡터로 표현하여 다루는 방법입니다.

### 2.3.2 단어의 분산 표현

- 세상의 다채로운 '색'을 표현하는 방법은 '파랑', '초록'과 같이 색의 수 만큼 이름을 부여하는 방법과 RGB를 사용한 3차원 벡터로 표현하는 방법이 있습니다.

  - 여기서 주목할 점은 RGB 벡터 표현이 색을 더 정확하게 명시할 수 있고 더 간결하게 표현할 수 있으며 더 짐작하기 쉽습니다.

  - 또한, 색상 사이의 관련성(비슷한 색인지의 여부 등)도 벡터 방식이 표현하기 더 쉽습니다.

- 이처럼 '단어'를 벡터로 표현할 수 있다면 더 편할 것입니다. 그를 위해서 간결하고 이치에 맞는 벡터 표현을 단어에 구축할 수 있어야 합니다.

  - 바로 이렇게 '단어의 의미'를 정확하게 파악할 수 있는 벡터 표현을 만드는게 자연어 처리의 **분산 표현**입니다.

#### Note

> 단어의 분산 표현은 단어를 고정 길이의 밀집벡터로 표현합니다.
>
> 밀집 벡터를 대부분의 원소가 0이 아닌 실수인 벡터를 의미합니다.
>
> 이를 통해서 단어의 분산 표현을 어떻게 구축할 것인가 알아볼 것입니다.

### 2.3.3 분포 가설

- 자연어 처리의 역사에서 단어를 벡터로 표현하는 연구는 수없이 이뤄져 왔는데 중요한 기번의 거의 모두가 단 하나의 간단한 아이디어에 뿌리를 두고 있습니다.

- 그 아이디어는 '단어의 의미는 주변 단어에 의해 형성된다'는 점으로 이를 **분포 가설**이라고 합니다.

- 분포 가설이 말하고자 하는 바는 매우 간단한데 단어 자체에는 의미가 없으며 그 단어가 사용된 '맥락'이 의미를 형성한다는 것입니다.

  - 물론 의미가 같은 단어들은 같은 맥락에서 더 많이 등장하는데 예를 들어 'I drink beer'와 'I guzzle beer' 모두 같은 맥락에서 사용됨을 보여줍니다.

  - 그에 따라 이번 장에서는 '맥락'이라는 말을 자주 사용할텐데 '맥락'이란 주목하는 단어 주변에 놓인 단어를 의미합니다.

  <img src="README.assets/fig 2-3.png" alt="fig 2-3" style="zoom:50%;" />

  - 이처럼 '맥락'이란 특정 단어를 중심에 둔 그 주변 단어를 의미하고 맥락의 크기, 즉 주변 단어를 몇 개나 포함할지를 '윈도우 크기'라고 합니다.

#### Warning

> 여기에서는 좌우로 똑같은 수의 단어를 맥락으로 사용했습니다.
>
> 하지만 상황에 따라서 한쪽의 단어만 쓰거나 문장의 시작과 끝을 고려하기도 합니다.
>
> 이 책에서는 이해하기 쉽게 문장 구분을 고려하지 않고 좌우 동수인 맥락만 취급합니다.

### 2.3.4 동시발생 행렬

- 이제 분포 가설에 기초해 단어를 벡터로 나타내는 방법을 생각해보면 주변 단어를 '세어보는' 방법이 자연스레 떠오를 것입니다.

  - 즉, 어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법입니다. 이를 '통계 기반 기법'이라고 하겠습니다.

- 통계 기반 기법은 이전 '2.3.1 Python으로 말뭉치 전처리하기'에서 봤던 말뭉치와 preprocess 함수를 사용해 전처리하는 일에서 시작합니다.

```python
from commons.util import preprocess
import numpy as np

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

print(corpus)
print(id_to_word)
```

- 위 결과에 따르면 단어의 수는 총 7개입니다. 이제 이를 바탕으로 각 단어의 맥락에 해당하는 단어의 빈도를 세어보겠습니다.

  - 여기서 윈도우의 크기는 1이며 단어 ID가 0인 you부터 시작하겠습니다.

  <img src="README.assets/fig 2-4.png" alt="fig 2-4" style="zoom:50%;" />

  - 이처럼 you의 맥락은 say라는 단어 하나뿐입니다. 이를 표로 정리하면 다음과 같습니다.

  <img src="README.assets/fig 2-5.png" alt="fig 2-5" style="zoom:50%;" />

  - 이처럼 you의 맥락으로 동시에 발생하는 단어의 빈도를 정리하면 you는 [0, 1, 0, 0, 0, 0, 0]이라는 벡터로 표현할 수 있습니다.

  - 이어서 say에 대해서도 같은 작업을 하면 다음과 같습니다.

  <img src="README.assets/fig 2-6.png" alt="fig 2-6" style="zoom:50%;" />

  - 즉, say는 [1, 0, 1, 0, 1, 1, 0]으로 표현할 수 있습니다. 이제 이 작업을 모든 단어에 대해 수행하면 다음과 같습니다.

  <img src="README.assets/fig 2-7.png" alt="fig 2-7" style="zoom:50%;" />

- 이처럼 모든 단어에 대해 동시발생하는 단어를 표에 정리하면 행은 해당 단어를 표현한 벡터가 됩니다. 이 때 이 표가 행렬의 형태를 띈다는 뜻에서 **동시발생 행렬**이라고 합니다.

  - 이제 이를 python으로 직접 구현하면 다음과 같습니다.

  ```python
  C = np.array([
      [0, 1, 0, 0, 0, 0, 0],
      [1, 0, 1, 0, 1, 1, 0],
      [0, 1, 0, 1, 0, 0, 0],
      [0, 0, 1, 0, 1, 0, 0],
      [0, 1, 0, 1, 0, 0, 0],
      [0, 1, 0, 0, 0, 0, 1],
      [0, 0, 0, 0, 0, 1, 0]
  ], dtype=np.int32)
  ```

- 이제 이를 기반으로 각 단어의 벡터를 다음과 같이 얻을 수 있습니다.

```python
print(C[0])  # ID가 0인 단어의 벡터 표현
print(C[word_to_id['goodbye']])  # goodbye의 벡터 표현
```

- 이처럼 동시발생 행렬을 바탕으로 단어를 벡터로 나타낼 수 있습니다. 이제 이를 자동화해 볼 것입니다.

  - 말뭉치에서 동시발생 행렬을 만들어주는 함수를 create_co_matrix(corpus, vocab_size, window_size=1)로 만든다면 다음과 같습니다.

  > 자세한 내용은 chapter02/commons/util.py의 create_co_matrix 함수를 확인하세요.

  - 해당 함수는 co_matrix를 0으로 채운 2차원 배열로 초기화한 뒤 말뭉치의 모든 단어 각각에 대해 윈도우에 포함된 주변 단어를 세어나갈 뿐 아니라 말뭉치의 경계도 확인합니다.

### 2.3.5 벡터 간 유사도

- 앞에서 동시발생 행렬을 활용해 단어를 벡터로 표현하는 방법을 알아봤으니 이어서 벡터 사이의 유사도를 측정하는 방법을 살펴보겠습니다.

- 벡터 사이의 유사도를 측정하는 방법은 다양하며 대표적으로 벡터의 내적이나 유클리드 거리 등을 꼽을 수 있습니다.

  - 다양한 방법 가운데 단어 벡터 사이의 유사도를 나타낼 때는 **코사인 유사도**를 자주 이용합니다. 이는 다 벡터가 존재할 때 다음과 같이 정의합니다.

  <img src="README.assets/e 2-1.png" alt="e 2-1" style="zoom:50%;" />

  - 분자에는 벡터의 내적, 분모에는 각 벡터의 노름이 등장하고 분모의 노름은 'L2노름'(각 원소의 제곱을 더한 뒤 제곱근을 구한 것)을 사용합니다. 즉, 벡터를 정규화하고 내적을 구합니다.

#### Note

> 코사인 유사도를 직관적으로 풀어보자면 '두 벡터가 가리키는 방향이 얼마나 유사한가'입니다.
>
> 두 벡터의 방향이 완전히 같다면 1, 반대라면 -1이 됩니다.

- 이제 이 방식을 python으로 구현해보겠습니다.

> 자세한 내용은 chapter02/commons/util.py의 cos_similarity 함수를 확인하세요.

- 이 코드에서 x, y를 numpy 배열이라고 가정하며 각 벡터를 정규화한 뒤 두 벡터의 내적을 구히먄 코사인 유사도를 구할 수 있습니다.

  - 다만 제로 벡터가 들어올 경우 0으로 나누기가 되어버리므로 매우 작은 값인 epsilon을 넣어 이를 방지하겠습니다.

#### Note

> epsilon은 일반적으로 부동소수점 계산 시 반올리됨어 다른 값에 흡수됩니다.
>
> 앞의 구현에서는 이 값이 벡터의 노름에 흡수되므로 대부분의 경우 eps를 더한다고 해서 최종 결과에 영향을 주지 않습니다.
>
> 한편, 벡터의 노름이 0일 때에는 이 작은 값이 그대로 유지되어 '0으로 나누기' 오류를 방지합니다.

- 이제 이 함수를 사용해 단어 벡터의 유사도를 구해보겠습니다.

> 자세한 내용은 2.3.5_similarity.py를 확인하세요.

### 2.3.6 유사 단어의 랭킹 표시

- 코사인 유사도까지 구현했으니 이 함수를 활용해 어떤 단어가 검색어로 주어질 때 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 구현하겠습니다.

  - 이름은 most_similar가 될 것이고 받을 인수는 다음과 같습니다.

    | 인수명      | 설명                                                            |
    | ----------- | --------------------------------------------------------------- |
    | query       | 검색어(단어)                                                    |
    | word_to_id  | 단어에서 단어 ID로 바꾸는 dictionary                            |
    | id_to_word  | 단어 ID에서 단어로 바꾸는 dictionary                            |
    | word_matrix | 단어 벡터들을 한데 모은 행렬, 각 행에는 대응하는 단어 벡터 저장 |
    | top         | 상위 몇 개 까지 출력할지 설정                                   |

  > 이를 구현한 함수는 chapter02/commons/util.py의 most_similar 함수를 확인하세요.

- 위의 코드는 다음의 순서대로 동작합니다.

  1. 검색어의 단어 벡터를 꺼냅니다.

  2. 검색어의 단어 벡터와 다른 모든 단어 벡터와의 코사인 유사도를 각각 구합니다.

  3. 계산한 코사인 유사도 결과를 기준으로 값이 높은 순서대로 출력합니다.

- 여기서 similarity 배열에 담긴 원소의 인덱스를 내림차순으로 정렬한 후 상위 원소들을 출력하기 위해서 argsort()라는 메서드를 사용합니다.

```python
x = np.array([100, -20, 2])
print(x.argsort())  # [1, 2, 0]
```

- 이 메서드는 해당 원소들을 오름차순으로 정렬하고 가장 작은 것부터 해당 인덱스를 반환합니다.

  - 즉, 앞의 결과는 -20, 2, 100 순서이므로 해당 숫자의 원래 인덱스인 1, 2, 0이 반환된 것입니다.

- 반면에 가장 큰 값부터 정렬하고 싶다면 해당 numpy 배열에 -1을 곱해주면 됩니다.

```python
print((-x).argsort())
```

- 이제 이 함수를 사용해 you를 검색어로 지정하여 유사한 단어들을 출력해보겠습니다.

> 자세한 내용은 2.3.6_most_similar.py를 확인하세요.

- 결과를 확인하면 goodbye와 I, hello가 유사도가 높게 나옵니다만 개선할 사항이 있습니다.

  - 물론 지금은 말뭉치의 크기가 작은 것이 원인이지만 나중에 큰 말뭉치를 사용해 동일한 실험을 하면 개선 사항을 더 잘 알 수 있습니다.

- 지금까지 동시발생 행렬을 사용해 단어를 벡터로 표현하는 통계 기반 기법의 '기본'을 마치겠습니다.

  - 이제 다음 절에서는 지금의 방법을 한층 개선하는 아이디어와 이를 구현하겠습니다.

## 2.4 통계 기반 기법 개선하기

- 앞 절에서 단어의 동시발생 행렬을 만들고 이를 이용해 단어를 벡터로 표현했지만 아직 개선할 점이 있습니다.

  - 따라서 이번 절에서는 이를 개선해보고 더 실용적인 말뭉치를 사용해 '진짜' 단어의 분산 표현을 해보겠습니다.

### 2.4.1 상호정보량

- 앞 절에서 본 동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타내지만 사실 그리 좋은 특징은 아닙니다.

  - 이는 고빈도 단어에서 바로 찾을 수 있는데 예를 들어 'the car'라는 단어는 단순히 차를 지칭하지만 'the'와 'car'와 연관성이 깊지 않습니다.

  - 오히려 'drive'와 같은 단어와 'car'의 연관성이 깊지만 단순히 'the'가 고빈도 단어이기 때문에 발생하는 현상입니다.

- 이 문제를 해결하기 위해 **점별 상호정보량**이라는 척도를 사용합니다.

  - PMI라고도 불리는 이것은 다음 식으로 정의됩니다.

  <img src="README.assets/e 2-2.png" alt="e 2-2" style="zoom:50%;" />

  - 위 식에 따르면 x, y가 동시에 일어날 확률을 x가 일어날 확률과 y가 일어날 확률을 곱한 것으로 나누는데 이 값이 높을수록 관련성이 높다는 것입니다.

- 이 식을 사용하면 어떤 단어 x가 말뭉치에 등장할 확률을 의미합니다. 이제 이 식을 동시발생 행렬에 적용해보겠습니다.

  - 여기서 C는 동시발생 행렬이며 x, y는 앞선 식의 x, y와 동일합니다. 다만, C(x)나 C(y)는 호가률이 아닌 등장 횟수를 의미합니다.

  <img src="README.assets/e 2-3.png" alt="e 2-3" style="zoom:50%;" />

  - 이 식을 사용해 동시발생행렬의 PMI를 계산할 수 있습니다. 앞선 예처럼 the, car, drive가 각각 1000, 20, 10번 등장하고 전체 단어 수가 10000개라면 PMI는 다음과 같습니다.

  <img src="README.assets/e 2-4.png" alt="e 2-4" style="zoom:50%;" />

  <img src="README.assets/e 2-5.png" alt="e 2-5" style="zoom:50%;" />

- 즉, PMI를 사용하면 단어가 단독으로 출현하는 횟수를 고려하므로 더 나은 결과를 얻을 수 있습니다.

- 다만, PMI도 문제점이 있는데 두 단어의 동시발생 횟수가 0이 되면 log의 성질에 의해 -inf가 된다는 점입니다. 따라서 이를 회피하기 위해 실제로 **양의 상호정보량**, PPMI를 구현합니다.

<img src="README.assets/e 2-6.png" alt="e 2-6" style="zoom:50%;" />

- 이 식에 따라 PMI가 음수일 때는 0으로 취급됩니다. 이를 python으로 구현하면 다음과 같습니다.

> 자세한 내용은 chapter02/commons/util.py의 ppmi 함수를 확인하세요.

- 여기에서 C는 동시발생 행렬, verbose는 진행상황 출력 여부를 결정하는 인자입니다.

  - 이 코드는 동시발생 행렬에 대해서만 PPMI 행렬을 구할 수 있도록 단순화한 것으로 완전히 원본과 같은 값은 아니지만 근사값으로 계산합니다.

  - 또한 epsilon을 사용해 np.log2(0)이 -inf가 되는 걸 막습니다.

#### Note

> '2.3.5 벡터간 유사도'에서 '0으로 나누기'를 막기 위해 eps를 사용한 것처럼 여기에서도 그를 사용한 것입니다.

- 이제 이를 사용해 실제 계산해볼 것입니다. 구현은 다음과 같습니다.

> 2.4.1_ppmi.py를 확인하세요.

- 이것으로 동시발생 행렬을 통해 PPMI 행렬을 만드는 방법을 알아보았습니다만 아직 PPMI 행렬도 문제점이 많습니다.

  - 우선 말뭉치의 어휘 수가 증가할수록 벡터의 차원 수도 증가합니다. 예를 들면 100000 단어에 대해 해당 차원의 벡터를 다루기는 무리입니다.

  - 또한 행렬 원소 대부분이 0입니다. 즉, 벡터의 원소 대부분이 중요하지 않다는 뜻입니다.

    - 이런 벡터는 각 원소의 중요도가 낮아 노이즈에 약하고 견고하지 못합니다. 따라서 이 문제를 해결하는 방법이 벡터의 차원 감소입니다.
