# chapter02. Dispersion Representation of Natural Language and Word

- 자연어 처리란 컴퓨터가 사람의 말을 알아듣게 만드는 것을 의미합니다.

- 이번 장에서는 컴퓨터에 말을 이해시킨다는 것이 무슨 의미인지, 어떤 방법들이 존재하는지를 중심으로 알아보겠습니다.

- 또한, 어떤 방법들이 존재하는지 알아보되 고전적인 기법들부터 딥러닝 기반 기법들까지 알아보겠습니다.

- 이번 장에서는 python으로 텍스트를 다루는 연습도 겸합니다.

  - 텍스트를 단어로 분할, 단어를 단어 ID로 변환 등을 구현하고 이 구현된 함수들은 다음 장에서도 사용됩니다.

## 2.1 자연어 처리란

- 우리가 평소에 쓰는 말을 **자연어**라고 합니다. 따라서 **자연어 처리(NLP)**는 자연어를 처리하는 분야입니다.

  - 더 쉽게 말해 '우리가 사용하는 말을 컴퓨터에게 이해시키기 위한 기술'을 의미하고 이를 통해 컴퓨터가 우리에게 도움이 되는 일을 수행하게 하는 것입니다.

- 그런데 컴퓨터가 이해할 수 있는 언어는 '프로그래밍 언어'나 '마크업 언어' 같은 것으로 코드의 의미를 고유하게 해석할 수 있도록 문법이 정의된 언어를 의미합니다.

- 즉, 일반적인 프로그래밍 언어는 기계적이고 고정되어 있습니다. 반면에 자연어는 부드러운 언어로 동일한 문장도 여러 형태로 표현 가능하는 등 의미나 형태가 유연하게 바뀝니다.

  - 이처럼 자연어는 살아 있는 언어로 컴퓨터에게 이를 이해시키는 것은 어려운 도전입니다.

  - 하지만, 이를 성공한다면 수많은 사람에게 도움이 되는 일을 컴퓨터에게 시킬 수 있습니다.

    - 예를 들어 검색 엔진, 기계 번역, 질의응답 시스템, IME, 문장 자동요약, 감정분석 등이 있습니다.

#### Note

> 자연어 처리를 응용한 예로 '질의응답 시스템'이 있고 가장 대표주자로 IBM의 Watson이 있습니다.
>
> 미국의 TV 퀴즈쇼에서 어떤 사람보다도 정확히 대답하여 우승한 이 AI는 '의사결정 지원 시스템'으로 분야를 넓혔습니다.
>
> 최근에 과거의 방대한 의료 데이터를 활용해 난치병 환자에게 올바른 치료법을 제안해 목숨을 구한 사례가 보고되기도 했습니다.

### 2.1.1 단어의 의미

- 우리의 말은 '문자'로 구성되며 말의 의미는 '단어'로 구성됩니다.

  - 단어는 최소 단위로 자연어를 컴퓨터를 이해시키는 데 무엇보다 '단어의 의미'를 이해시키는 것이 중요합니다.

- 이번 장의 주제는 컴퓨터에게 '단어의 의미' 이해시키기로 이를 잘 파악하는 표현 방법에 관해 알아볼 것입니다.

  - 이번 장과 다음 장에 걸쳐 나오는 대표적인 기법은 **시소러스**, **통계 기반 기법**, **추론 기반 기법**이 있습니다.

- 가장 먼저 사람의 손으로 만든 시소러스(유의어 사전)를 이용하는 방법을 간단히 살펴보고 통계 정보로 단어를 표현하는 '통계 기반 기법"을 배우겠습니다.

- 다음 장에서는 신경망을 활용한 '추론 기반 기법'을 배울 것이며 구체적으로는 word2vec 기법입니다.

## 2.2 시소러스

- '단어의 의미'를 나타내는 방법은 사람이 직접 단어의 의미를 정의하는 방식을 생각할 수 있습니다.

  - 그 중 한 방법으로 사전처럼 각 단어에 그 의미를 설명해 넣을 수 있습니다.

- 자연어 처리의 역사에서 단어의 의미를 인력을 동원해 정의하려는 시도는 수없이 있었으나 사람이 이용하는 일반적인 사전이 아닌 **시소러스**를 애용했습니다.

  - 시소러스는 유의어 사전으로 동의어나 유의어가 한 그룹으로 분류되어 있습니다.

  <img src="README.assets/fig 2-1.png" alt="fig 2-1" style="zoom:50%;" />

  - 또한 자연어 처리에 이용되는 시소러스에서는 단어 사이의 상하관계, 포함관례 등 더 세세한 관계까지 정의한 경우도 있으며 그 관계는 다음과 같습니다.

  <img src="README.assets/fig 2-2.png" alt="fig 2-2" style="zoom:50%;" />

- 이처럼 모든 단어에 대한 유의어 집합을 만들고 단어들의 관계를 그래프로 표현하여 단어 사이의 연결을 정의할 수 있습니다.

  - 그럼 이 '단어 네트워크'를 컴퓨터에 입력해 가르칠 수 있고 간접적으로 단어의 의미를 이해시킬 수 있습니다.

#### Note

> 시소러스를 어떻게 사용하는가는 자연어 처리 애플리케이션에 따라 다릅니다.
>
> 검색엔진의 예시에서는 car와 automobile이 유의어임을 안다면 car의 검색 결과에 automobile의 검색 결과도 보여줄 것입니다.

### 2.2.1 WordNet

- 자연어 처리 분야에서 가장 유명한 시소러스는 **WordNet**으로 1985년부터 구축된 전통적인 시소러스입니다. 현재도 많은 연구와 다양한 자연어 처리 애플리케이션에 활용됩니다.

- WordNet을 사용하면 유의어를 얻거나 '단어 네트워크'를 사용할 수 있으며 단어 사의 유사도를 단어 네트워크를 통해 구할 수 있습니다.

  - 이 책에서는 자세히 설명하지 않으나 'Appedix B. WordNet 맛보기'에 일부가 설명되어 있습니다.

  - 해당 부록에서는 WordNet, 정확히는 NLTK 모듈을 설치하고 몇 가지 간단한 실험을 합니다.

#### Note

> Appedix B에서는 실제 WordNet을 사용한 단어 유사도 구하기 실험을 합니다.
>
> 사람이 정의한 '단어 네트워크'를 기초로 단어 사이의 유사도를 구하면 '단어의 의미'를 이해하는 첫걸음을 내딛었다고 할 수 있습니다.

### 2.2.2 시소러스의 문제점

- WordNet과 같은 시소러스에는 수많은 단어에 대한 동의어와 계층 구조 등 관계가 정의되어이으며 단어의 의미를 컴퓨터에 전달할 수 있습니다.

- 하지만 사람이 수작업으로 레이블링하는 방식에는 큰 결점이 존재하는데 다음은 시소러스 방식의 대표적 문제점들입니다.

  - 시대 변화에 대응하기 어렵다.

    - 우리가 사용하는 말은 살아있어 신조어가 등장하고 옛말이 잊혀집니다.

    - 또한 시대에 따라 언어의 의미가 변하기도 하는데 대표적으로 'heavy'는 '심각하다'는 의미도 있지만 과거에는 없었던 의미입니다.

    - 이에 대응하려면 사람이 시소러스를 끊임없이 갱신해야 합니다.

  - 사람을 쓰는 비용이 크다.

    - 시소러스를 만드는 데 엄청난 인적 비용이 발생합니다. 이상적으로 방대한 단어 모두에 대해 관계를 정의하고 뜻을 기록하는 것은 매우 힘든 일입니다.

  - 단어의 미묘한 차이를 표현할 수 없다.

    - 시소러스는 뜻이 비슷한 단어를 묶지만 실제로 비슷한 단어들이라도 미묘한 차이가 있습니다. 이를 수작업으로 하기엔 상당히 곤란한 일입니다.

- 이처럼 시소러스를 사용하는 기법은 커다란 문제가 있습니다.

  - 이 문제를 해결하기 위해 '통계 기반 기법'과 신경망을 사용한 '추론 기반 기법'이 등장합니다.

  - 이 두 기법은 대량의 텍스트 데이터에서 '단어의 의미'를 자동으로 추출하여 사람이 수작업으로 하지 않습니다.

#### Note

> 자연어 처리뿐 아니라 이미지 인식도 특징을 사람이 수동으로 설계하는 일이 오랜 세월 계속되었습니다.
>
> 그러다 딥러닝이 실용화되며 이미지에서 원하는 결과를 바로 얻을 수 있게 되었습니다.
>
> 이는 자연어 처리에서 벌어지는 일과 마찬가지도 사람의 개입을 최소한으로 줄이고 텍스트 데이터 만으로 원하는 결과를 얻어내는 방향으로 변화 중입니다.

## 2.3 통계 기반 기법

- 이제부터 통계 기반 기법을 살펴보며 **말뭉치**를 이용할 것입니다.

  - 말 뭉치란 간단히 말해 대량의 텍스트 데이터로 맹목적으로 수집된 데이터가 아닌 자연어 처리 연구나 애플리케이션을 염두에 두고 수집된 데이터를 의미합니다.

- 결국 말뭉치란 텍스트 데이터에 지나지 않지만 그 안의 문장들은 사람이 쓴 글이므로 다른 시각에서 자연어에 대한 사람의 지식이 충분히 담겨 있다고 볼 수 있습니다.

  - 문장을 쓰는 방법, 단어를 선택하는 방법, 단어의 의미 등 사람이 알고 있는 자연어에 대한 지식을 바탕으로 통계 기반 기법은 말뭉치에서 자동으로, 효율적으로 핵심을 추출하는 것입니다.

#### Warning

> 자연어 처리에 사용되는 말뭉치에는 텍스트 데이터에 대한 추가 정보가 포함되는 경우가 있습니다.
>
> 예를 들어 데이터의 단어 각각에 '품사'가 레이블링될 수 있습니다.
>
> 이 경우 말뭉치는 컴퓨터가 다루기 쉬운 형태(트리 등)로 가공되어 주어집니다.
>
> 이 책에서는 이러한 추가 레이블 없이 단순한 텍스트 데이터로 주어졌다고 가정합니다.

### 2.3.1 Python으로 말뭉치 전처리하기

- 자연어 처리에는 다양한 말뭉치가 사용되는데 유명한 것으로 위키백과나 구글 뉴스 등이 있으며 세익스피어와 같은 대문호의 작품들도 사용됩니다.

  - 이번 장에서는 우선 문장 하나로 이루어진 단순한 텍스트를 사용하고 그 뒤 실용적인 말뭉치를 다뤄보겠습니다.

- 우선 python을 이용해 매우 작은 텍스트 데이터에 전처리를 하겠습니다.

  - 여기서 전처리란 데이터를 단어로 분할하고 그 분할된 단어들을 ID 목록으로 변환하는 것을 의미합니다.

```python
# 이번 절에서 사용할 말뭉치는 문장 하나로 이루어진 텍스트입니다.
text = 'You say goodbye and I say hello.'

text = text.lower()  # 소문자로 만듭니다.
text = text.replace('.', ' .')  # 문장 부호에 띄어쓰기를 넣습니다.
print(text)

words = text.split(' ')  # 띄어쓰기를 기준으로 나눕니다.
print(words)
```

#### Warning

> 여기서 단어를 분할할 때 문장부호 앞에 공백을 넣었지만 더 현명하고 범용적인 방법으로 '정규표현식'이 있습니다.
>
> 정규표현식 라이브러리 re를 import하고 `re.split('(\W+)?'.text)`를 호출하면 단어 단위로 쉽게 분할할 수 있습니다.
>
> 더 자세한 내용은 정규표현식 관련 도서 등을 참고하세요.

- 이제 원래 문장을 단어 목록으로 이용할 수 있으나 단어를 텍스트 그대로 조작하기란 여러 면에서 불편합니다.

- 따라서 단어에 ID를 부여하고 ID 리스트로 이용할 수 있도록 손질할 것이며 그 사전 준비로 python의 dictionary를 이용해 단어 ID와 단어를 짝지어 주겠습니다.

```python
word_to_id, id_to_word = {}, {}
for word in words:
    if word not in word_to_id:
        new_id = len(word_to_id)
        word_to_id[word], id_to_word[new_id] = new_id, word
```

- 여기에서 단어 ID에서 단어로 변환은 id_to_word가 담당하며 단어에서 단어 ID로 변환은 word_to_id가 담당합니다.

  - 이를 통해 단어 ID와 단어 대응표가 만들어졌습니다.

- 마지막으로 '단어 목록'을 '단어 ID 목록'으로 바꾸어 보겠습니다.

```python
import numpy as np

corpus = np.array([word_to_id[word] for word in words])
print(corpus)
```

#### Note

> 내포(comprehension)은 반복문 처리를 간단히 쓰기 위한 기법으로 위의 예시처럼 사용됩니다.

- 이것으로 말뭉치를 사용하기 위한 사전 준비는 끝났습니다. 이 처리를 한 데 모아 함수로 구현하면 다음과 같습니다.

> 자세한 내용은 chapter02/commons/util.py의 preprocess() 함수를 확인하세요.

- 여기서 준비한 corpus, word_to_id, id_to_word는 앞으로 이 책 곳곳에서 등장하며 각각 단어 ID 목록, 단어에서 단어 ID 변환, 단어 ID에서 단어로 변환을 의미합니다.

- 이상으로 말뭉치를 다룰 준비를 마쳤으니 이제 이를 사용해 '단어의 의미'를 추출하겠습니다. 그 한 방법으로 이번 절에서는 '통계 기반 기법'을 살펴볼 것입니다.

  - 이 기법은 단어를 벡터로 표현하여 다루는 방법입니다.
