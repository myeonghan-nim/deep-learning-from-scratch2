# chapter05. RNN

- 지금까지 살펴본 신경망은 **피드포워드** 유형의 신경망으로 단방향을 가진 신경망입니다.

  - 다시 말해, 입력 신호가 다음 층으로 전달되고 그 신호를 받은 층이 다음 층으로 전달하는 방식으로 한 방향으로만 전달됩니다.

- 피드포워드 신경망은 구성이 단순해 구조를 이해하기 쉽고 많은 문제에 응용할 수 있습니다. 하지만 시계열 데이터를 잘 다루지 못한다는 단점이 있습니다.

  - 더 정확하게, 단순한 피드포워드 신경망에서는 시계열 데이터의 성질(패턴)을 잘 학습할 수 없습니다. 그래서 등장한 것이 **순환 신경망(RNN)**입니다.

- 이번 장에서는 피드포워드 신경망의 문제점을 지적하고, RNN이 그 문제를 훌륭하게 해결할 수 있음을 설명합니다.

- 또한, RNN의 구조를 차분히 시간을 들여 설명하고 Python으로 구현해볼 것입니다.

## 5.1 확률과 언어 모델

- 이번 절에서는 RNN 이야기를 시작하기 전 준비 과정으로 앞 장의 word2vec을 복습하겠습니다.

- 그런 다음 자연어에 관한 현상을 '확률'을 사용해 기술하고 마지막에 언어를 확률로 다루는 '언어 모델'에 대해 설명합니다.

### 5.1.1 word2vec을 확률 관점에서 바라보다.

- 그럼 word2vec의 CBOW 모델부터 복습하겠습니다. 여기에서는 w<sub>T</sub>라는 단어열로 표현되는 말뭉치에서 t번째 단어를 '타깃'으로 그 전후 단어를 '맥락'으로 취급하겠습니다.

#### Warning

> 이 책에서 타깃은 '중앙 단어'를, 맥락은 타깃의 '주변 단어'를 가리킵니다.

- 이 때 CBOW 모델은 타깃 단어 전후 맥락으로부터 타깃을 추측하는 일을 수행합니다.

<img src="README.assets/fig 5-1.png" alt="fig 5-1" style="zoom:50%;" />

- 그럼 맥락이 주어졌을 때 타깃이 w<sub>T</sub>일 확률을 수식으로 구현하면 다음과 같습니다.

<img src="README.assets/e 5-1.png" alt="e 5-1" style="zoom:50%;" />

- CBOW 모델은 위 식의 사후 확률을 모델링하는데 여기서 사후 확률은 '맥락이 주어졌을 때 타깃 단어를 찾을 확률'을 의미합니다.

  - 이것이 윈도우 크기가 1일 때의 CBOW 모델입니다.

- 그런데 지금까지 맥락을 항상 좌우 대칭으로 생각했습니다. 이번에는 맥락을 왼쪽 윈도우로만 한정하는 다음과 같은 경우를 생각해보겠습니다.

<img src="README.assets/fig 5-2.png" alt="fig 5-2" style="zoom:50%;" />

- 이와 같이 왼쪽 두 단어만을 맥락으로 생각했을 때 CBOW 모델이 출력할 확률은 다음과 같습니다.

<img src="README.assets/e 5-2.png" alt="e 5-2" style="zoom:50%;" />

#### Note

> word2vec에서 맥락의 윈도우 크기는 하이퍼파라미터로 임의의 값을 설정할 수 있습니다.
>
> 이번 예에서는 윈도우 크기를 '왼쪽 2, 오른쪽 0'처럼 좌우 비대칭으로 설정했습니다.
>
> 이렇게 설정한 이유는 나중에 '언어 모델'에서 이야기하겠습니다.

- 그런데 위 식 표기를 사용하면 CBOW 모델이 다루는 손실함수를 교차 엔트로피 오차에 의해 다음과 같이 유도할 수 있습니다.

<img src="README.assets/e 5-3.png" alt="e 5-3" style="zoom:50%;" />

- CBOW 모델의 학습으로 수행하는 일은 위 식의 손실 함수(정확히는 말뭉치 전체의 손실 함수의 총합)를 최소화하는 가중치 매개변수를 찾는 것입니다.

  - 이러한 가중치 매개변수가 발견되면 CBOW 모델을 맥락에서 타깃을 더 정확하게 추측할 수 있습니다.

- 이처럼 CBOW 모델을 학습시키는 목적은 맥락에서 타깃을 정확하게 추측하는 것으로 학습 부산물로 단어의 의미가 인코딩된 '단어의 분산표현'을 얻을 수 있습니다.

- 한편 CBOW 모델의 목적인 '맥락에서 타깃을 추측하는 것'은 어디에 이용될 수 있는가와 위 식의 확률의 실용적인 쓰임이 바로 '언어 모델'에서 나타나게 됩니다.

### 5.1.2 언어 모델

- **언어 모델**은 단어 나열에 확률을 부여합니다.

  - 특정 단어의 시퀀스에 대해서, 그 시퀀스가 일어날 가능성이 어느 정도인지 즉, 얼마나 자연스러운 단어 순서인지를 확률로 평가합니다.

  - 예를 들어 'you say goodbye'라는 단어 시퀀스는 높은 확률을, 'you say good die'는 낮은 확률을 출력하는 모델입니다.

- 이 언어 모델은 다양하게 응용되는데 기계 번역과 음성 인식이 대표적인 예입니다.

  - 음성 인식의 경우 사람의 음성에서 몇 개의 문장을 후보로 생성하고 언어 모델을 사용해 후보 문장이 '문장으로서 자연스러운지'를 기준으로 순서를 매깁니다.

  - 또한 언어 모델이 단어 순서의 자연스러움을 확률적으로 평가할 수 있으므로 새로운 문장을 생성하는 용도로도 이용할 수 있습니다.

    - 그 확률 분포에 따라 다음으로 적합한 단어를 '자아낼(샘플링)' 수 있기 때문으로 이는 '7장 RNN을 사용한 문장 생성'에서 배울 것입니다.

- 그러면 언어 모델을 수식으로 설명해보겠습니다. 여기서는 w<sub>m</sub>까지 m개의 단어로 된 문장을 생각해보겠습니다.

- 이 때 단어가 w<sub>1</sub>에서 w<sub>m</sub> 순서로 나타날 확률을 P(w<sub>1</sub>, ..., w<sub>m</sub>)이라고 할 수 있고 이는 동시에 일어나므로 동시 확률입니다. 이 동시 확률 P는 사후 확률을 사용해 다음과 같이 분해할 수 있습니다.

<img src="README.assets/e 5-4.png" alt="e 5-4" style="zoom:50%;" />

- 위 식에서 파이 기호는 모든 원소를 곱하는 '총곱'을 의미하며 동시 확률을 사후 확률의 총곱으로 나타낼 수 있습니다.

- 위 식의 경과는 확률의 **곱셈정리**에서 유도한 것으로 곱셈정리에 대한 간단한 설명 후에 이를 도출하는 과정을 설명하겠습니다. 우선 곱셈 정리는 다음과 같습니다.

<img src="README.assets/e 5-5.png" alt="e 5-5" style="zoom:50%;" />

- 위와 같은 곱셈정리는 확률론에서 가장 중요한 정리로 'A와 B가 모두 일어날 확률 P(A, B)는 B가 일어날 확률 P(B)에 B가 일어난 후 A가 일어날 확률 P(A|B)를 곱한 것과 같다.'는 것입니다.

#### Warning

> 확률 P(A, B)는 P(A, B) = P(B|A)P(A)로 분해할 수 있습니다.
>
> 즉, A와 B 중 어느 것을 사후 확률의 조건으로 할지에 따라 두 가지 식 표현 방법이 존재합니다.

- 이 곱셈정리를 사용하면 m개의 단어의 동시 확률을 사후 확률로 나타낼 수 있는데 이 때 수행하는 식 변형을 알기 쉽게 나타내면 다음과 같습니다.

<img src="README.assets/e 5-6.png" alt="e 5-6" style="zoom:50%;" />

- 여기에서 w<sub>m</sub>을 제외한 나머지를 A로 표현했는데 이 때 P(A) 역시 다음과 같은 식 변형을 할 수 있습니다.

<img src="README.assets/e 5-7.png" alt="e 5-7" style="zoom:50%;" />

- 이처럼 단어 시퀀스를 하나씩 줄여가면서 매번 사후 확률로 분해하고 이를 반복해 [식 5.4]를 이끌어낼 수 있습니다.

- 이 때 [식 5.4]에서 알 수 있듯이 목적으로 하는 동시 확률은 사후 확률의 총곱으로 대표할 수 있고 여기서 주목할 것은 이 사후 확률은 타깃 단어보다 왼쪽에 있는 모든 단어를 맥락(조건)으로 했을 때의 확률이라는 것입니다.

<img src="README.assets/fig 5-3.png" alt="fig 5-3" style="zoom:50%;" />

- 즉, 이를 정리하면 우리의 목표인 사후 확률을 구하는 것으로 이를 계산할 수 있다면 언어 모델의 동시 확률을 구할 수 있습니다.

#### Note

> P(w<sub>t</sub>|others)를 나타내는 모델은 **조건부 언어 모델**이라고 합니다.
>
> 한편 해당 확률을 나타내는 모델을 '언어 모델'이라 하는 경우도 많이 볼 수 있습니다.

### 5.1.3 CBOW 모델을 언어 모델로?

- 그렇다면 word2vec의 CBOW 모델을 언어 모델에 적용하는 방법은 무엇이 있을까요?

  - 이는 맥락의 크기를 특정 값으로 한정하여 근사적으로 나타낼 수 있는데 수식으로는 다음과 같습니다.

  <img src="README.assets/e 5-8.png" alt="e 5-8" style="zoom:50%;" />

  - 여기에서는 맥락을 왼쪽 두 개의 단어로 한정한 것으로 CBOW 모델에 따라(정확히는 모델의 사후 확률) 근사적으로 나타낼 수 있습니다.

#### Note

> 머신러닝이나 통계학에서는 **마르코프 연쇄** 또는 **마르코프 모델**이라는 말을 자주 듣습니다.
>
> 마르코프 연쇄란 미래의 상태가 현재 상태에만 의존해 결정되는 것을 의미합니다.
>
> 또한, 이 사상의 확률을 '그 직전' N개의 사건에만 의존할 때 이를 'N층 마르코프 연쇄'라고 합니다.
>
> 이번 예는 직전 2개의 단어에만 의존해 다음 단어가 정해지는 모델이므로 '2층 마르코프 연쇄라고 할 수 있습니다.

- 위 식에서는 맥락으로 두 개의 단어를 이용했지만 맥락의 크기는 임의의 길이로 설정할 수 있습니다.

- 하지만 결국 특정 길이로 '고정'되는데 예를 들어 왼쪽 10개의 단어를 맥락으로 하는 CBOW 모델은 그 맥락보다 더 왼쪽의 단어 정보를 무시합니다.

  - 이러한 경우 문제가 발생하는데 그 예가 다음과 같습니다.

<img src="README.assets/fig 5-4.png" alt="fig 5-4" style="zoom:50%;" />

- 위 예시의 문제의 맥락을 고려하면 정답은 'Tom(he)'이지만 이를 알기 위해 타깃보다 18번째 앞의 단어 'Tom'을 기억해야 합니다.

  - 따라서 10개의 맥락을 가지는 CBOW 모델은 이를 제대로 답할 수 없습니다.

- 그렇다고 CBOW 모델의 맥락 크기를 계속해서 키워나가도 CBOW 모델이 맥락 안의 단어 순서를 무시한다는 한계가 있습니다.

#### Note

> CBOW는 continuous bag-of-words의 약자로 bag-of-words는 '가방 안의 단어'를 의미하는데 이 때 단어의 '순서'는 무시된다는 뜻이 내포되어 있습니다.

- 맥락의 단어 순서가 무시되는 구체적인 예로 맥락으로 2개의 단어를 다루는 CBOW 모델이 두 개의 단어 벡터의 '합'이 은닉층으로 넘어오는 과정은 다음과 같습니다.

<img src="README.assets/fig 5-5.png" alt="fig 5-5" style="zoom:50%;" />

- 위 그림의 왼쪽과 같이 CBOW 모델의 은닉층에서는 단어 벡터들이 더해지므로 맥락의 단어 순서는 무시됩니다.

- 이상적으로는 맥락의 단어 순서도 고려한 모델이 바람직한데 이는 위 그림의 오른쪽 모델과 같습니다.

  - 오른쪽 모델은 단어 벡터를 은닉층에서 **연결**하는 방식을 사용하는데 실제 신경 확률론적 언어 모델에서 제안한 모델은 이 방식을 취합니다.

  - 그러나 연결하는 방식은 맥락의 크기에 비례해 가중치 변수가 늘어나는 단점을 불러옵니다.

- 따라서 이러한 문제를 해결하기 위해 순환 신경망, 즉 RNN이 등장했습니다.

- RNN은 맥락이 아무리 길더라도 그 맥락의 정보를 기억하는 매커니즘을 갖추고 있습니다. 그래서 RNN을 사용하면 아무리 긴 시계열 데이터에도 대응할 수 있습니다.

#### Warning

> word2vec은 단어의 분산 표현을 얻을 목적으로 고안된 기법으로 이를 언어 모델로 사용하는 경우는 보통 잘 없습니다.
>
> 여기에서는 RNN의 매력을 알아가기 위해 word2vec의 CBOW 모델을 억지고 언어 모델에 적용했을 뿐입니다.
>
> 특히 word2vec의 RNN보다 늦게 제안되었는데 RNN 언어 모델이 단어의 분산 표현을 얻을 수 있지만 어휘 수 증가에 따른 대응이나 단어의 분산 표현의 '질' 개선을 위해 word2vec을 제안되었습니다.

## 5.2 RNN이란

- RNN은 'Recurrent'라는 라틴어에서 온 말로, '몇 번이나 반복해서 일어나는 일'을 의미합니다. 그래서 RNN을 직역하면 '순환하는 신경망'으로 이번 절에서는 '순환한다'의 의미를 짚어보겠습니다.

#### Warning

> Recurrent Neural Network는 우리말로 '순환 싱경망'이라고 번역합니다.
>
> 반면 Recursive Neural Network라는 '재귀 신경망'도 있는데 이는 주로 트리 구조로 데이터를 처리하기 위한 신경망입니다.

### 5.2.1 순환하는 신경망

- '순환하다'의 의미는 '반복해서 되돌아감'을 의미합니다. 즉, 어느 한 지점에서 시작한 것이 시간을 지나 원래 장소로 돌아오는 것과 이를 반복하는 것이 바로 '순환'입니다.

- 여기서 주목할 점은 순환하기 위해서는 '닫힌 경로'가 필요하다는 점입니다.

  - '닫힌 경로' 혹은 '순환하는 경로'가 존재해야 데이터가 같은 장소를 반복해 왕래할 수 있고 그 결과 정보가 끊임없이 갱신되게 됩니다.

#### Note

> 비유하자면 우리의 체내를 순환하는 혈액은 그 생명을 얻은 순간부터 계속해 흘러 체내를 순환하며 과거에서 현재까지 끊임없이 '갱신'됩니다.

- RNN의 특징은 순환 경로(닫힌 경로)가 있다는 것으로 이 경로를 따라 데이터가 끊임없이 순환합니다.

  - 그리고 데이터가 순환되기에 과거의 정보를 기억하는 동시에 최신 데이터로 갱신될 수 있는 것입니다.'

- 그럼 RNN을 구체적으로 살펴보겠습니다. RNN에 이용되는 계층을 'RNN 계층'이라고 할 때 그는 다음과 같습니다.

<img src="README.assets/fig 5-6.png" alt="fig 5-6" style="zoom:50%;" />

- 이처럼 RNN 계층은 순환하는 경로를 포함하는데 이 순환 경로에 따라 데이터를 계층 안에서 순환시킬 수 있습니다.

  - RNN 계층은 x<sub>t</sub>을 입력받는데 이는 시간 t에 대한 데이터로 이들을 시계열 데이터라고 합니다.

  - 그리고 이에 대응해서 h<sub>t</sub>가 다음 층으로 출력됩니다

- 또한 각 시각에 입력되는 x<sub>t</sub>가 벡터라고 할 때 문장을 다루는 경우 각 단어의 분산 표현이 입력이 되고 이들이 순서대로 하나씩 RNN 계층에 입력됩니다.

#### Warning

> 위 그림에 따르면 출력이 2개로 분기하는데 이 분기되는 데이터들은 같은 데이터가 복제되어 '분기'함을 의미합니다.
>
> 그리고 그 중 하나가 RNN 계층에 다시 입력됩니다.

- 이어서 순환 구조를 자세히 살펴보기 전에 RNN 계층을 다음과 같이 그리겠습니다.

<img src="README.assets/fig 5-7.png" alt="fig 5-7" style="zoom:50%;" />

- 이처럼 지금까지 데이터가 왼쪽에서 오른쪽으로 흐르는 형태의 계층과 달리 아래에서 위로 흐르는 계층으로 그리겠습니다.

### 5.2.2 순환 구조 펼치기

- 그럼 RNN 계층의 순환 구조에 대해 자세하게 살펴보겠습니다.

- RNN의 순환 구조는 지금까지 신경망에 없었던 구조지만 이 구조를 펼치면 친숙한 신경망으로 '변신'합니다.

<img src="README.assets/fig 5-8.png" alt="fig 5-8" style="zoom:50%;" />

- 위 그림과 같이 RNN 계층의 순환 구조를 펼치면 오른쪽으로 성장하는 긴 신경망을 볼 수 있습니다. 그리고 이는 지금까지 본 피드포워드 신경망과 같은 구조입니다.

  - 다만, 등장하는 RNN 계층 모두가 사실 '같은 계층'이라는 것만 지금까지 신경망과 다릅니다.

#### Warning

> 시계얼 데이터는 시간 방향으로 데이터가 나열됩니다.
>
> 따라서 시계열 데이터의 인덱스를 가리킬 때는 '시각'이라는 용어를 사용합니다.
>
> 이는 자연어에서도 't번째 단어 혹은 RNN 계층'말고도 '시각 t의 단어 혹은 RNN 계층'이라고 표현합니다.

- 이처럼 각 시각의 RNN 계층은 그 계층의 입력과 1개 전의 RNN 계층의 출력을 받고 두 정보를 바탕으로 현 시각의 출력을 계산합니다. 그 수식은 다음과 같습니다.

<img src="README.assets/e 5-9.png" alt="e 5-9" style="zoom:50%;" />

- 위 수식을 보면 RNN에 사용되는 가중치는 두 개인데 하나는 입력 x를 출력 h로 바꾸는 가중치 W<sub>x</sub>이고 다른 하나는 1개 전의 출력을 다음 시각의 출력으로 변환하기 위한 W<sub>h</sub>입니다.

  - 또한 편향 b도 존재하며 h<sub>t-1</sub>과 x<sub>t</sub>는 행벡터입니다.

- 위 수식은 행렬곱을 계산하고 그 합을 ranh 함수를 이용해 변환하여 시각 t의 출력 h<sub>t</sub>을 계산합니다. 그리고 이 출력은 다음 계층과 다음 시각의 RNN 계층을 향해 출력됩니다.

- 그런데 위 수식을 보면 현재의 출력은 한 시각 이전의 출력에 기초해 계산됨을 알 수 있는데 다른 관점으로 이를 보면, RNN이 h라는 '상태'를 가지고 있고 이를 위 수식을 사용해 갱신한다고 할 수 있습니다.

  - 그래서 RNN 계층을 '상태를 가지는 계층' 혹은 '메모리가 있는 계층'이라고 합니다.

#### Note

> RNN의 h는 '상태'를 기억해 시각이 1 단위 진행될 때마다 위 식에 의해 갱신됩니다.
>
> 많은 문헌에서 RNN의 출력 h<sub>t</sub>을 **은닉 (벡터) 상태**라고 부르며 이 책에서도 동일하게 부릅니다.

- 또 펼처진 RNN 계층을 많은 문헌에서 다음과 같이 그리기도 합니다.

<img src="README.assets/fig 5-9.png" alt="fig 5-9" style="zoom:50%;" />

- 왼쪽 그림은 RNN 계층에서 나가는 두 화살표가 같은 데이터가 복사되어 분기됨을 확인하기 어렵습니다.

- 따라서 이 책에서는 지금처럼 오른쪽 그림과 같이 하나의 출력이 분기하는 그림을 사용해 명시하겠습니다.

### 5.2.3 BPTT

- 앞에서 봤듯이 RNN 계층은 가로로 펼친 신경망으로 간주할 수 있으며 학습도 보통의 신경망과 같은 순서로 진행할 수 있습니다.

<img src="README.assets/fig 5-10.png" alt="fig 5-10" style="zoom:50%;" />

- 위 그림처럼 순환 구조를 펼친 후의 RNN에는 일반적인 오차역전파법을 적용할 수 있습니다.

  - 즉, 먼저 순전파를 수행하고, 이어서 역전파를 수행해 원하는 기울기를 구할 수 있습니다.

  - 여기서 오차역전파법은 '시간 방향으로 펼쳐진 신경망의 오차역전파법'이라는 뜻으로 **BPTT**라고 합니다.

- 이 BRTT를 사용하면 RNN을 학습할 수 있을 듯 보이지만 해결해야할 문제가 하나 있는데 그것은 바로 긴 시계열 데이터를 학습할 때 발생합니다.

  - 이는 시계열 데이터의 시간 크기가 커지는 것에 비례해 BPTT가 소비하는 컴퓨팅 자원도 증가하기 때문입니다.

  - 또한, 시간 크기가 커지면 역전파의 기울기가 불안정해지는 것도 문제입니다.

#### Note

> BPTT를 이용해 기울기를 구하려면 매 시각 RNN 계층의 중간 데이터를 메모리에 유지해야 합니다.
>
> 따라서 시계열 데이터가 길어짐에 따라 계산량과 메모리 사용량이 증가합니다.

### 5.2.4 Truncated BPTT

- 큰 시계열 데이터를 취급할 때 흔히 신경망 연결을 적당한 길이로 '끊습니다'.

  - 시간축 방향으로 너무 길어진 신경망을 적당한 지점에서 잘라 작은 신경망 여러 개로 만드는 아이디어로 이 작은 신경망들에 대해 오차역전파법을 수행합니다.

  - 이것이 바로 **Truncated BPTT** 기법입니다.

#### Note

> Truncated는 '잘린'이라는 뜻이므로 Truncated BPTT는 적당한 길이로 '잘라낸' 오차역전파법입니다.

- Truncated BPTT는 신경망의 연결을 끊지만 제대로 구현하기 위해서는 '역전파'의 연결만 끊고 순전파는 유지해야 순전파의 흐름이 끊어지지 않고 전파됩니다.

  - 한편, 역전파의 연결은 적당한 길이오 잘라내 그 잘라낸 신경망 단위로 학습을 수행합니다.

- 구체적인 예와 함께 살펴보자면 길이가 1000개인 시계열 데이터(자연어 처리라면 단어 1000개의 말뭉치)가 있다고 가정하겠습니다.

  - 지금껏 다룬 PTB 데이터셋도 여러 문장을 연결한 것을 하나의 큰 시계열 데이터로 취급했는데 여기에서도 마찬가지로 데이터를 취급하겠습니다.

- 그런데 길이가 1000인 시계열 데이터를 다루는 RNN 계층을 펼치면 가로로 1000개나 늘어선 신경망이 됩니다.

  - 아무리 계층이 늘어서더라도 오차역전파법으로 기울기를 계산할 수는 있지만 너무 길면 계산량과 메모리 사용량 등에서 문제가 발생합니다.

  - 또한, 길어진 계층에 의해 신경망을 하나 통과할 때마다 기울기 값이 조금씩 작아져 이전 시각 t까지 역전파되기 전에 0이 되어 소멸할 수 있습니다.

- 바로 이런 이유로 길게 뻗은 신경망의 역전파는 연결을 적당한 길이로 끊을 생각을 한 것입니다.

<img src="README.assets/fig 5-11.png" alt="fig 5-11" style="zoom:50%;" />

- 위 예시에서는 RNN 계층을 10개 단위로 학습할 수 있도록 역전파의 연결을 끊었습니다.

- 이처럼 역전파의 연결을 잘라버리면 그보다 미래의 데이터에 대해서는 생각할 필요가 없어집니다.

  - 따라서, 각 블록 단위로 미래의 블록과 독립적으로 오차역전파법을 완결시킬 수 있습니다.

- 여기서 기억할 점은 역전파의 연결은 끊어지지만 순전파의 연결은 그렇지 않다는 점입니다.

  - 그러므로 RNN을 학습시킬때는 순전파가 연결된다는 점을 고려해야 합니다.

  - 즉, 데이터를 '순서대로' 입력해야 한다는 뜻인데 이에 대해서는 이어서 자세히 설명하겠습니다.

#### Warning

> 지금까지 본 신경망은 미니배치 학습을 수행할 때 데이터를 무작위로 선택해 입력했습니다.
>
> 하지만 RNN에서 Truncated BPTT를 수행할 때는 데이터를 '순서대로' 입력해야 합니다.

- 이제 Truncated BPTT 방식으로 RNN을 학습시켜 보겠습니다.

  - 가장 먼저 할 일은 첫 번째 블록 입력 데이터를 RNN 계층에 제공하는 것입니다.

  <img src="README.assets/fig 5-12.png" alt="fig 5-12" style="zoom:50%;" />

  - 이처럼 순전파를 먼저 수행하고 다음에 역전파를 수행해 원하는 기울기를 구할 수 있습니다. 이어서 다음 블록의 입력 데이터를 입력해 오차역전파법을 수행합니다.

  <img src="README.assets/fig 5-13.png" alt="fig 5-13" style="zoom:50%;" />

  - 여기도 첫 번째 블록과 마찬가지로 순전파를 수행하고 역전파를 수행합니다. 그리고 여기서 중요한 점은 이번 순전파 계산에는 앞 블록의 마지막 은닉 상태의 정보가 필요하다는 점으로 이를 통해 순전파는 계속 연결됩니다.

  - 같은 방식으로 세 번째 블록을 대상으로 학습을 수행하는데 이 때도 두 번째 블록의 마지막 은닉 상태 정보를 이용합니다.

  - 이처럼 RNN 학습은 데이터를 순서대로 입력하며 은식 상태를 계승하면서 학습을 수행합니다. 이들을 총망라하면 다음과 같습니다.

  <img src="README.assets/fig 5-14.png" alt="fig 5-14" style="zoom:50%;" />

- 이와 같이 Truncated BPTT에서 데이터를 순서대로 입력해 학습할 수 있고 순전파의 연결을 유지하면서 블록 단위로 오차역전파법을 적용할 수 있습니다.

### 5.2.5 Truncated BPTT의 미니배치 학습

- 지금까지 Truncated BPTT에서는 미니배치 학습 시 각 미니배치가 어떤 식으로 이루어지는지에 대해 생각하지 않았습니다.

  - 굳이 말해 지금까지 이야기는 미니배치가 1일 때에 해당되는데 이제 미니배치 학습을 위해 구체적인 배치 방식을 고려해 데이터를 순서대로 입력해야 합니다.

  - 그렇게 하기 위해서는 데이터를 주는 시작 위치를 각 미니배치의 시작 위치로 '옮겨줘야' 합니다.

- '옮긴다'는 의미를 앞서 설명한 예(길이 1000인 시계열 데이터와 10 시각 단위로 자른 Truncated BPTT)로 설명하겠습니다.

  - 만일 미니배치를 두 개로 구성한가면 RNN 계층의 입력 데이터로 첫 번째 미니배치 때는 처음부터 순서대로 데이터를 제공합니다.

  - 그리고 두 번째 미니배치 때는 500번째 데이터를 시작 위치로 정하고 그 위치부터 다시 순서대로 데이터를 제공합니다. 이들은 다음과 같습니다.

  <img src="README.assets/fig 5-15.png" alt="fig 5-15" style="zoom:50%;" />

- 이와 같이 미니배치 데이터를 RNN의 입력 데이터로 사용해 학습을 수행합니다. 이후로는 순서대로 진행되므로 다음에 넘길 데이터는 각 시계열 데이터의 10 시각 다음 데이터가 되는 형식입니다.

- 이처럼 미니배치 학습을 수행할 때 각 미니배치의 시작 위치를 오프셋으로 옮겨준 후 순서대로 제공하면 됩니다. 또한, 데이터를 순서대로 입력하다가 끝에 도달하면 다시 처음부터 입력하도록 합니다.

- 지금까지 살펴본 Truncated BPTT의 원리는 단순하지만 '데이터 제공 방법'면에서는 몇 가지 주의가 필요합니다.

  - 구체적으로는 '데이터 순서대로 제공하기'와 '미니배치별로 데이터 제공하는 시작위치를 옮기기'입니다.

  - 이에 대한 설명이 복잡해 잘 이해되지 않을수도 있지만 이어서 소스 코드와 함께 동작을 지켜보면 어렵지 않게 이해할 수 있을 것입니다.

## 5.3 RNN 구현

- 앞 절까지 RNN의 전체 모습을 볼 수 있었습니다. 이제 지금부터 구현해야 할 것은 가로 방향으로 성장한 신경망입니다.

  - 특히 Truncated BPTT 방식의 학습을 따른다면 가로 크기가 일정한 일련의 신경망을 만들면 됩니다.

  <img src="README.assets/fig 5-16.png" alt="fig 5-16" style="zoom:50%;" />

- 이처럼 우리가 다룰 신경망은 길이가 T인 시계열 데이터를 받고 각 시각의 은닉 상태를 T개 출력합니다. 그리고 모듈화를 생각해 옆으로 성장한 신경망을 '하나의 계층'으로 구현하겠습니다.

<img src="README.assets/fig 5-17.png" alt="fig 5-17" style="zoom:50%;" />

- 이처럼 상하 방향의 입력과 출력을 각각 하나로 묶으면 옆으로 늘어선 계층을 하나의 계층으로 간주할 수 있습니다.

  - 즉, 입력을 묶은 xs를입력하면 은닉값을 묶은 hs를 출력하는 단일 계층으로 볼 수 있습니다.

  - 이때 Time RNN 계층 내에서 한 단계의 작업을 수행하는 계층을 'RNN 계층'이라고 하고 T개 단계분의 작업을 한꺼번에 처리하는 계층을 'Time RNN 계층'이라고 합니다.

#### Note

> Time RNN 같이 시계열 데이터를 한꺼번에 처리하는 계층에는 앞에 'Time'을 붙이는 데 이는 이 책의 독자적인 명명규칙입니다.
>
> 나중에 Time Affine 계층과 Time Embedding 계층도 구현하는데 이것들도 시계열 데이터를 한꺼번에 처리합니다.

- 앞으로 할 구현의 흐름은 다음과 같습니다.

  1. RNN의 한 단계를 처리하는 클래스를 RNN이라는 이름으로 구현합니다.

  2. RNN 클래스를 이용해 T개 단계의 처리를 한꺼번에 수행하는 계층을 TimeRNN이란 이름으로 완성합니다.

### 5.3.1 RNN 계층 구현

- RNN 처리를 한 단계만 수행하는 RNN 클래스를 구현하겠습니다. 이 때 복습하자면 RNN의 순전파는 다음과 같습니다.

<img src="README.assets/e 5-10.png" alt="e 5-10" style="zoom:50%;" />

- 이 클래스는 데이터를 미니배치로 모아 처리합니다. 따라서 x<sub>t</sub>와 h<sub>t</sub>에서는 각 샘플 데이터를 행 방향에 저장합니다.

- 한편, 행렬을 계산할 때는 행렬의 '형상 확인'이 중요한데 미니배치 크기가 N, 입력 벡터의 차원 수가 D, 은닉 상태 벡터의 차원 수가 H라면 지금 계산에 형상 확인은 다음과 같습니다.

<img src="README.assets/fig 5-18.png" alt="fig 5-18" style="zoom:50%;" />

- 위와 같은 과정을 수행하여 올바로 구현되었는지 확인할 수 있습니다. 그럼 이를 바탕으로 RNN 클래스의 초기화와 순전파 메서드를 구현합니다.

```python
# chapter05/commons/time_layers.py
class RNN:
    def __init__(self, Wx, Wh, b):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None

    def forward(self, x, h_prev):
        Wx, Wh, b = self.params
        t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b
        h_next = np.tanh(t)

        self.cache = (x, h_prev, h_next)
        return h_next
```

- RNN의 초기화 메서드는 가중치 2개와 편향 1개를 인수로 받습니다.

  - 받은 인수 매개변수는 인스턴스 변수 params에 리스트로 저장합니다.

  - 각 매개변수에 대응하는 형태로 기울기를 초기화한 후 grads에 저장합니다.

  - 마지막으로 역전파 계산 시 사용하는 중간 데이터를 담을 cache를 None으로 초기화합니다.

- 순전파인 forward 메서드는 인수 2개(아래로부터의 입력 x와 왼쪽으로부터의 입력 h_prev)를 받습니다.

  - 그 다음은 forward 계산식을 그대로 계산하는데 여기서 앞 RNN 계층에서 받는 입력이 h_prev, 현 시각 RNN 계층의 출력이 h_next입니다.

- 다음은 RNN의 역전파를 구현할 차례입니다. 그 전에 RNN의 순전파를 계산 그래프로 확인해보겠습니다.

<img src="README.assets/fig 5-19.png" alt="fig 5-19" style="zoom:50%;" />

- RNN 계층의 순전파는 위와 같은 계산 그래프로 나타내는데 여기에는 행렬 곱인 'MatMul'과 '덧셈', 그리고 'tanh'라는 3개의 연산으로 구성됩니다.

  - 참고로 편향 b의 덧셈에서는 브로드캐스트가 일어나기 때문에 정확하게는 Repeat 노드를 이용하지만 여기에선 간략하게 표기했습니다.

- 그럼 이 그래프의 역전파는 앞에서 배웠던 세 가지 연산에 대한 역전파를 사용합니다. 즉, 다음과 같이 순전파와 반대 방향으로 각 연산자의 역전파를 수행합니다.

<img src="README.assets/fig 5-20.png" alt="fig 5-20" style="zoom:50%;" />

- 위 계산 그래프를 구현한 RNN 계층의 backward 메서드의 코드는 다음과 같습니다.

```python
# class RNN:
    def backward(self, dh_next):
        Wx, Wh, b = self.params
        x, h_prev, h_next = self.cache

        dt = dh_next * (1 - h_next ** 2)
        db = np.sum(dt, axis=0)
        dWh = np.matmul(h_prev, dt)
        dh_prev = np.matmul(dt, Wh.T)
        dWx = np.matmul(x.T, dt)
        dx = np.matmul(dt, Wx.T)

        self.grads[0][...] = dWx
        self.grads[1][...] = dWh
        self.grads[2][...] = db

        return dx, dh_prev
```

- 이상이 RNN 계층의 역전파 구현입니다. 이어서 Time RNN 계층을 구현하겠습니다.

### 5.3.2 Time RNN 계층 구현

- Time RNN 계층은 T개의 RNN 계층으로 구성됩니다. 여기서 T는 임의의 수로 설정할 수 있으며 이 계층은 다음과 같습니다.

<img src="README.assets/fig 5-21.png" alt="fig 5-21" style="zoom:50%;" />

- 이처럼 Time RNN 계층은 RNN 계층 T개를 연결한 것으로 이 신경망을 Time RNN 클래스로 구현할 것입니다.

  - 여기에서는 RNN 계층의 은닉 상태 h를 인스턴스 변수로 유지하고 이 변수를 은닉 상태를 '인계'받는 용도로 이용합니다.

  <img src="README.assets/fig 5-22.png" alt="fig 5-22" style="zoom:50%;" />

- 이처럼 RNN 계층의 은닉 상태를 Time RNN 계층에서 관리하기로 합니다.

  - 이렇게 하면 Time RNN 사용자는 RNN 계층 사이에서 은닉 상태를 '인계하는 작업'을 생각하지 않아도 된다는 장점이 생깁니다.

  - 이 책에서는 이 기능을 stateful이라는 변수로 조정할 수 있도록 구현할 것입니다.

- 그럼 Time RNN 계층의 코드를 구현해보겠습니다. 우선 초기화와 또 다른 메서드 2개를 살펴보겠습니다.

```python
# chapter05/commons/time_layers.py
class TimeRNN:
    def __init__(self, Wx, Wh, b, stateful=False):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.layers = None

        self.h, self.dh = None, None
        self.stateful = stateful

    def set_state(self, h):
        self.h = h

    def reset_state(self):
        self.h = None
```

- 초기화 메서드는 가중치와 편향, 그리고 stateful이라는 boolean 값을 인수로 받습니다.

  - 인스턴스 변수 중 layers는 다수의 RNN 계층을 리스트로 저장하는 데 사용됩니다.

  - 인스턴스 변수 h는 forward 메서드를 불렀을 때 마지막 RNN 계층의 은닉 상태를 저장하고, dh는 backward를 불렀을 때 하나 앞 블록의 은닉 상태의 기울기를 저장합니다.

#### Warning

> TimeRNN 클래스는 확장성을 고려해 Time RNN 계층의 은닉 상태를 설정하거나 초기화하는 메서들를 구현했습니다.

- 앞의 인수 중 stateful은 '상태가 있는'이란 뜻의 단어로 이 책에서는 stateful이 True라면 Time RNN 계층은 '상태가 있다'고 합니다.

  - 여기서 '상태가 있다'란, Time RNN 계층이 은닉 상태를 유지한다는 뜻입니다. 즉, 아무리 긴 시계열 데이터라도 Time RNN 계층의 순전파를 끊지 않고 전파합니다.

- 한편 stateful이 False일 때 Time RNN 계층은 은닉 상태를 '영행렬'로 초기화합니다. 이것이 상태가 없는 모드로 '무상태'라고 합니다.

#### Note

> 긴 시계열 데이터를 처리할 때는 RNN의 은닉 상태를 유지해야 합니다.
>
> 이처럼 은닉 상태를 유지하는 기능을 흔히 'stateful'이라는 단어로 표현합니다.
>
> 많은 딥러닝 프레임워크에서 RNN 계층의 인수로 stateful을 받으며 이를 통해 이전 시각의 은닉 상태를 유지할지 지정할 수 있습니다.

- 그럼 계속해서 순전파를 구현하겠습니다.

```python
# class TimeRNN:
    def forward(self, xs):
        Wx, Wh, b = self.params
        N, T, D = xs.shape
        D, H = Wx.shape

        self.layers = []
        hs = np.empty((N, T, H), dtype='f')
        if not self.stateful or self.h is None:
            self.h = np.zeros((N, H), dtype='f')

        for t in range(T):
            layer = RNN(*self.params)
            self.h = layer.foward(xs[:, t, :], self.h)
            hs[:, t, :] = self.h
            self.layers.append(layer)

        return hs
```

- 순전파 메서드인 forward(xs)는 아래로부터 xs를 받습니다.

  - xs는 T개 분량의 시계열 데이터를 하나로 모은 것으로 미니배치 크기를 N, 입력 벡터의 차원 수를 D라고 하면 xs의 형상은 (N, T, D)가 됩니다.

- RNN 계층의 은닉 상태 h는 처음 호출 시(self.h가 None일 때)에는 원소가 모두 0인 영행렬로 초기화합니다. 이는 인스턴스 변수 stateful이 False일 때도 항상 영행렬로 초기화합니다.

  - 이어서 `hs = np.empty((N, T, H), dtype='f')`에서 출력값을 담은 hs를 준비합니다.

  - 그 다음 총 T회 반복되는 for 문 안에서 RNN 계층을 생성하여 인스턴스 변수 layers에 추가합니다.

    - 그 사이에 RNN 계층이 각 시각 t의 은닉 상태 h를 계산하고, 이를 hs에 해당 인덱스의 값으로 설정합니다.

#### Note

> Time RNN 계층의 forward 메서드가 불리면 인스턴스 변수 h에는 마지막 RNN 계층의 은닉 상태가 저장됩니다.
>
> 그래서 다음번 forward 메서드 호출 시 stateful이 True면 먼저 저장된 h 값이 그대로 이용되고 False면 h가 다시 영행렬로 초기화됩니다.

- 이어서 Time RNN 계층의 역전파를 구현하겠습니다. 이 계산 그래프는 다음과 같습니다.

<img src="README.assets/fig 5-23.png" alt="fig 5-23" style="zoom:50%;" />

- 이와 같이 여기에서는 상츄에서 전해지는 기울기를 dhs로 쓰고 하류로 내보내는 기울기를 dxs로 사용합니다.

  - 여기에서 Truncated BPTT를 수행하기 때문에 이 블록의 이전 시각 역전파는 필요하지 않습니다.

  - 단, 이전 시각의 은닉 상태 기울기는 인스턴스 변수 dh에 저장하겠으며 이는 7장의 seq2seq에서 더 활용하도록 하겠습니다.

- 이상이 Time RNN 계층에서 이뤄지는 역전파의 전체 그림으로 이 때 t번째 RNN 계층에 주목하면 그 역전파는 다음과 같습니다.

<img src="README.assets/fig 5-24.png" alt="fig 5-24" style="zoom:50%;" />

- t번째 RNN 계층에서는 위로부터 기울기 dh<sub>t</sub>와 '한 시각 뒤(미래) 계층'으로부터의 기울기 dh<sub>next</sub>가 전해집니다.

  - 여기에서 주의할 점은 RNN 계층의 순전파에서는 출력이 2개로 분기된다는 것입니다.

  - 이 때 순전파에서 분기한 경우 그 역전파는 기울기가 합산되어 전해져 역전파 시 RNN 계층에는 합산된 기울기가 입력됩니다.

- 이상을 주의하며 역전파를 구현하면 다음과 같습니다.

```python
# class TimeRNN:
    def backward(self, dhs):
        Wx, Wh, b = self.params
        N, T, H = dhs.shape
        D, H = Wx.shape

        dxs = np.empty((N, T, D), dtype='f')
        dh, grads = 0, [0, 0, 0]
        for t in reversed(range(T)):
            layer = self.layers[t]
            dx, dh = layer.backward(dhs[:, t, :] + dh)  # 기울기를 합산합니다.
            dxs[:, t, :] = dx
            for i, grad in enumerate(layer.grads):
                grads[i] += grad

        for i, grad in enumerate(grads):
            self.grads[i][...] = grad
        self.dh = dh

        return dxs
```

- 여기에서도 가장 먼저 하류로 흘려보낼 기울기를 담을 그릇인 dxs를 만듭니다.

  - 그리고 순전파 때와 반대 순서로 RNN 계층의 backward 메서드를 호출하여 각 시각의 기울기 dx를 구해 dxs의 해당 인덱스(시각)에 저장합니다.

  - 그리고 가중치 매개변수에 대해서도 각 RNN 계층의 가중치 기울기를 합산하여 최종 결과를 멤버 변수 self.grads에 덮어씁니다.

#### Warning

> Time RNN 계층 안에는 RNN 계층이 여러 개 있습니다.
>
> 그리고 그 RNN 계층들에서 똑같은 가중치를 사용하고 있기에 Time RNN 계층의 최종 가중치의 기울기는 RNN 계층의 가중치 기울기들을 모두 더한 게 됩니다.

- 이상으로 Time RNN 계층의 구현을 살펴봤습니다.

## 5.4 시계열 데이터 처리 계층 구현

- 이번 장의 목표는 RNN을 사용하여 '언어 모델'을 구현하는 것입니다.

- 지금까지 RNN 계층과 시계열 데이터를 한 번에 처리하는 Time RNN 계층을 구현했는데 이번 절에서는 시계열 데이터를 처리하는 층을 몇 개 더 만들 예정입니다,

  - 여기서, RNN을 사용한 언어 모델은 RNN Language Model로 부르므로 이를 줄여 RNNLM이라고 부르겠습니다.

### 5.4.1 RNNLM의 전체 그림

- RNNLM에서 사용되는 신경망을 한 번 알아보겠습니다. 다음 그림은 가장 단순한 RNNLM으로 왼쪽이 계층 구성, 오른쪽이 시간축으로 펼친 신경망입니다.

<img src="README.assets/fig 5-25.png" alt="fig 5-25" style="zoom:50%;" />

- 계층 순서는 다음과 같습니다.

  - 첫 번째 가장 아래 계층은 Embedding 계층으로 단어 ID를 단어의 분산 표현(단어 벡터)으로 변환합니다.

  - 그리고 그 분산 표현이 RNN 계층으로 입력되어 RNN 계층은 은닉 상태를 다음 층으로 출력함과 동시에 다음 시각의 RNN 계층으로 출력합니다.

  - RNN 계층이 위로 출력한 은닉 상태는 Affine 계층을 거쳐 Softmax 계층으로 전해집니다.

- 이제 신경망의 순전파에 한정해 구체적인 데이터를 흘려보면서 출력 결과를 관찰해 보겠습니다. 입력 데이터는 'You say goodbye and I say hello.'입니다.

<img src="README.assets/fig 5-26.png" alt="fig 5-26" style="zoom:50%;" />

- 순전파에 입력되는 데이터는 단어 ID의 배열입니다.

  - 그 중 첫 번째 시각에 입력되는 단어는 단어 ID가 0인 'you'가 입력됩니다. 이 때 Softmax 계층이 출력한 확률 분포는 'say'가 가장 높습니다.

  - 즉, 'you' 다음에 출현하는 단어가 'say'임을 올바르게 예측했습니다. 이처럼 학습하려면 잘 학습된 좋은 가중치를 사용해야 합니다.

  - 두 번째 단어인 'say'를 입력된 부분의 Softmax 계층 출력은 'goodbye'와 'hello' 두 곳에서 가장 높게 나왔습니다.

  - 이를 연결하면 'you say goodbye'와 'you say hello' 모두 자연스러운 문장이므로 올바르게 학습했다고 할 수 있습니다.

- 여기서 주목할 점은 RNN 계층은 'you say'라는 맥락을 '기억'하고 있다는 점입니다.

  - 더 정확하게는 RNN이 'you say'라는 과거 정보를 응집된 은닉 상태 벡터로 저장해두고 있습니다.

- 이러한 정보를 Affine 계층에 그리고 다음 시각의 RNN 계층에 전달하는 것이 RNN 계층이 하는 일입니다.

- 이처럼 RNNLM은 지금까지 입력된 단어를 '기억'하고 그것을 바탕으로 다음에 출현할 단어를 예측합니다.

  - 이 일을 가능하게 하는 비결이 바로 RNN 계층의 존재입니다.

  - RNN 계층이 과거에서 현재로 데이터를 계속 흘려보내줌으로써 과거의 정보를 인코딩해 저장할 수 있는 것입니다.

### 5.4.2 Time 계층 구현

- 지금까지 시계열 데이터를 한꺼번에 처리하는 계층을 Time RNN이라는 이름의 계층으로 구현했습니다.

- 이번 절에서도 마찬가지로, 시계열 데이터를 한꺼번에 처리하는 계층을 Time Embedding, Time Affine 형태의 이름으로 구현하겠습니다.

<img src="README.assets/fig 5-27.png" alt="fig 5-27" style="zoom:50%;" />

#### Note

> T개분의 시계열 데이터를 한꺼번에 처리하는 계층을 'Time XX 계층'이라고 부르겠습니다.
>
> 이러한 계층들이 구현되어 있다면, 그 계층들을 레고 블록처럼 조립하는 것만으로 시계열 데이터를 다루는 신경망을 완성할 수 있습니다.

- Time 계층은 간단하게 구현할 수 있습니다. 예를 들어 Time Affine 계층은 다음과 같이 Affine 계층 T개를 준비해 각 시각의 데이터를 개별적으로 처리하면 됩니다.

<img src="README.assets/fig 5-28.png" alt="fig 5-28" style="zoom:50%;" />

- Time Embedding 계층 역시 순전파 시에 T개의 Embedding 계층을 준비하고 각 Embedding 계층이 각 시각의 데이터를 처리합니다.

- Time Affine 계층과 Time Embedding 계층은 특별히 어려운 부분이 없으니 설명을 생략하겠습니다.

  - 이 중 Time Affine 계층은 Affine 계층 T개를 사용하는 대신 행렬 연산으로 한 번에 처리하는 방식을 구현했습니다.

  > 자세한 내용은 chapter05/commons/time_layers.py의 TimeAffine class를 확인하세요.

- 한편, 시계열 버전의 Softmax는 손실 오차를 구하는 Cross Entropy Error 계층도 함께 구현해야 합니다. 이 구성을 Time Softmax with Loss로 구현합니다.

<img src="README.assets/fig 5-29.png" alt="fig 5-29" style="zoom:50%;" />

- 위 그림에서 x는 아래층에서 전해지는 '점수'를 의미하고 t는 정답 레이블을 의미합니다.

  - 이처럼 T개의 Softmax with Loss 계층 각각이 손실을 산출하고 그 손실을 합산해 평균한 값이 최종 손실이 됩니다.

  <img src="README.assets/e 5-11.png" alt="e 5-11" style="zoom:50%;" />

- 중요한 점은 이 책의 Softmax with Loss 계층은 해당 미니배치의 손실의 평균을 구했습니다.

  - 즉, 데이터 N개 짜리 미니배치라면 그 N개의 손실을 더해 다시 N으로 나눠 데이터 1개당 평균 손실을 구했습니다.

  - 이와 마찬가지로 Time Softmax with Loss 계층도 시계열에 대한 평균을 구하므로 결과적으로 데이터 1개당 평균 손실을 구해 최종 출력으로 내보냅니다.

- 지금까지 Time 계층들에 관해 알아봤습니다.

> 실제 구현에 관심이 있다면 chapter05/commons/time_layers.py를 확인하세요.
